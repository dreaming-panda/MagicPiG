<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Publications</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h2 id="publications">Publications</h2>
<p><strong>HexGen: Generative Inference of Large Language Model over Heterogeneous Environment</strong> <font color=Blue>Forty-first International Conference on Machine Learning</font>
Jiang, Youhe; Yan, Ran; Yao, Xiaozhe; Zhou, Yang; Chen, Beidi; Yuan, Binhang;</p>
<p><strong>SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices</strong> arXiv preprint arXiv:2406.02532<br>
Svirschevski, Ruslan; May, Avner; Chen, Zhuoming; Chen, Beidi; Jia, Zhihao; Ryabinin, Max;</p>
<p><strong>Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache</strong> Proceedings of Machine Learning and Systems<br>
Zhang, Zhenyu; Liu, Shiwei; Chen, Runjin; Kailkhura, Bhavya; Chen, Beidi; Wang, Atlas;</p>
<p><strong>Layer skip: Enabling early exit inference and self-speculative decoding</strong> arXiv preprint arXiv:2404.16710<br>
Elhoushi, Mostafa; Shrivastava, Akshat; Liskovich, Diana; Hosmer, Basil; Wasti, Bram; Lai, Liangzhen; Mahmoud, Anas; Acun, Bilge; Agarwal, Saurabh; Roman, Ahmed;</p>
<p><strong>Galore: Memory-efficient llm training by gradient low-rank projection</strong> arXiv preprint arXiv:2403.03507<br>
Zhao, Jiawei; Zhang, Zhenyu; Chen, Beidi; Wang, Zhangyang; Anandkumar, Anima; Tian, Yuandong;</p>
<p><strong>Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference</strong> arXiv preprint arXiv:2402.09398<br>
Dong, Harry; Yang, Xinyu; Zhang, Zhenyu; Wang, Zhangyang; Chi, Yuejie; Chen, Beidi;</p>
<p><strong>KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</strong> arXiv preprint arXiv:2402.02750<br>
Liu, Zirui; Yuan, Jiayi; Jin, Hongye; Zhong, Shaochen; Xu, Zhaozhuo; Braverman, Vladimir; Chen, Beidi; Hu, Xia;</p>
<p><strong>Laughing hyena distillery: Extracting compact recurrences from convolutions</strong> Advances in Neural Information Processing Systems<br>
Massaroli, Stefano; Poli, Michael; Fu, Dan; Kumbong, Hermann; Parnichkun, Rom; Romero, David; Timalsina, Aman; McIntyre, Quinn; Chen, Beidi; Rudra, Atri;</p>
<p><strong>Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention</strong> arXiv preprint arXiv:2310.00535<br>
Tian, Yuandong; Wang, Yiping; Zhang, Zhenyu; Chen, Beidi; Du, Simon;</p>
<p><strong>Efficient streaming language models with attention sinks</strong> arXiv preprint arXiv:2309.17453<br>
Xiao, Guangxuan; Tian, Yuandong; Chen, Beidi; Han, Song; Lewis, Mike;</p>
<p><strong>Fast Algorithms for a New Relaxation of Optimal Transport</strong> The Thirty Sixth Annual Conference on Learning Theory<br>
Charikar, Moses; Chen, Beidi; Ré, Christopher; Waingarten, Erik;</p>
<p><strong>CocktailSGD: Fine-tuning foundation models over 500Mbps networks</strong> International Conference on Machine Learning<br>
Wang, Jue; Lu, Yucheng; Yuan, Binhang; Chen, Beidi; Liang, Percy; De Sa, Christopher; Re, Christopher; Zhang, Ce;</p>
<p><strong>H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</strong> International Conference on Machine Learning<br>
Zhang, Zhenyu; Sheng, Ying; Zhou, Tianyi; Chen, Tianlong; Zheng, Lianmin; Cai, Ruisi; Song, Zhao; Tian, Yuandong; Ré, Christopher; Barrett, Clark;</p>
<p><strong>Deja vu: Contextual sparsity for efficient llms at inference time</strong> International Conference on Machine Learning<br>
Liu, Zichang; Wang, Jue; Dao, Tri; Zhou, Tianyi; Yuan, Binhang; Song, Zhao; Shrivastava, Anshumali; Zhang, Ce; Tian, Yuandong; Re, Christopher;</p>
<p><strong>FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU</strong> International Conference on Machine Learning<br>
Sheng, Ying; Zheng, Lianmin; Yuan, Binhang; Li, Zhuohan; Ryabinin, Max; Chen, Beidi; Liang, Percy; Re, Christopher; Stoica, Ion; Zhang, Ce;</p>
<p><strong>Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer</strong> International Conference on Machine Learning<br>
Tian, Yuandong; Wang, Yiping; Chen, Beidi; Du, Simon;</p>
<p><strong>Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt</strong> arXiv preprint arXiv:2305.11186<br>
Xu, Zhaozhuo; Liu, Zirui; Chen, Beidi; Tang, Yuxin; Wang, Jue; Zhou, Kaixiong; Hu, Xia; Shrivastava, Anshumali;</p>
<p><strong>Decentralized training of foundation models in heterogeneous environments</strong> Neural Information Processing Systems.<br>
Yuan, Binhang; He, Yongjun; Davis, Jared Quincy; Zhang, Tianyi; Dao, Tri; Chen, Beidi; Liang, Percy; Re, Christopher; Zhang, Ce;</p>
<p><strong>Fine-tuning language models over slow networks using activation compression with guarantees</strong> arXiv preprint arXiv:2206.01299<br>
Wang, Jue; Yuan, Binhang; Rimanic, Luka; He, Yongjun; Dao, Tri; Chen, Beidi; Re, Christopher; Zhang, Ce;</p>
<p><strong>Halos: Hashing large output space for cheap inference</strong> Proceedings of Machine Learning and Systems<br>
Liu, Zichang; Xu, Zhaozhuo; Ji, Alan; Zhang, Junyan; Li, Jonathan; Chen, Beidi; Shrivastava, Anshumali;</p>
<p><strong>Monarch: Expressive structured matrices for efficient and accurate training</strong> International Conference on Machine Learning<br>
Dao, Tri; Chen, Beidi; Sohoni, Nimit S; Desai, Arjun; Poli, Michael; Grogan, Jessica; Liu, Alexander; Rao, Aniruddh; Rudra, Atri; Ré, Christopher;</p>
<p><strong>Pixelated butterfly: Simple and efficient sparse training for neural network models</strong> International Conference on Learning Representations<br>
Chen, Beidi; Dao, Tri; Liang, Kaizhao; Yang, Jiaming; Song, Zhao; Rudra, Atri; Re, Christopher;</p>
<p><strong>Satellite Images and Deep Learning to Identify Discrepancy in Mailing Addresses with Applications to Census 2020 in Houston</strong> JSM Proceedings, Statistical Learning and Data Science Section. American Statistical Association.<br>
Xu, Zhaozhuo; Ji, Alan Baonan; Woods, Andrew; Chen, Beidi; Shrivastava, Anshumali;</p>
<p><strong>Scatterbrain: Unifying sparse and low-rank attention</strong> Advances in Neural Information Processing Systems<br>
Chen, Beidi; Dao, Tri; Winsor, Eric; Song, Zhao; Rudra, Atri; Ré, Christopher;</p>
<p><strong>Locality sensitive teaching</strong> Advances in Neural Information Processing Systems<br>
Xu, Zhaozhuo; Chen, Beidi; Li, Chaojian; Liu, Weiyang; Song, Le; Lin, Yingyan; Shrivastava, Anshumali;</p>
<p><strong>A tale of two efficient and informative negative sampling distributions</strong> International conference on machine learning<br>
Daghaghi, Shabnam; Medini, Tharun; Meisburger, Nicholas; Chen, Beidi; Zhao, Mengnan; Shrivastava, Anshumali;</p>
<p><strong>MONGOOSE: A learnable LSH framework for efficient neural network training</strong> International Conference on Learning Representations<br>
Chen, Beidi; Liu, Zichang; Peng, Binghui; Xu, Zhaozhuo; Li, Jonathan Lingjie; Dao, Tri; Song, Zhao; Shrivastava, Anshumali; Re, Christopher;</p>
<p><strong>SOLAR: Sparse Orthogonal Learned and Random Embeddings</strong> International Conference on Learning Representations<br>
Medini, Tharun; Chen, Beidi; Shrivastava, Anshumali;</p>
<p><strong>Locality Sensitive Sampling for Extreme-Scale Optimization and Deep Learning</strong> nan<br>
Chen, Beidi;</p>
<p><strong>Fast and accurate stochastic gradient estimation</strong> Advances in Neural Information Processing Systems<br>
Chen, Beidi; Xu, Yingchen; Shrivastava, Anshumali;</p>
<p><strong>Angular visual hardness</strong> International Conference on Machine Learning<br>
Chen, Beidi; Liu, Weiyang; Yu, Zhiding; Kautz, Jan; Shrivastava, Anshumali; Garg, Animesh; Anandkumar, Animashree;</p>
<p><strong>SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-scale Deep Learning Systems</strong> Proceedings of Machine Learning and System<br>
Chen, Beidi; Medini, Tharun; Farwell, James; Gobriel, Sameh; Tai, Charlie; Shrivastava, Anshumali;</p>
<p><strong>Densified winner take all (WTA) hashing for sparse datasets</strong> Uncertainty in artificial intelligence<br>
Chen, Beidi; Shrivastava, Anshumali;</p>
<p><strong>Unique entity estimation with application to the Syrian conflict</strong> The Annals of Applied Statistics<br>
Chen, Beidi; Shrivastava, Anshumali; Steorts, Rebecca C;</p>
<p><strong>Analyzing log analysis: An empirical study of user log mining</strong> 28th Large Installation System Administration Conference (LISA14)<br>
Alspaugh, Sara; Chen, Beidi; Lin, Jessica; Ganapathi, Archana; Hearst, Marti; Katz, Randy;</p>
<h2 id="preprints">Preprints</h2>
<p><strong>Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity</strong> arXiv preprint arXiv:2406.02913<br>
Guo, Wentao; Long, Jikai; Zeng, Yimeng; Liu, Zirui; Yang, Xinyu; Ran, Yide; Gardner, Jacob R; Bastani, Osbert; De Sa, Christopher; Yu, Xiaodong;</p>
<p><strong>SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices</strong> arXiv preprint arXiv:2406.02532<br>
Svirschevski, Ruslan; May, Avner; Chen, Zhuoming; Chen, Beidi; Jia, Zhihao; Ryabinin, Max;</p>
<p><strong>Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</strong> arXiv preprint arXiv:2405.19325<br>
Li, Minghan; Chen, Xilun; Holtzman, Ari; Chen, Beidi; Lin, Jimmy; Yih, Wen-tau; Lin, Xi Victoria;</p>
<p><strong>Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding</strong> arXiv preprint arXiv:2404.11912<br>
Sun, Hanshi; Chen, Zhuoming; Yang, Xinyu; Tian, Yuandong; Chen, Beidi;</p>
<p><strong>Megalodon: Efficient llm pretraining and inference with unlimited context length</strong> arXiv preprint arXiv:2404.08801<br>
Ma, Xuezhe; Yang, Xiaomeng; Xiong, Wenhan; Chen, Beidi; Yu, Lili; Zhang, Hao; May, Jonathan; Zettlemoyer, Luke; Levy, Omer; Zhou, Chunting;</p>
<p><strong>Prompt-prompted Mixture of Experts for Efficient LLM Generation</strong> arXiv preprint arXiv:2404.01365<br>
Dong, Harry; Chen, Beidi; Chi, Yuejie;</p>
<p><strong>Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding</strong> arXiv preprint arXiv:2403.04797<br>
Zhang, Zhenyu; Chen, Runjin; Liu, Shiwei; Yao, Zhewei; Ruwase, Olatunji; Chen, Beidi; Wu, Xiaoxia; Wang, Zhangyang;</p>
<p><strong>Sample-efficient Surrogate Model for Frequency Response of Linear PDEs using Self-Attentive Complex Polynomials</strong> arXiv preprint arXiv:2301.02747<br>
Cohen, Andrew; Dou, Weiping; Zhu, Jiang; Koziel, Slawomir; Renner, Peter; Mattsson, Jan-Ove; Yang, Xiaomeng; Chen, Beidi; Stone, Kevin; Tian, Yuandong;</p>
<p><strong>LLM Inference Unveiled: Survey and Roofline Model Insights</strong> arXiv preprint arXiv:2402.16363<br>
Yuan, Zhihang; Shang, Yuzhang; Zhou, Yang; Dong, Zhen; Xue, Chenhao; Wu, Bingzhe; Li, Zhikai; Gu, Qingyi; Lee, Yong Jae; Yan, Yan;</p>
<p><strong>Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding</strong> arXiv preprint arXiv:2402.12374<br>
Chen, Zhuoming; May, Avner; Svirschevski, Ruslan; Huang, Yuhsun; Ryabinin, Max; Jia, Zhihao; Chen, Beidi;</p>
<p><strong>Learn To be Efficient: Build Structured Sparsity in Large Language Models</strong> arXiv preprint arXiv:2402.06126<br>
Zheng, Haizhong; Bai, Xiaoyan; Chen, Beidi; Lai, Fan; Prakash, Atul;</p>
<p><strong>Inrank: Incremental low-rank learning</strong> arXiv preprint arXiv:2306.11250<br>
Zhao, Jiawei; Zhang, Yifei; Chen, Beidi; Schäfer, Florian; Anandkumar, Anima;</p>
<p><strong>Sub-linear privacy-preserving near-neighbor search</strong> arXiv preprint arXiv:1612.01835<br>
Riazi, M Sadegh; Chen, Beidi; Shrivastava, Anshumali; Wallach, Dan; Koushanfar, Farinaz;</p>
<h2 id="workshops">Workshops</h2>
<p><strong>Towards Structured Sparsity in Transformers for Efficient Inference</strong> Workshop on Efficient Systems for Foundation Models@ ICML2023<br>
Dong, Harry; Chen, Beidi; Chi, Yuejie;</p>
<p><strong>BearLoc: a composable distributed framework for indoor localization systems</strong> Proceedings of the 2015 Workshop on IoT challenges in Mobile and Industrial Systems<br>
Chen, Kaifei; He, Siyuan; Chen, Beidi; Kolb, John; Katz, Randy H; Culler, David E;</p>

            
            
        </body>
        </html>