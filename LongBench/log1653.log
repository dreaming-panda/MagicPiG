rm: cannot remove './pred_e/llama3-8b-instruct-8k-hash-K-10-L-60': No such file or directory
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.57it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.70it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.73it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.50it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.15it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.47it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.54it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.56it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.00it/s]
  0%|          | 0/23 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.33it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.49it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.46it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.77it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
  0%|          | 0/23 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.52it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.63it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.60it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.04it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/23 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.29it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.43it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.51it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.87it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/23 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.41it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.49it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.61it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.01it/s]
  0%|          | 0/22 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.32it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.55it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.64it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.51it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.07it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/22 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.48it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.59it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.66it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.47it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.09it/s]
  0%|          | 0/22 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:15<05:41, 15.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.62it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.47it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.48it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.81it/s]
  0%|          | 0/22 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.35it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.24it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.71it/s]
  4%|▍         | 1/23 [00:16<06:05, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/22 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/22 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  4%|▍         | 1/23 [00:16<06:11, 16.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▍         | 1/22 [00:13<04:35, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  4%|▍         | 1/23 [00:15<05:50, 15.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▍         | 1/22 [00:13<04:46, 13.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▊         | 2/23 [00:29<05:09, 14.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▍         | 1/22 [00:13<04:40, 13.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▍         | 1/22 [00:17<06:11, 17.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▍         | 1/22 [00:14<05:07, 14.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▊         | 2/23 [00:33<05:51, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  5%|▍         | 1/22 [00:15<05:20, 15.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▉         | 2/22 [00:29<05:01, 15.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▊         | 2/23 [00:31<05:24, 15.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▊         | 2/23 [00:34<06:00, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▉         | 2/22 [00:24<03:51, 11.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▉         | 2/22 [00:28<04:42, 14.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▉         | 2/22 [00:30<05:13, 15.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▉         | 2/22 [00:34<05:40, 17.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 3/23 [00:47<05:25, 16.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 3/23 [00:45<04:51, 14.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  9%|▉         | 2/22 [00:29<04:54, 14.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 4/23 [00:47<03:00,  9.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 3/23 [00:49<05:32, 16.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 3/23 [00:45<04:59, 14.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 14%|█▎        | 3/22 [00:40<04:04, 12.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 14%|█▎        | 3/22 [00:45<04:50, 15.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 14%|█▎        | 3/22 [00:39<04:04, 12.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 14%|█▎        | 3/22 [00:37<03:57, 12.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 18%|█▊        | 4/22 [00:41<02:32,  8.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 18%|█▊        | 4/22 [00:40<02:38,  8.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 18%|█▊        | 4/22 [00:47<03:09, 10.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 4/23 [01:02<04:57, 15.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 14%|█▎        | 3/22 [00:41<04:11, 13.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 22%|██▏       | 5/23 [00:59<03:05, 10.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 22%|██▏       | 5/23 [01:04<03:13, 10.76s/it] 14%|█▎        | 3/22 [00:52<05:29, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 4/23 [00:58<04:27, 14.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 5/22 [00:50<02:30,  8.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 4/23 [01:05<05:04, 16.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 18%|█▊        | 4/22 [01:00<04:35, 15.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 5/22 [00:51<02:39,  9.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 18%|█▊        | 4/22 [00:49<03:26, 11.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 5/22 [00:59<03:07, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 22%|██▏       | 5/23 [01:08<03:49, 12.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 26%|██▌       | 6/23 [01:13<03:15, 11.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 18%|█▊        | 4/22 [01:06<04:50, 16.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 5/22 [01:11<03:54, 13.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 26%|██▌       | 6/23 [01:22<03:43, 13.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 6/22 [01:06<02:57, 11.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 7/23 [01:23<02:26,  9.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 6/22 [01:10<02:58, 11.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 22%|██▏       | 5/23 [01:22<04:54, 16.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 7/23 [01:22<02:52, 10.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 6/22 [01:07<03:10, 11.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 5/22 [01:06<03:45, 13.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 26%|██▌       | 6/23 [01:20<03:33, 12.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 6/22 [01:07<02:28,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 5/22 [01:20<04:20, 15.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 32%|███▏      | 7/22 [01:16<02:42, 10.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 32%|███▏      | 7/22 [01:22<02:51, 11.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 26%|██▌       | 6/23 [01:34<04:13, 14.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 32%|███▏      | 7/22 [01:18<02:51, 11.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 6/22 [01:29<04:01, 15.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 7/23 [01:31<03:09, 11.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 35%|███▍      | 8/23 [01:40<02:53, 11.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 6/22 [01:29<03:32, 13.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 32%|███▏      | 7/22 [01:20<02:35, 10.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 35%|███▍      | 8/23 [01:36<03:00, 12.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 36%|███▋      | 8/22 [01:27<02:31, 10.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 36%|███▋      | 8/22 [01:28<02:34, 11.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 7/23 [01:45<03:42, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 32%|███▏      | 7/22 [01:39<03:24, 13.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 36%|███▋      | 8/22 [01:37<02:57, 12.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 36%|███▋      | 8/22 [01:43<02:25, 10.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 36%|███▋      | 8/22 [01:32<02:32, 10.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 39%|███▉      | 9/23 [01:54<02:55, 12.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 39%|███▉      | 9/23 [01:50<02:54, 12.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 35%|███▍      | 8/23 [01:48<03:22, 13.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 41%|████      | 9/22 [01:42<02:36, 12.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 32%|███▏      | 7/22 [01:46<03:39, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 41%|████      | 9/22 [01:39<02:23, 11.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 39%|███▉      | 9/23 [01:51<02:26, 10.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 10/23 [01:53<01:40,  7.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 41%|████      | 9/22 [01:53<02:14, 10.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 10/23 [02:03<02:26, 11.31s/it] 45%|████▌     | 10/22 [01:46<01:56,  9.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 35%|███▍      | 8/23 [02:01<03:35, 14.38s/it] 41%|████      | 9/22 [01:42<02:18, 10.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 48%|████▊     | 11/23 [01:56<01:13,  6.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 10/23 [02:00<02:33, 11.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 48%|████▊     | 11/23 [02:04<01:53,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 41%|████      | 9/22 [01:55<03:05, 14.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 45%|████▌     | 10/22 [02:02<01:59, 10.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 45%|████▌     | 10/22 [01:53<02:24, 12.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 36%|███▋      | 8/22 [02:01<03:23, 14.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 45%|████▌     | 10/22 [01:55<02:15, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 11/22 [01:57<01:34,  8.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 45%|████▌     | 10/22 [02:05<02:35, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 11/22 [02:04<02:15, 12.36s/it] 39%|███▉      | 9/23 [02:18<03:34, 15.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 48%|████▊     | 11/23 [02:21<02:42, 13.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 52%|█████▏    | 12/23 [02:19<02:04, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 13/23 [02:21<01:26,  8.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 10/23 [02:28<02:57, 13.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 41%|████      | 9/22 [02:19<03:22, 15.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 52%|█████▏    | 12/23 [02:26<02:24, 13.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 11/22 [02:24<02:29, 13.63s/it] 50%|█████     | 11/22 [02:14<02:40, 14.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 55%|█████▍    | 12/22 [02:15<01:55, 11.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 11/22 [02:23<02:39, 14.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 52%|█████▏    | 12/23 [02:41<02:49, 15.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 55%|█████▍    | 12/22 [02:26<02:31, 15.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 45%|████▌     | 10/22 [02:31<02:53, 14.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 61%|██████    | 14/23 [02:40<01:44, 11.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 55%|█████▍    | 12/22 [02:31<02:33, 15.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 48%|████▊     | 11/23 [02:48<03:06, 15.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 13/23 [02:46<02:31, 15.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 55%|█████▍    | 12/22 [02:47<02:45, 16.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 59%|█████▉    | 13/22 [02:36<02:09, 14.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 55%|█████▍    | 12/22 [02:46<02:51, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 13/23 [03:02<02:49, 16.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 59%|█████▉    | 13/22 [02:46<02:31, 16.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 65%|██████▌   | 15/23 [02:58<01:48, 13.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 11/22 [02:54<03:08, 17.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 64%|██████▎   | 14/22 [02:50<01:41, 12.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 52%|█████▏    | 12/23 [03:06<02:59, 16.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 61%|██████    | 14/23 [03:05<02:28, 16.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 59%|█████▉    | 13/22 [02:51<02:30, 16.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 65%|██████▌   | 15/23 [03:07<01:36, 12.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 64%|██████▎   | 14/22 [02:53<01:38, 12.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|██████▉   | 16/23 [03:11<01:07,  9.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 64%|██████▎   | 14/22 [02:56<02:07, 15.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 59%|█████▉    | 13/22 [03:09<02:42, 18.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 59%|█████▉    | 13/22 [03:06<02:42, 18.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 61%|██████    | 14/23 [03:23<02:45, 18.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 55%|█████▍    | 12/22 [03:14<03:00, 18.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 13/23 [03:24<02:46, 16.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 68%|██████▊   | 15/22 [03:10<01:45, 15.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|██████▉   | 16/23 [03:19<01:50, 15.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 68%|██████▊   | 15/22 [03:11<01:38, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 64%|██████▎   | 14/22 [03:29<02:29, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 68%|██████▊   | 15/22 [03:17<02:02, 17.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 74%|███████▍  | 17/23 [03:34<01:22, 13.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 65%|██████▌   | 15/23 [03:41<02:25, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 68%|██████▊   | 15/22 [03:34<01:42, 14.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 64%|██████▎   | 14/22 [03:30<02:36, 19.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 59%|█████▉    | 13/22 [03:32<02:42, 18.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 16/22 [03:24<01:25, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 74%|███████▍  | 17/23 [03:41<01:45, 17.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 16/22 [03:39<01:10, 11.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 61%|██████    | 14/23 [03:47<02:47, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 16/22 [03:33<01:45, 17.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 16/22 [03:34<01:40, 16.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 78%|███████▊  | 18/23 [03:51<01:13, 14.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|██████▉   | 16/23 [04:01<02:10, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 64%|██████▎   | 14/22 [03:53<02:32, 19.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 78%|███████▊  | 18/23 [03:59<01:28, 17.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 65%|██████▌   | 15/23 [04:04<02:26, 18.36s/it] 68%|██████▊   | 15/22 [03:53<02:24, 20.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 17/22 [03:58<01:08, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 17/22 [03:46<01:23, 16.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 17/22 [03:52<01:28, 17.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 17/22 [03:52<01:25, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 19/23 [04:09<01:02, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 74%|███████▍  | 17/23 [04:18<01:48, 18.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 78%|███████▊  | 18/23 [04:24<01:12, 14.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 16/22 [04:10<01:57, 19.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 82%|████████▏ | 18/22 [04:16<01:00, 15.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 82%|████████▏ | 18/22 [04:04<01:08, 17.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 17/22 [04:11<01:10, 14.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|██████▉   | 16/23 [04:23<02:09, 18.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 19/23 [04:18<01:13, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 68%|██████▊   | 15/22 [04:16<02:20, 20.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 82%|████████▏ | 18/22 [04:12<01:14, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 82%|████████▏ | 18/22 [04:15<00:43, 10.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 82%|████████▏ | 18/22 [04:10<01:09, 17.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 20/23 [04:30<00:51, 17.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 19/23 [04:42<01:03, 15.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 86%|████████▋ | 19/22 [04:34<00:48, 16.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 16/22 [04:34<01:56, 19.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 86%|████████▋ | 19/22 [04:25<00:54, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 74%|███████▍  | 17/23 [04:44<01:55, 19.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 86%|████████▋ | 19/22 [04:31<00:55, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 20/23 [04:39<00:57, 19.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 86%|████████▋ | 19/22 [04:28<00:52, 17.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 86%|████████▋ | 19/22 [04:35<00:41, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 91%|█████████▏| 21/23 [04:43<00:28, 14.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 91%|█████████▏| 21/23 [04:53<00:37, 18.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 20/23 [05:01<00:49, 16.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 91%|█████████ | 20/22 [04:51<00:32, 16.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 91%|█████████ | 20/22 [04:49<00:36, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 91%|█████████ | 20/22 [04:46<00:35, 17.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 17/22 [04:54<01:39, 19.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 78%|███████▊  | 18/23 [05:06<01:39, 19.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 91%|█████████ | 20/22 [04:48<00:39, 19.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 91%|█████████ | 20/22 [04:58<00:33, 16.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 96%|█████████▌| 22/23 [05:06<00:17, 17.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 96%|█████████▌| 22/23 [05:16<00:20, 20.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 91%|█████████▏| 21/23 [05:24<00:37, 18.55s/it] 95%|█████████▌| 21/22 [05:14<00:18, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 82%|████████▏ | 18/22 [05:14<01:18, 19.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 19/23 [05:24<01:17, 19.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 95%|█████████▌| 21/22 [05:12<00:19, 19.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 95%|█████████▌| 21/22 [05:09<00:19, 19.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 95%|█████████▌| 21/22 [05:11<00:20, 20.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 95%|█████████▌| 21/22 [05:21<00:18, 18.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 23/23 [05:29<00:00, 18.94s/it]100%|██████████| 23/23 [05:29<00:00, 14.34s/it]
100%|██████████| 23/23 [05:39<00:00, 21.11s/it]100%|██████████| 23/23 [05:39<00:00, 14.77s/it]
 86%|████████▋ | 19/22 [05:32<00:57, 19.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 20/23 [05:42<00:57, 19.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 96%|█████████▌| 22/23 [05:47<00:19, 19.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 22/22 [05:38<00:00, 19.87s/it]100%|██████████| 22/22 [05:38<00:00, 15.37s/it]
 91%|█████████ | 20/22 [05:38<00:30, 15.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 22/22 [05:35<00:00, 20.82s/it]100%|██████████| 22/22 [05:35<00:00, 15.25s/it]
100%|██████████| 22/22 [05:32<00:00, 20.47s/it]100%|██████████| 22/22 [05:32<00:00, 15.12s/it]
100%|██████████| 22/22 [05:35<00:00, 21.48s/it]100%|██████████| 22/22 [05:35<00:00, 15.23s/it]
100%|██████████| 22/22 [05:44<00:00, 19.93s/it]100%|██████████| 22/22 [05:44<00:00, 15.68s/it]
 91%|█████████▏| 21/23 [06:05<00:40, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 23/23 [06:10<00:00, 20.94s/it]100%|██████████| 23/23 [06:10<00:00, 16.13s/it]
 95%|█████████▌| 21/22 [06:01<00:17, 17.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 96%|█████████▌| 22/23 [06:11<00:15, 15.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 22/22 [06:24<00:00, 19.23s/it]100%|██████████| 22/22 [06:24<00:00, 17.47s/it]
100%|██████████| 23/23 [06:34<00:00, 18.08s/it]100%|██████████| 23/23 [06:34<00:00, 17.16s/it]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.78it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.80it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.79it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.65it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.28it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.63it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.69it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.66it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.06it/s]
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.53it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.54it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.51it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.08it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.56it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.69it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.77it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.59it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.20it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.49it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.48it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.54it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.88it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.65it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.72it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.76it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.59it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.21it/s]
  7%|▋         | 1/15 [00:09<02:12,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 1/15 [00:06<01:35,  6.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.67it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.80it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.59it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.00it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 1/15 [00:04<01:08,  4.92s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.68it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.65it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.66it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.48it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.12it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 13%|█▎        | 2/15 [00:15<01:33,  7.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.23it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.27it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.19it/s]  0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.84it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.58it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 1/15 [00:11<02:39, 11.36s/it]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 13%|█▎        | 2/15 [00:14<01:35,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.57it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.55it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s]
  7%|▋         | 1/15 [00:09<02:14,  9.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:19<01:10,  5.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 1/15 [00:07<01:46,  7.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 1/15 [00:06<01:26,  6.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/15 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:19<01:13,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 2/15 [00:14<01:41,  7.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 1/15 [00:05<01:16,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 2/15 [00:13<01:24,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 1/15 [00:06<01:25,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:18<01:12,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 1/15 [00:04<01:06,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 2/15 [00:11<01:17,  5.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 2/15 [00:21<02:18, 10.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:27<01:13,  6.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 2/15 [00:17<01:53,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 2/15 [00:11<01:12,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:17<01:06,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:25<01:31,  7.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 2/15 [00:11<01:14,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:31<00:57,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:28<01:22,  7.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:24<01:05,  5.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 2/15 [00:10<01:10,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:17<01:10,  5.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:32<00:38,  4.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:23<01:28,  7.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:17<01:09,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:24<00:55,  5.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:29<01:08,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:14<00:58,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:37<00:34,  4.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:21<00:57,  5.24s/it] 27%|██▋       | 4/15 [00:26<01:12,  6.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:31<01:02,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:22<00:35,  3.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 3/15 [00:19<01:21,  6.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:34<00:56,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:23<00:24,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:37<01:18,  7.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:25<01:13,  6.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:20<00:58,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:32<01:00,  6.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:31<01:03,  6.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 4/15 [00:25<01:08,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:38<00:57,  6.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:23<00:45,  4.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:44<01:07,  7.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [00:49<00:47,  6.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:44<01:05,  7.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:42<00:46,  5.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:36<01:20,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:41<01:03,  7.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:35<00:46,  5.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:37<00:51,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:30<00:47,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 5/15 [00:36<01:20,  8.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:43<01:14,  8.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:54<01:07,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:45<00:51,  6.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [00:53<00:52,  7.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:01<00:50,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:56<01:10,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:50<01:02,  7.78s/it] 47%|████▋     | 7/15 [00:53<01:07,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 6/15 [00:44<01:12,  8.05s/it] 53%|█████▎    | 8/15 [00:47<00:54,  7.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:42<01:00,  7.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [00:54<00:51,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [01:06<01:07,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:06<00:53,  8.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:13<00:47,  9.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [01:08<01:09,  9.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [01:02<01:04,  9.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [01:05<01:07,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 7/15 [00:55<01:12,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [00:59<00:54,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [00:55<01:02,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:04<00:49,  8.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:18<01:02, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:18<00:58,  9.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:17<00:48,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:25<00:41, 10.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:14<01:00, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:17<01:02, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 8/15 [01:07<01:10, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:11<00:50, 10.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:07<00:59,  9.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:17<00:47,  9.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:31<00:54, 10.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:30<00:52, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:23<00:48,  9.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:29<00:41, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:37<00:32, 10.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:29<00:54, 10.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 9/15 [01:19<01:04, 10.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:23<00:42, 10.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:19<00:53, 10.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:26<00:37,  9.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:22<00:34,  8.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:42<00:44, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:42<00:43, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:35<00:41, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:31<00:29,  9.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:41<00:32, 10.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [01:50<00:22, 11.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 10/15 [01:31<00:55, 11.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:41<00:45, 11.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:38<00:30, 10.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:35<00:28,  9.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:54<00:34, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:52<00:32, 10.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:47<00:32, 10.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [01:43<00:21, 10.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [01:53<00:22, 11.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [02:01<00:11, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 11/15 [01:42<00:44, 11.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 12/15 [01:53<00:34, 11.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [01:50<00:21, 10.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [01:47<00:20, 10.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [02:06<00:23, 11.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [02:04<00:22, 11.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [01:59<00:22, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [01:55<00:10, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [02:05<00:11, 11.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 15/15 [02:13<00:00, 11.53s/it]100%|██████████| 15/15 [02:13<00:00,  8.90s/it]
 80%|████████  | 12/15 [01:54<00:34, 11.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 13/15 [02:06<00:23, 11.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [02:02<00:11, 11.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [01:59<00:10, 10.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [02:18<00:11, 11.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [02:16<00:11, 11.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [02:11<00:11, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 15/15 [02:07<00:00, 11.33s/it]100%|██████████| 15/15 [02:07<00:00,  8.51s/it]
100%|██████████| 15/15 [02:18<00:00, 11.76s/it]100%|██████████| 15/15 [02:18<00:00,  9.20s/it]
 87%|████████▋ | 13/15 [02:07<00:23, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 14/15 [02:18<00:11, 11.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 15/15 [02:14<00:00, 11.48s/it]100%|██████████| 15/15 [02:14<00:00,  8.99s/it]
100%|██████████| 15/15 [02:11<00:00, 11.26s/it]100%|██████████| 15/15 [02:11<00:00,  8.76s/it]
100%|██████████| 15/15 [02:30<00:00, 11.86s/it]100%|██████████| 15/15 [02:30<00:00, 10.05s/it]
100%|██████████| 15/15 [02:28<00:00, 11.63s/it]100%|██████████| 15/15 [02:28<00:00,  9.93s/it]
100%|██████████| 15/15 [02:23<00:00, 11.63s/it]100%|██████████| 15/15 [02:23<00:00,  9.58s/it]
 93%|█████████▎| 14/15 [02:19<00:11, 11.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 15/15 [02:30<00:00, 11.97s/it]100%|██████████| 15/15 [02:30<00:00, 10.03s/it]
100%|██████████| 15/15 [02:31<00:00, 11.89s/it]100%|██████████| 15/15 [02:31<00:00, 10.09s/it]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.70it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.79it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.64it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.27it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.44it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.52it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.55it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.00it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.63it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.65it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.62it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.04it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.20it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.47it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.46it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.79it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.49it/s]  3%|▎         | 1/30 [00:06<02:57,  6.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.54it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.65it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.46it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.07it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:05<02:38,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.36it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.33it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.79it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  3%|▎         | 1/30 [00:04<02:21,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.65it/s]  3%|▎         | 1/30 [00:02<01:26,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.63it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  7%|▋         | 2/30 [00:11<02:35,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:01<00:53,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.06it/s]
  7%|▋         | 2/30 [00:09<02:03,  4.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  7%|▋         | 2/30 [00:06<01:29,  3.21s/it]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.65it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.70it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.06it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s] 10%|█         | 3/30 [00:10<01:28,  3.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:07<01:52,  4.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.56it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.67it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.67it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.49it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.12it/s]
  7%|▋         | 2/30 [00:07<01:48,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:16<02:29,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:14<02:08,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:05<02:31,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.61it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.70it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.67it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.49it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.14it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:16<01:40,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:05<02:45,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:12<01:59,  4.41s/it] 13%|█▎        | 4/30 [00:15<01:42,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:11<01:54,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:07<01:27,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:21<02:19,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:04<02:18,  4.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:09<02:15,  4.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:20<01:36,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:18<01:30,  3.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:02<01:16,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:05<02:40,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:24<01:51,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:17<01:56,  4.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:04<01:00,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:16<01:52,  4.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:09<02:07,  4.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:11<01:45,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:24<01:28,  3.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:19<01:29,  3.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:14<02:09,  4.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:08<01:52,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:22<01:31,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:18<01:30,  3.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:08<01:16,  2.82s/it] 13%|█▎        | 4/30 [00:14<01:29,  3.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:29<01:49,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:21<01:16,  3.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:27<01:24,  3.66s/it] 10%|█         | 3/30 [00:12<01:53,  4.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:19<02:01,  4.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:24<01:09,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:13<01:58,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:27<01:34,  4.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:23<01:37,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:11<01:23,  3.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:34<01:45,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:16<01:45,  4.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:19<01:40,  4.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:31<01:24,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:27<01:08,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:23<01:55,  4.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:31<01:32,  4.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:18<01:57,  4.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:28<01:37,  4.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:16<01:37,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:38<01:40,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:24<01:40,  4.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:21<01:47,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:17<01:08,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:31<01:08,  3.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:36<01:28,  4.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:27<01:48,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:36<01:32,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:24<01:32,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:27<01:28,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:22<01:56,  4.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:32<01:34,  4.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:21<01:11,  3.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:40<01:20,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:35<01:12,  3.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:43<01:38,  4.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:39<01:18,  3.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:26<01:39,  4.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:35<01:24,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:33<01:51,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:31<01:27,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:28<01:32,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:46<01:23,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:26<01:21,  3.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:34<01:15,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:30<01:34,  4.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:41<01:23,  4.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:47<01:31,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:32<01:27,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:37<01:43,  4.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:40<01:26,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:45<01:30,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:39<01:19,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:33<01:22,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:31<01:24,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:52<01:28,  4.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:39<01:21,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:38<01:31,  4.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:43<01:14,  3.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:48<01:31,  5.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:37<01:21,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:53<01:36,  5.37s/it] 33%|███▎      | 10/30 [00:35<01:20,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:47<01:35,  5.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:52<01:35,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:58<01:30,  5.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:42<01:26,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:40<01:14,  3.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:45<01:31,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:49<01:27,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:53<01:32,  5.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:54<01:33,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:00<01:37,  5.75s/it] 37%|███▋      | 11/30 [00:41<01:31,  4.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:59<01:36,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:04<01:33,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:48<01:31,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:47<01:27,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:52<01:36,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:55<01:29,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:59<01:35,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:01<01:32,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:48<01:36,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:06<01:36,  6.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:05<01:35,  5.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:11<01:33,  5.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:54<01:36,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:53<01:33,  5.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:00<01:26,  5.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:59<01:37,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:06<01:34,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:07<01:30,  6.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:54<01:37,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:13<01:32,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:12<01:32,  6.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:17<01:29,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:01<01:37,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:04<01:30,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:00<01:35,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:07<01:28,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:12<01:31,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:14<01:27,  6.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:01<01:35,  5.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:20<01:28,  6.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:17<01:23,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:24<01:26,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:07<01:32,  5.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:11<01:29,  5.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:07<01:34,  5.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:14<01:27,  5.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:19<01:27,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:21<01:22,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:08<01:32,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:26<01:23,  6.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:24<01:20,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:29<01:17,  5.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:13<01:30,  6.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:19<01:20,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:17<01:26,  6.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:13<01:31,  6.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:26<01:22,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:27<01:17,  6.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:14<01:28,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:33<01:17,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:31<01:15,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:36<01:13,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:20<01:26,  6.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:26<01:17,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:24<01:21,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:20<01:27,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:32<01:17,  6.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:34<01:11,  6.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:21<01:23,  6.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:40<01:11,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:37<01:10,  6.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:43<01:09,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:26<01:21,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:32<01:14,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:31<01:16,  6.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:26<01:22,  6.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:39<01:11,  6.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:41<01:05,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:27<01:17,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:46<01:05,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:49<01:03,  6.32s/it] 67%|██████▋   | 20/30 [01:44<01:04,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:33<01:16,  6.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:39<01:09,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:37<01:09,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:33<01:17,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:45<01:04,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:47<00:59,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:34<01:11,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:53<00:59,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:56<00:57,  6.41s/it] 70%|███████   | 21/30 [01:50<00:58,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:40<01:10,  6.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:45<01:03,  6.39s/it] 67%|██████▋   | 20/30 [01:43<01:03,  6.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:40<01:11,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:43<00:55,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:52<00:58,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:54<00:52,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:41<01:05,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [02:00<00:52,  6.60s/it] 73%|███████▎  | 22/30 [02:02<00:51,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:57<00:52,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:52<00:58,  6.45s/it] 70%|███████   | 21/30 [01:50<00:58,  6.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:46<01:04,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:50<00:53,  5.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:58<00:52,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:01<00:46,  6.60s/it] 70%|███████   | 21/30 [01:47<00:58,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:06<00:46,  6.61s/it] 77%|███████▋  | 23/30 [02:09<00:45,  6.52s/it] 77%|███████▋  | 23/30 [02:04<00:45,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:59<00:51,  6.50s/it] 73%|███████▎  | 22/30 [01:57<00:51,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:53<00:58,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:56<00:48,  6.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:05<00:45,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:07<00:39,  6.61s/it] 73%|███████▎  | 22/30 [01:54<00:52,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:15<00:39,  6.54s/it] 80%|████████  | 24/30 [02:13<00:39,  6.62s/it] 80%|████████  | 24/30 [02:10<00:39,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:05<00:45,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:03<00:45,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:59<00:52,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:03<00:43,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:12<00:39,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:14<00:33,  6.62s/it] 77%|███████▋  | 23/30 [02:00<00:46,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:22<00:32,  6.57s/it] 83%|████████▎ | 25/30 [02:19<00:33,  6.63s/it] 83%|████████▎ | 25/30 [02:17<00:32,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:12<00:39,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:10<00:39,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:06<00:46,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:10<00:38,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:18<00:32,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:07<00:39,  6.60s/it] 87%|████████▋ | 26/30 [02:20<00:26,  6.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:29<00:26,  6.58s/it] 87%|████████▋ | 26/30 [02:23<00:26,  6.60s/it] 87%|████████▋ | 26/30 [02:26<00:26,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:18<00:32,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:16<00:32,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:13<00:39,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:16<00:32,  6.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:25<00:26,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:14<00:33,  6.60s/it] 90%|█████████ | 27/30 [02:27<00:19,  6.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:35<00:19,  6.59s/it] 90%|█████████ | 27/30 [02:30<00:19,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:33<00:19,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:25<00:26,  6.58s/it] 87%|████████▋ | 26/30 [02:23<00:26,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:19<00:33,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:23<00:25,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:31<00:19,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:20<00:26,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:34<00:13,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:42<00:13,  6.60s/it] 93%|█████████▎| 28/30 [02:37<00:13,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:39<00:13,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:32<00:19,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:30<00:19,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:26<00:26,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:29<00:19,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:38<00:13,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:27<00:19,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:40<00:06,  6.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:49<00:06,  6.61s/it] 97%|█████████▋| 29/30 [02:43<00:06,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:46<00:06,  6.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:38<00:13,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:36<00:13,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:32<00:19,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:36<00:13,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:45<00:06,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:34<00:13,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:47<00:00,  6.63s/it]100%|██████████| 30/30 [02:47<00:00,  5.58s/it]
100%|██████████| 30/30 [02:55<00:00,  6.61s/it]100%|██████████| 30/30 [02:55<00:00,  5.86s/it]
100%|██████████| 30/30 [02:50<00:00,  6.61s/it]100%|██████████| 30/30 [02:50<00:00,  5.68s/it]
100%|██████████| 30/30 [02:53<00:00,  6.64s/it]100%|██████████| 30/30 [02:53<00:00,  5.77s/it]
 97%|█████████▋| 29/30 [02:45<00:06,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:43<00:06,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:39<00:13,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:42<00:06,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:51<00:00,  6.62s/it]100%|██████████| 30/30 [02:51<00:00,  5.73s/it]
 97%|█████████▋| 29/30 [02:40<00:06,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:51<00:00,  6.59s/it]100%|██████████| 30/30 [02:51<00:00,  5.73s/it]
100%|██████████| 30/30 [02:49<00:00,  6.60s/it]100%|██████████| 30/30 [02:49<00:00,  5.66s/it]
 97%|█████████▋| 29/30 [02:46<00:06,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:49<00:00,  6.56s/it]100%|██████████| 30/30 [02:49<00:00,  5.65s/it]
100%|██████████| 30/30 [02:47<00:00,  6.62s/it]100%|██████████| 30/30 [02:47<00:00,  5.58s/it]
100%|██████████| 30/30 [02:52<00:00,  6.62s/it]100%|██████████| 30/30 [02:52<00:00,  5.76s/it]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.67it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.75it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.59it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.68it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.70it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.99it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.63it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.69it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.66it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.48it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.51it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.45it/s]  3%|▎         | 1/30 [00:04<02:12,  4.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.84it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.63it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.37it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.46it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.80it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
  3%|▎         | 1/30 [00:04<02:24,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.44it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.57it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.61it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.06it/s]
  7%|▋         | 2/30 [00:09<02:13,  4.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s] 10%|█         | 3/30 [00:10<01:21,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:05<02:48,  5.81s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.60it/s]  7%|▋         | 2/30 [00:09<02:06,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.65it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.03it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:05<02:25,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.65it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.70it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.67it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.49it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.14it/s]
 10%|█         | 3/30 [00:11<01:38,  3.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s] 13%|█▎        | 4/30 [00:15<01:39,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.54it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  7%|▋         | 2/30 [00:10<02:30,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.64it/s]  3%|▎         | 1/30 [00:05<02:53,  5.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.63it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.42it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.07it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:09<02:04,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.62it/s]  3%|▎         | 1/30 [00:05<02:47,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.46it/s] 17%|█▋        | 5/30 [00:18<01:30,  3.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.54it/s] 13%|█▎        | 4/30 [00:16<01:46,  4.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.95it/s]
 10%|█         | 3/30 [00:14<02:02,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:05<02:29,  5.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:10<02:25,  5.21s/it] 10%|█         | 3/30 [00:13<01:55,  4.27s/it]  3%|▎         | 1/30 [00:04<02:02,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:09<02:07,  4.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:17<01:43,  3.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:22<01:30,  3.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:15<01:31,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:09<02:10,  4.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:21<01:52,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:08<01:52,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:14<02:06,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:05<02:34,  5.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:08<01:06,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:04<02:18,  4.77s/it] 10%|█         | 3/30 [00:13<01:59,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:21<01:37,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:19<01:28,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:26<01:29,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:13<01:54,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:25<01:45,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:09<02:01,  4.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:19<01:58,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:12<01:22,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:29<01:18,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [00:08<01:59,  4.27s/it] 20%|██        | 6/30 [00:25<01:33,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:18<01:56,  4.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:31<00:59,  2.85s/it] 23%|██▎       | 7/30 [00:28<01:29,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:17<01:46,  4.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:10<01:25,  3.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:24<01:39,  4.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [00:13<02:01,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:23<01:55,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:21<01:42,  4.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:17<01:34,  3.77s/it] 27%|██▋       | 8/30 [00:32<01:21,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:29<01:33,  4.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:14<01:28,  3.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:21<01:44,  4.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:28<01:33,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:36<01:11,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:20<01:24,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [00:18<01:55,  4.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:35<01:16,  3.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:25<01:39,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:28<01:55,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:34<01:34,  4.29s/it] 23%|██▎       | 7/30 [00:22<01:08,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:25<01:39,  4.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:18<01:34,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:32<01:31,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [00:22<01:48,  4.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:31<01:35,  4.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:21<01:18,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:42<01:25,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:40<01:21,  4.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:30<01:39,  4.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:38<01:31,  4.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:29<01:33,  4.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:36<01:25,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:27<01:19,  3.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:34<01:23,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:24<01:13,  3.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [00:26<01:43,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:34<01:31,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:41<01:19,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:26<01:07,  3.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [00:33<01:31,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:48<01:28,  4.92s/it] 30%|███       | 9/30 [00:32<01:21,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:38<01:20,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:41<01:28,  4.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:47<01:32,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:37<01:22,  3.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [00:31<01:41,  4.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:36<01:16,  3.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:29<01:01,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:42<01:17,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:36<01:21,  4.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:48<01:30,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:41<01:19,  3.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:40<01:14,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:52<01:30,  5.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:55<01:32,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:34<01:09,  3.48s/it] 27%|██▋       | 8/30 [00:36<01:40,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:48<01:36,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:53<01:29,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:48<01:27,  4.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:43<01:32,  4.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:45<01:19,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:47<01:25,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [00:40<01:36,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:39<01:16,  4.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:59<01:33,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:02<01:32,  5.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:54<01:38,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:59<01:27,  5.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:54<01:28,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [00:45<01:35,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:51<01:25,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:49<01:36,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:54<01:32,  5.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:46<01:26,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:05<01:33,  5.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:08<01:30,  6.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:01<01:39,  5.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:06<01:29,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:01<01:31,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [00:51<01:38,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:57<01:27,  5.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:56<01:37,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:00<01:34,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:11<01:26,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [00:52<01:30,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:14<01:23,  5.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:07<01:37,  6.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:12<01:28,  5.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:07<01:31,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:03<01:26,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [00:58<01:41,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:17<01:19,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:02<01:35,  5.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [00:58<01:27,  5.46s/it] 47%|████▋     | 14/30 [01:07<01:34,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:20<01:19,  6.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:14<01:33,  6.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:18<01:21,  5.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:13<01:25,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:10<01:26,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [01:05<01:40,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:23<01:16,  5.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:13<01:28,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:09<01:32,  6.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:04<01:26,  5.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:26<01:13,  6.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:23<01:14,  5.74s/it] 53%|█████▎    | 16/30 [01:21<01:28,  6.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:19<01:23,  5.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [01:10<01:32,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:16<01:22,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:18<01:20,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:29<01:10,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:16<01:27,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:11<01:24,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:33<01:08,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:30<01:12,  6.00s/it] 57%|█████▋    | 17/30 [01:27<01:23,  6.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:21<01:14,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [01:16<01:28,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:26<01:20,  6.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:25<01:16,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:35<01:07,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:22<01:22,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:18<01:20,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:40<01:03,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [01:22<01:20,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:37<01:08,  6.18s/it] 60%|██████    | 18/30 [01:34<01:17,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:27<01:10,  5.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:32<01:14,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:31<01:13,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:42<01:01,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:29<01:17,  6.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:24<01:15,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:46<00:57,  6.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:40<01:10,  6.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [01:28<01:18,  6.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:43<01:03,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:34<01:06,  6.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:39<01:09,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:38<01:08,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:48<00:56,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:30<01:07,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:35<01:11,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:53<00:52,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:47<01:04,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [01:35<01:14,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:50<00:57,  6.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:40<01:02,  6.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:46<01:04,  6.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:44<01:01,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:55<00:50,  6.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:36<01:02,  6.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [01:42<01:04,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:00<00:45,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [01:41<01:06,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:53<00:58,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:56<00:51,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:47<00:57,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:52<00:58,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:50<00:56,  6.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:01<00:45,  6.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:43<00:57,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:48<00:58,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:06<00:39,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:57<00:46,  5.85s/it] 67%|██████▋   | 20/30 [01:47<01:01,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [02:00<00:52,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:03<00:45,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:54<00:51,  6.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:57<00:51,  6.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:08<00:39,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:50<00:51,  6.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [01:55<00:52,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:13<00:32,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:03<00:42,  6.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [01:54<00:56,  6.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:07<00:46,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:10<00:39,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:00<00:45,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:04<00:45,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:15<00:32,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [01:56<00:45,  6.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [02:02<00:45,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:20<00:26,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:10<00:37,  6.24s/it] 73%|███████▎  | 22/30 [02:00<00:51,  6.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:13<00:39,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:16<00:32,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:07<00:39,  6.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:05<00:33,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:10<00:39,  6.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:21<00:26,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [02:03<00:39,  6.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:26<00:19,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:16<00:31,  6.35s/it] 77%|███████▋  | 23/30 [02:07<00:45,  6.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:20<00:32,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:23<00:26,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:13<00:32,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:12<00:29,  5.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:17<00:32,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:28<00:19,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [02:09<00:32,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:33<00:13,  6.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:23<00:25,  6.44s/it] 80%|████████  | 24/30 [02:14<00:39,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:26<00:26,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:30<00:19,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:20<00:26,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:18<00:24,  6.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:23<00:26,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:35<00:13,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [02:16<00:26,  6.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:39<00:06,  6.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:30<00:19,  6.50s/it] 83%|████████▎ | 25/30 [02:20<00:32,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:33<00:19,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:36<00:13,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:27<00:19,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:25<00:18,  6.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:30<00:19,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:41<00:06,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [02:23<00:19,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:46<00:00,  6.64s/it]100%|██████████| 30/30 [02:46<00:00,  5.55s/it]
 93%|█████████▎| 28/30 [02:36<00:13,  6.53s/it] 87%|████████▋ | 26/30 [02:27<00:26,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:40<00:13,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:43<00:06,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:33<00:13,  6.58s/it] 93%|█████████▎| 28/30 [02:31<00:12,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [02:37<00:13,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:48<00:00,  6.61s/it]100%|██████████| 30/30 [02:48<00:00,  5.61s/it]
 93%|█████████▎| 28/30 [02:29<00:13,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:43<00:06,  6.57s/it] 90%|█████████ | 27/30 [02:34<00:19,  6.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:46<00:06,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:49<00:00,  6.62s/it]100%|██████████| 30/30 [02:49<00:00,  5.67s/it]
 97%|█████████▋| 29/30 [02:40<00:06,  6.58s/it] 97%|█████████▋| 29/30 [02:38<00:06,  6.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:43<00:06,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [02:36<00:06,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:44<00:00,  5.71s/it]100%|██████████| 30/30 [02:44<00:00,  5.47s/it]
100%|██████████| 30/30 [02:50<00:00,  6.59s/it]100%|██████████| 30/30 [02:50<00:00,  5.67s/it]
 93%|█████████▎| 28/30 [02:40<00:13,  6.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:53<00:00,  6.62s/it]100%|██████████| 30/30 [02:53<00:00,  5.78s/it]
100%|██████████| 30/30 [02:44<00:00,  6.47s/it]100%|██████████| 30/30 [02:44<00:00,  5.50s/it]
100%|██████████| 30/30 [02:50<00:00,  6.60s/it]100%|██████████| 30/30 [02:50<00:00,  5.68s/it]
100%|██████████| 30/30 [02:42<00:00,  6.59s/it]100%|██████████| 30/30 [02:42<00:00,  5.43s/it]
 97%|█████████▋| 29/30 [02:47<00:06,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [02:53<00:00,  6.60s/it]100%|██████████| 30/30 [02:53<00:00,  5.80s/it]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.73it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.83it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.58it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.26it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.60it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.59it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.67it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.50it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.12it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.46it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.56it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.59it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.98it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.33it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.53it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.58it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.93it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.26it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.45it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.53it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.20it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.87it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.57it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.70it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.76it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.66it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.24it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.43it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.58it/s]  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.86it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.15it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.50it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.44it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.91it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.69it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.64it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.64it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.95it/s]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.50it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.53it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.58it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.02it/s]
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 0/30 [00:00<?, ?it/s]/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:31<15:10, 31.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:37<18:06, 37.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:44<21:38, 44.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:45<21:55, 45.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:47<22:43, 47.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:54<26:11, 54.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:48<23:36, 48.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [01:00<29:07, 60.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:54<26:10, 54.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  3%|▎         | 1/30 [00:53<26:02, 53.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:22<18:58, 40.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:17<18:09, 38.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:22<20:09, 43.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:32<21:31, 46.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:29<20:46, 44.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:29<20:05, 43.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:37<22:24, 48.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:48<24:49, 53.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [01:54<17:06, 38.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:41<23:42, 50.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  7%|▋         | 2/30 [01:49<25:30, 54.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:03<18:59, 42.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:18<21:22, 47.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:08<18:54, 42.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:24<21:57, 48.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:14<19:49, 44.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:25<21:32, 47.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:36<22:47, 50.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:32<22:21, 49.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 10%|█         | 3/30 [02:33<23:11, 51.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [02:56<19:03, 43.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [02:52<19:55, 45.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [02:56<19:59, 46.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [02:50<18:13, 42.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [03:03<19:36, 45.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [02:57<18:51, 43.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [03:07<18:39, 43.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [03:12<19:50, 45.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [03:10<20:13, 46.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:28<16:46, 40.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:39<17:23, 41.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:37<18:32, 44.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 13%|█▎        | 4/30 [03:24<22:10, 51.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:35<17:16, 41.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:54<20:18, 48.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:48<17:39, 42.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:52<21:17, 51.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:52<18:13, 43.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [03:51<18:34, 44.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:12<15:32, 38.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 17%|█▋        | 5/30 [04:01<19:12, 46.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:15<16:22, 40.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:28<18:39, 46.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:21<17:54, 44.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:41<19:16, 48.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:35<19:16, 48.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:35<17:18, 43.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:41<18:20, 45.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:46<19:18, 48.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:05<16:36, 43.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:07<16:53, 44.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 20%|██        | 6/30 [04:51<18:59, 47.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:00<16:17, 42.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:09<17:36, 45.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:16<16:14, 42.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:21<16:59, 44.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:36<19:18, 50.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:31<19:27, 50.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [05:47<15:40, 42.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:36<18:44, 48.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [05:39<15:09, 41.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 23%|██▎       | 7/30 [05:37<18:01, 47.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [05:59<17:08, 46.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [05:58<17:10, 46.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [06:05<16:43, 45.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [06:12<17:05, 46.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [06:11<16:53, 46.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [06:24<14:21, 41.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [06:32<19:13, 52.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [06:18<16:31, 45.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [06:30<14:47, 42.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [06:40<15:46, 45.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [06:29<15:26, 44.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 27%|██▋       | 8/30 [06:35<19:07, 52.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [06:46<15:00, 42.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [06:50<15:48, 45.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:10<14:14, 42.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [07:04<16:51, 48.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [07:18<17:38, 50.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [06:58<15:14, 43.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:19<14:47, 44.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:32<15:41, 47.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:28<14:22, 43.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:31<14:30, 43.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:24<15:48, 47.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 30%|███       | 9/30 [07:24<17:52, 51.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:59<15:47, 47.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:47<15:29, 46.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [07:51<15:28, 46.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [08:10<15:07, 47.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 33%|███▎      | 10/30 [08:11<16:37, 49.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [08:50<17:56, 56.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [08:48<17:11, 54.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [08:49<18:26, 58.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [08:47<16:03, 50.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [09:01<18:16, 57.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [08:54<19:07, 60.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [09:29<19:07, 60.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [09:18<17:25, 55.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 37%|███▋      | 11/30 [09:21<18:54, 59.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [09:40<18:11, 60.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [09:54<18:02, 60.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [09:58<17:03, 56.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [10:20<20:00, 66.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [10:18<19:31, 65.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [10:19<20:20, 67.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [10:31<20:16, 67.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [10:26<18:25, 61.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [10:59<20:48, 69.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 40%|████      | 12/30 [10:45<19:24, 64.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [11:10<19:42, 69.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [11:10<17:25, 61.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [11:28<19:00, 67.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [11:19<19:14, 67.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [11:43<20:09, 71.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [11:49<21:05, 74.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [11:44<18:51, 66.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [12:02<21:04, 74.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [12:00<19:12, 67.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [12:20<18:35, 69.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 43%|████▎     | 13/30 [12:28<21:24, 75.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [12:33<17:43, 66.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [12:35<18:19, 68.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [12:33<18:32, 69.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [13:12<20:28, 76.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [13:18<21:04, 79.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [13:26<17:08, 68.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [13:14<19:36, 73.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [13:36<19:28, 73.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [13:32<21:06, 79.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 47%|████▋     | 14/30 [13:30<19:51, 74.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [14:02<18:21, 73.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [13:56<18:26, 73.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [14:05<18:46, 75.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [14:35<18:33, 74.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [14:39<19:57, 79.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [14:31<18:38, 74.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [14:37<18:04, 72.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [14:48<20:33, 82.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [14:56<17:30, 75.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 50%|█████     | 15/30 [15:05<19:31, 78.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [15:21<17:31, 75.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [15:26<17:56, 76.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [15:26<18:20, 78.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [15:35<16:21, 70.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [15:37<16:02, 68.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [15:41<17:06, 73.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [16:09<19:19, 82.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [16:25<18:17, 78.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 53%|█████▎    | 16/30 [16:18<19:42, 84.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [16:26<17:12, 79.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [16:28<15:41, 72.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [16:28<15:54, 73.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [16:50<17:08, 79.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [16:52<15:37, 72.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [17:14<16:44, 77.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [17:07<16:15, 75.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [17:06<16:37, 76.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [17:36<15:19, 76.63s/it] 57%|█████▋    | 17/30 [17:40<16:47, 77.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [17:31<14:05, 70.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [17:42<14:33, 72.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 57%|█████▋    | 17/30 [17:47<18:38, 86.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [18:09<14:43, 73.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [18:19<16:27, 82.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [18:12<14:42, 73.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [18:28<15:17, 76.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [18:51<13:58, 76.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [18:36<15:52, 79.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [18:49<15:43, 78.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [18:52<13:11, 71.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 60%|██████    | 18/30 [19:10<16:14, 81.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [19:01<13:58, 76.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [19:27<14:16, 77.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [19:39<14:24, 78.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [19:48<11:45, 70.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [19:33<13:52, 75.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [19:58<14:44, 80.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [20:00<14:00, 76.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [19:59<11:43, 70.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [20:18<14:11, 77.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [20:23<11:53, 71.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 63%|██████▎   | 19/30 [20:06<15:07, 82.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [20:26<13:10, 79.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [20:46<12:27, 74.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [21:09<13:40, 82.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [21:18<11:27, 76.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [21:20<13:30, 81.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [21:08<12:42, 76.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [21:28<13:17, 79.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [21:28<11:25, 76.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 67%|██████▋   | 20/30 [21:48<13:31, 81.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [21:53<11:31, 76.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [21:56<12:19, 82.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [22:15<11:53, 79.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [22:39<12:40, 84.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [22:48<10:44, 80.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [22:50<12:33, 83.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [22:38<12:02, 80.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [22:57<12:24, 82.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [22:58<10:42, 80.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 70%|███████   | 21/30 [23:18<12:33, 83.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [23:22<10:45, 80.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [23:26<11:15, 84.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [23:45<10:59, 82.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [24:10<11:29, 86.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [24:18<09:42, 83.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [24:20<11:24, 85.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [24:08<11:05, 83.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [24:27<11:18, 84.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [24:28<09:42, 83.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 73%|███████▎  | 22/30 [24:48<11:24, 85.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [24:52<09:43, 83.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [24:56<10:02, 86.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [25:15<09:52, 84.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [25:40<10:11, 87.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [25:48<08:31, 85.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [25:50<10:07, 86.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [25:37<09:56, 85.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [25:57<10:03, 86.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [25:58<08:30, 85.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 77%|███████▋  | 23/30 [26:18<10:08, 86.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [26:22<08:31, 85.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [26:25<08:43, 87.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [26:45<08:36, 86.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [27:10<08:48, 88.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [27:18<07:13, 86.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [27:19<08:46, 87.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [27:07<08:39, 86.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [27:26<08:43, 87.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [27:27<07:12, 86.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 80%|████████  | 24/30 [27:47<08:45, 87.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [27:51<07:12, 86.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [27:55<07:19, 87.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [28:14<07:16, 87.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [28:40<07:23, 88.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [28:48<05:50, 87.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [28:49<07:21, 88.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [28:37<07:17, 87.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [28:56<07:20, 88.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [28:57<05:50, 87.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 83%|████████▎ | 25/30 [29:17<07:21, 88.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [29:21<05:50, 87.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [29:25<05:54, 88.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [29:44<05:51, 87.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [30:10<05:56, 89.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [30:18<04:25, 88.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [30:19<05:55, 88.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [30:07<05:52, 88.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [30:26<05:54, 88.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [30:27<04:24, 88.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 87%|████████▋ | 26/30 [30:47<05:55, 88.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [30:51<04:24, 88.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [30:55<04:26, 88.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [31:14<04:25, 88.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [31:40<04:28, 89.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [31:48<02:57, 88.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [31:49<04:27, 89.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [31:37<04:26, 88.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [31:56<04:26, 88.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [31:57<02:57, 88.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 90%|█████████ | 27/30 [32:17<04:27, 89.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [32:21<02:57, 88.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [32:24<02:58, 89.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [32:44<02:57, 88.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [33:10<02:58, 89.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [33:18<01:29, 89.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [33:19<02:58, 89.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [33:06<02:58, 89.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [33:25<02:58, 89.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [33:27<01:29, 89.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 93%|█████████▎| 28/30 [33:47<02:58, 89.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [33:50<01:28, 88.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [33:54<01:29, 89.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [34:13<01:29, 89.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [34:40<01:29, 89.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [34:48<00:00, 89.52s/it]100%|██████████| 30/30 [34:48<00:00, 69.62s/it]
 97%|█████████▋| 29/30 [34:48<01:29, 89.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [34:36<01:29, 89.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
 97%|█████████▋| 29/30 [34:55<01:29, 89.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [34:57<00:00, 89.34s/it]100%|██████████| 30/30 [34:57<00:00, 69.92s/it]
 97%|█████████▋| 29/30 [35:17<01:29, 89.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
100%|██████████| 30/30 [35:20<00:00, 89.18s/it]100%|██████████| 30/30 [35:20<00:00, 70.68s/it]
100%|██████████| 30/30 [35:24<00:00, 89.48s/it]100%|██████████| 30/30 [35:24<00:00, 70.82s/it]
100%|██████████| 30/30 [35:43<00:00, 89.33s/it]100%|██████████| 30/30 [35:43<00:00, 71.45s/it]
100%|██████████| 30/30 [36:10<00:00, 89.84s/it]100%|██████████| 30/30 [36:10<00:00, 72.35s/it]
100%|██████████| 30/30 [36:18<00:00, 89.58s/it]100%|██████████| 30/30 [36:18<00:00, 72.63s/it]
100%|██████████| 30/30 [36:06<00:00, 89.45s/it]100%|██████████| 30/30 [36:06<00:00, 72.22s/it]
100%|██████████| 30/30 [36:25<00:00, 89.43s/it]100%|██████████| 30/30 [36:25<00:00, 72.84s/it]
100%|██████████| 30/30 [36:47<00:00, 89.62s/it]100%|██████████| 30/30 [36:47<00:00, 73.57s/it]
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-51:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-52:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-53:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-54:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-55:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-56:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-57:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-58:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-59:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-60:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-61:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-62:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-63:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-64:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-65:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-66:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-67:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-68:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-69:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-70:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-71:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-72:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-73:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-74:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-75:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-76:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-77:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-78:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-79:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-80:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-81:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-82:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-83:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-84:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-85:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-86:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-87:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-88:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-89:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-90:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-91:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-92:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-93:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-94:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-95:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-96:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-97:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-98:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-99:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-100:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-101:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-102:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-103:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-104:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-105:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-106:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-107:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-108:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-109:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-110:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-111:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-112:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-113:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-114:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-115:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-116:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-117:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-118:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-119:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-120:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-121:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-122:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-123:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-124:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-125:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-126:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-127:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-128:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-129:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-130:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
Evaluating on: ['qasper.jsonl', 'multifieldqa_en.jsonl', 'hotpotqa.jsonl', '2wikimqa.jsonl', 'gov_report.jsonl']
rm: cannot remove './pred_e/llama3-8b-instruct-8k-hash-K-10-L-55': No such file or directory
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-1:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-2:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-3:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-4:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-5:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-6:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-7:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-8:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-9:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-10:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-11:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-12:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-13:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-14:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-15:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-16:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-17:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-18:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-19:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-20:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-21:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-22:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-23:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-24:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-25:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-26:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-27:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-28:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-29:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-30:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-31:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-32:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-33:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-34:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-35:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-36:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-37:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-38:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-39:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-40:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-41:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-42:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-43:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-44:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-45:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-46:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-47:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-48:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-49:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-50:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-51:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-52:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-53:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-54:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-55:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-56:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-57:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-58:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-59:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-60:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-61:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-62:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-63:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-64:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-65:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-66:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-67:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-68:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-69:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-70:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-71:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-72:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-73:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-74:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-75:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-76:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-77:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-78:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-79:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-80:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-81:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-82:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-83:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-84:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-85:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-86:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-87:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-88:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-89:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-90:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-91:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-92:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-93:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-94:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-95:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-96:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-97:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-98:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-99:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-100:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-101:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-102:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-103:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-104:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-105:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-106:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-107:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-108:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-109:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-110:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-111:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-112:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-113:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-114:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-115:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-116:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-117:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-118:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-119:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-120:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-121:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-122:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-123:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-124:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-125:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-126:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-127:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-128:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-129:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Process Process-130:
Traceback (most recent call last):
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/zhuominc/anaconda3/envs/llm2/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 60, in get_pred
    model, tokenizer = load_model_and_tokenizer(model2path[model_name], model_name, device, args)
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/pred.py", line 120, in load_model_and_tokenizer
    from llama_sim import LlamaForCausalLM
  File "/home/zhuominc/MGP3/MagicPiG/LongBench/llama_sim.py", line 17, in <module>
    from transformers.cache_utils import Cache, SinkCache
ModuleNotFoundError: No module named 'transformers.cache_utils'
Evaluating on: []
