{"pred": "Facebook pages", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Yes.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "The baselines in the article are not explicitly mentioned. However, the authors propose a new method called Cell-aware Stacked LSTM (CAS-LSTM) and evaluate it on multiple benchmark datasets.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "The article discusses the use of cognitive features extracted from eye-tracking data to improve sarcasm detection in NLP processing systems. The authors created a novel framework that combines traditional linguistic features with cognitive features obtained from readers' eye-movement data, resulting in an improved success rate for sarcasm detection. The article provides examples of how the system performs on different sentences and compares its classification accuracy to the best available system. The authors also discuss the limitations of the approach and suggest future work in exploring deeper graph and gaze features.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The article discusses the use of neural models to predict instructor intervention in MOOC forums. The models use a thread structure and the sequence of posts to make predictions. The article proposes attention models to infer the latent context of the thread, which is defined as a series of posts that trigger an intervention. The article also discusses the use of context in discussions and the role of structure and sequence in predicting intervention. The article evaluates the performance of the proposed models over a diverse dataset of 12 MOOC iterations on Coursera.org.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Pivot-based baselines are methods that translate source language to a pivot language, and then translate the pivot language to the target language. This approach is commonly used in transfer learning for neural machine translation (NMT).", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The article does not provide specific information on the datasets used in evaluation. It only mentions that the experiments were conducted on four languages, which were part of SemEval-2016 Challenge Task 5. The article does not provide any details on the specific datasets used for evaluation. Therefore, the answer to this question is \"unanswerable\".", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "Unanswerable.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "Unanswerable.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "They use datasets with transcribed text.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "Yes.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Unanswerable.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention used in a Chinese word segmentation model. It captures localness and directional information by adding a directional mask and a Gaussian mask to the standard self-attention mechanism. The model uses only unigram features and achieves new state-of-the-art performance on the SIGHAN Bakeoff 2005 benchmark dataset.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The article describes the DeepMine database, a large speech corpus collected using crowdsourcing and designed for text-dependent and text-prompted speaker verification, as well as text-independent speaker verification evaluation. The database contains speech data from 1969 speakers, with 1149 male and 820 female speakers, and includes fixed common phrases, random sequences of words, and phrases with word- and phoneme-level transcription. The article provides details on the data collection process, including the use of an Android application and post-processing steps to", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "The article mentions two datasets, EmotionLines and Friends. EmotionLines is a dialogue dataset composed of two subsets, Friends and EmotionPush, and includes $1,000$ English dialogues, each divided into a few consecutive utterances. The dataset is annotated by five annotators on a crowd-sourcing platform, and the labeling work is based on the textual content. Friends is a speech-based dataset that includes $1,000$ English dialogues from the TV sitcom Friends, and EmotionPush is a chat-based dataset that includes $1", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The article describes the use of a Recurrent Neural Network (RNN) for end-to-end speech recognition.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "The baseline is a neural network-based Named Entity Recognition (NER) system for the Nepali language, which uses grapheme-level representations and POS one-hot encoding vectors.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "The article discusses the automatic summarization of discharge summary notes, which are critical to ensuring continuity of care after hospitalization. The authors explore the upper bound on extractive summarization and develop a classifier for labeling the topics of history of present illness notes. They use the MIMIC-III database, which contains electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012. The dataset contains 55,177 discharge reports and 4,475 discharge", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Attention is different from alignment in cases where the attention model uses the most relevant parts of the source sentence at each translation step, and where the attention mechanism captures information beyond alignment in the case of verbs.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "Unanswerable.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "BERT", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "The article describes the use of a hierarchical three-level annotation model to create a new large publicly available dataset of English tweets. The article also discusses related work on abusive language identification, including aggression identification, bullying detection, hate speech, toxic comments, and offensive language. The article proposes a new hierarchical annotation model that captures the similarities among prior work on abusive language and distinguishes between whether the language is directed towards a specific individual or entity or towards a generalized group and whether the abusive content is explicit or implicit. The article also describes the use of", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "The article does not provide information on the length of the dataset for each step of the hierarchy.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "The article proposes a neural-based Named Entity Recognition (NER) model for Nepali language using the latest state-of-the-art architecture. The model uses grapheme-level features and does not require any hand-crafted features or data pre-processing. The authors compare their model with various baselines, including Support Vector Machine (SVM) and Hidden Markov Model (HMM) with rule-based approaches, and show that their model performs better in terms of entity recognition accuracy. They also release a dataset to support future research and perform empirical evaluation of their model with state-of-", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Yes.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "The article discusses the use of tag recommendations to annotate e-books, specifically focusing on the use of editor tags and user-generated search terms from Amazon to improve the quality of tag recommendations. The authors evaluate various tag recommendation approaches, including popularity-based, similarity-based, and hybrid approaches, using a dataset containing editor tags, Amazon search terms, and Amazon review keywords. The study finds that combining both data sources enhances the quality of tag recommendations and that approaches that train on Amazon search terms provide poor performance in terms of accuracy but deliver good results in terms of semantic similarity and recommendation diversity.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "Unanswerable.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The article does not provide a specific architecture for the models mentioned. It only mentions that they use an end-to-end approach and provide baseline results for various language pairs.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "Attention captures useful information beyond traditional word alignment, such as syntactic and morphological information, which contributes to the translation quality.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The article does not mention a specific baseline method.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The more predictive set of features to detect fake news is a combination of words embeddings, style, and morality features.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The baselines were BOW, TFIDF, and various neural-based word embedding methods, as well as Logistic Regression, Random Forest, TextCNN with initial word embedding as GloVe, and the proposed model.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The article does not provide specific information on the data used by the authors. However, it mentions that the Pyramidal Recurrent Unit (PRU) was evaluated on the Penn Treebank (PTB) and WikiText2 (WT-2) datasets.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The article does not mention which datasets are used for evaluation. Therefore, the answer is \"unanswerable\".", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "The article does not provide information on the size of the collection of COVID-19 literature.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The ancient Chinese dataset comes from the parallel corpus crawling and cleaning, paragraph alignment, clause alignment based on aligned paragraphs, and augmenting data by merging aligned adjacent clauses.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The proposed model is compared to several baseline models, including traditional features such as TF-IDF, as well as deep features such as Doc2vec, LSTM, and ELMo.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Unanswerable.", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Macaw has a modular design with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "The article does not mention the specific datasets used for experiments.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The article describes experiments in finding out whether lemmatization helps modern contextualised embeddings, specifically the ELMo model, on the example of English and Russian languages. The authors trained their own ELMo models from scratch on English and Russian corpora, consisting of the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018, respectively, with the RNC added to the English corpus. The texts were tokenized and lemmatized with the UDPipe models for the respective languages trained on the Universal Dependencies 2.3 treebanks. The ELMo models", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "Yes.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The encoder in the system described in the article is an LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "The article proposes a neural probabilistic model that captures word embeddings in a lower dimensional space, considering both the word's semantic similarity and its context. The model is trained using a variant of max-margin objective based on the asymmetric KL divergence energy function. The article also presents a KL approximation between Gaussian mixtures to capture textual entailment. The proposed approach is evaluated on benchmark word similarity and entailment datasets, showing better performance than existing models.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "Unanswerable.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "The article discusses the relation classification task, where the goal is to assign sentences with two marked entities to a predefined set of relations. The authors propose using neural networks (NNs) for this task, specifically convolutional neural networks (CNNs) and recurrent neural networks (RNNs), and present a new context representation for CNNs. They also introduce a ranking loss function for RNNs and combine CNNs and RNNs using a voting scheme. The article is based on the SemEval 2010 shared task 8 dataset, which consists of manually annotated data for relation classification.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The article describes a scientific article that proposes a new task of generating plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences. The authors release a new dataset of 180K+ recipes and 700K+ user reviews for this task and introduce new evaluation strategies for generation quality in instructional texts. They explore a hierarchical approach that condition", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "Unanswerable.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The article describes a new dataset called ReviewQA, which is a question-answering dataset for hotel reviews. The dataset contains 8 competencies that evaluate the reader's ability to understand and reason with relevant information extracted from a document. The questions are natural language questions over the 500 most popular articles of Wikipedia, and the answers are spans of text extracted from the source documents. The dataset is designed to challenge models to perform strongest reasoning tasks.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Unanswerable.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "The Random Kitchen Sink (RKS) approach is a method used in natural language processing tasks, including offensive language detection, by explicitly mapping data vectors to a space where linear separation is possible. It provides an approximate kernel function via explicit mapping and has been explored for sentiment analysis, background-foreground separation, load forecasting, and saliency detection.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Unanswerable.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "The results show that error detection performance is improved by using artificially generated data created by the proposed methods. The combination of the pattern-based approach with the machine translation system gave the best overall performance on all datasets.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "They compare their approach to a baseline model that does not use pre-trained language model representations.", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The article describes an experiment in which the authors incorporated subword information into word embedding models and evaluated the resulting word embeddings using intrinsic and extrinsic evaluations. The intrinsic evaluations consisted of word similarity and word analogy tasks, while the extrinsic evaluations included downstream tasks from the VecEval suite. The authors found that incorporating subword information into the LexVec model resulted in similar improvements as observed in moving from Skip-gram to fastText, and that unsupervised morphemes offered no advantage over n-grams. They also found that subword information improved syntactic results while pres", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "The article does not provide a clear answer to this question. It mentions that the dataset was collected from 35 mobile applications and their privacy policies, but it does not provide any information about the experts who were used for annotation. It is possible that the authors of the article did not include this information in the final version of the article, or that the information was not available at the time of writing. Therefore, the question is unanswerable.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Yes.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "The article does not mention which baseline model is used.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "They addressed cyberbullying on social media platforms such as Formspring, Twitter, and Wikipedia.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "Unanswerable.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "The article describes the creation of a corpus for training machine learning models, the development of a gold-standard test corpus, and the evaluation of established approaches for named entity recognition in the Armenian language. The authors used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. They also developed GloVe word embeddings and baseline results for 3 different models on the proposed benchmark data set. The article does not mention any specific named entity recognition models.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "IMDb dataset", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "The article does not mention which datasets are used to evaluate the proposed method.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "The article does not provide information on the sources of the datasets used in the study.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "The article does not provide information on the performance of the system.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "The article discusses the challenges of language identification (LID) in South Africa, where there are 11 official languages. The authors propose a hierarchical naive Bayesian and lexicon-based classifier for LID of short pieces of text. The algorithm is evaluated against recent approaches using existing test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) 2015 and 2017 shared tasks. The article also reviews existing works on the topic and summarizes the remaining research problems. The authors suggest that data harvesting, building", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "Unanswerable.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "PolyResponse is a restaurant search and booking system that is currently available in 8 languages: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The best performing model among the author's submissions is ranked 3rd in the Fragment-level Classification (FLC) task with a score of 0.673 on the test set.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The article describes a study that uses a dataset of Twitter tweets annotated with depression-related classes. The dataset is constructed based on a hierarchical model of depression-related symptoms, and it contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. The dataset is encoded with 7 feature groups with associated feature values binarized to represent potentially inform", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "Yes.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "The article proposes the use of context tweets, which provide additional information about the context in which an abusive tweet was made, to improve the accuracy of abusive language detection. The article also explores the use of various features and models, including traditional machine learning classifiers and neural network models, to detect abusive language.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The new context representation is proposed in the article and is based on the idea of using all parts of the sentence, including the relation arguments, left of the relation arguments, between the arguments, and right of the arguments, and paying special attention to the middle part.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The Adversarial-neural Event Model (AEM) outperforms K-means, LEM, and DPEMM in event extraction from online texts.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "KNN, RF, SVM, and MLP have been trained.", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19, which consists of subparts of DTA corpus and contains texts from the 16th to the 20th century.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Unanswerable.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "InferSent, Universal Sentence Encoder, and SBERT are evaluated in the article.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "The article does not mention the specific metrics used for evaluation.", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "Multi-granularity and multi-tasking neural architecture design involve combining different levels of granularity and different tasks in a single model, allowing for more efficient and effective propaganda detection.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "The size of the TweetQA dataset is not specified in the article.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "The article discusses various approaches to grammar induction, including probabilistic context-free grammars (PCFGs) and compound PCFGs. PCFGs are grammars with rule probabilities that are specified by a probabilistic grammar. Compound PCFGs are a type of PCFG that use a compound probability distribution to model the generative process. The article states that the compound PCFG approach is able to induce linguistically meaningful grammars by optimizing log likelihood, and that this can be done through a collapsed approach using amortized inference", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The data in the article is in the Europarl language pair, which consists of English (INLINEFORM0 and INLINEFORM1) and French.", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "The backoff strategies in the article involve falling back on a generic word recognizer trained on a larger corpus when the word recognizer used in the main defense model encounters unobserved and rare words. The goal is to handle these unseen words and improve the defense model's robustness against adversarial attacks.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "Two metrics are proposed to evaluate keyphrase generation: F1 score and diversity of generated sequences.", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The CORD-19 dataset is a collection of scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. It contains over 45,000 articles contributed by hospitals and medical institutes all over the world and is used to study the correlation between radiological findings and COVID-19.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "The state-of-the-art models for automatic judgment prediction in civil cases are based on the Legal Reading Comprehension (LRC) framework, which incorporates the reading mechanism to better model the complementary inputs of fact descriptions, plaintiff's pleas, and related law articles. The LRC framework has been shown to improve the performance of judgment prediction compared to previous text classification-based approaches.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "Unanswerable.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "No.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "The Neural User Simulator (NUS) is introduced, which outputs natural language and learns from a corpus of recorded dialogues.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "The article discusses the use of automatic diacritic recovery in Arabic language processing, specifically focusing on the recovery of both core-word diacritics (CW) and case-endings (CE). The authors propose two separate Deep Neural Network (DNN) architectures for recovering both types of diacritic types, using character-level and word-level bidirectional Long-Short Term Memory (biLSTM) based recurrent neural models for CW diacritic and CE recovery respectively. They train models for both Modern Standard Arabic (MSA) and Classical Arabic (CA", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "Disinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "The article discusses the use of natural language processing (NLP) and machine learning (ML) techniques for detecting online hate speech. The authors propose a transfer learning approach using the pre-trained language model BERT and introduce new fine-tuning strategies to improve the performance of hate speech detection. The article also discusses previous studies on online hate speech and offensive language, and transfer learning. The authors conclude that their model outperforms previous works and can detect some biases in the process of collecting or annotating datasets.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "The article describes a method for performing both grammatical error correction and style transfer with a single trained model, using zero-shot neural machine translation. The model is trained on bilingual examples in both directions, and then used to translate the input into the same language for both tasks. The success of the model is evaluated in terms of performance in grammatical error correction and style transfer, and the model is shown to be effective in correcting errors and adapting the output into a desired style. The approach is applicable to multiple languages within the same model, and the paper presents thorough quantitative and qualitative manual evaluation of the model", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "The article discusses a method to learn more interpretable dense word embeddings by modifying the objective function of the GloVe algorithm. The proposed method aligns the dimensions of the embedding vector with predefined concepts, resulting in an increase in interpretability. The article demonstrates the effectiveness of the method through quantitative and qualitative evaluations and shows that the semantic structure of the original embedding is not harmed. The method can be applied to other word embedding algorithms with a similar learning routine. The proposed methodology can also be helpful in computational cross-lingual studies.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "The baseline is not mentioned in the article.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The article proposes a new structured-data encoder that assumes the data structure should be hierarchically captured. The decoder is chosen to be a classical module as used in previous models. The encoder introduces a hierarchical attention mechanism to compute the hierarchical context fed into the decoder. The experiments are conducted on the RotoWire benchmark, which contains around $5K$ statistical tables of NBA basketball games paired with human-written descriptions. The proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics. The article also mentions that the proposed approach", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "The document-level encoder proposed in the paper is based on the pretrained Bert model, which is trained on a large corpus of text and uses a masked language modeling objective. The encoder is designed to encode a document and obtain representations for its sentences. This is a novel approach in text summarization, as previous models have focused on sentence-level encodings. The document-level encoder is a key component of the proposed summarization models, which achieve state-of-the-art results in both extractive and abstractive summarization tasks.", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "The article discusses the use of reinforcement learning to solve text-based games. The authors propose a methodology that uses the Go-Explore algorithm to extract high-performing trajectories in these games, and then trains a Seq2Seq model on these trajectories to play the game. The experiments are performed on the CoinCollector game and the \"First TextWorld Problems\" dataset, which consists of 4,440 games generated using the Z-Machine. The results show that the proposed methodology is effective in solving text-based games, with the Seq2Seq model trained on Go-", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "The article proposes a new approach that enables learning for different manipulation schemes with the same single algorithm. The authors adapt an off-the-shelf reward learning algorithm from the reinforcement learning literature for data manipulation in the supervised setting. The algorithm is used to learn data augmentation and weighting, and it shows improved performance over strong base models and previous manipulation methods.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "Unanswerable.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "The state of the art models for sarcasm detection are unanswerable.", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "The additive modification to the objective function is the introduction of an additional term that favors an increase in the value of the embedding vector dimension corresponding to the concept that the particular word belongs to.", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "Improvement over the best performing state-of-the-art is not mentioned in the article.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "The article discusses a scientific article that introduces a new model called the Transformer, which is a sequence-to-sequence (seq2seq) model used in natural language processing (NLP). The Transformer uses multi-head attention mechanisms to map an input sequence to an output sequence, allowing for the dynamic, context-dependent propagation of information within and across sentences. The article also introduces a new model called the \"sparse attention\" Transformer, which allows for the attention to be limited to a contiguous span of past tokens, rather than the entire sequence. The article concludes with a discussion of", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "Yes.", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "Yes.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "Reuters database", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "The article discusses the use of RNNs in Statistical Machine Translation (SMT) and their application to morphologically rich languages. The use of RNNs has revolutionized the approaches to automated translation, but the main downside is the heavy corpus requirement. The efficiency of an MT system depends on the availability and size of parallel corpora, and the syntactic divergence between the languages. Morphologically rich languages are difficult to translate due to the structural and semantic discordance with English. The article also discusses the use of word2vec and morph2vec models for", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "The article discusses the use of a deep learning framework for detecting fake news on social media. The authors propose a classification framework based on a multi-layer formulation of Twitter diffusion networks, which allows them to disentangle different social interactions on Twitter, such as tweets, retweets, mentions, replies, and quotes, to build a diffusion network composed of multiple layers. They compute structural features separately for each layer and use a logistic regression model to classify news articles as either credible or non-credible. The article presents experimental results showing that the use of a multi-layer, disentang", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "The model is more reliable for spelling, word order, and grammatical errors in grammatical error correction. It is less reliable for lexical choice errors.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "The article discusses the use of computational approaches to assist human moderators in online conversation platforms. The model introduced in the article is capable of forecasting the derailment of a conversation based on early warning signs, and can capture the dynamics of a conversation as it develops. The article also discusses the challenges of forecasting conversational events, including the need to capture inter-comment dynamics and deal with an unknown horizon. The article concludes with a discussion of future work, including the need to address questions about the usefulness of early warnings to moderators and the ethical implications of such a system.", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "The article does not mention the specific datasets used in the experiment.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "Yes.", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "Unanswerable.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "The article describes a scientific competition called BioASQ, which is a biomedical document classification, document retrieval, and question answering competition. The competition is in its seventh year and the article describes the system used by the authors to participate in the question answering task. The system is based on BioBERT, a biomedical version of the BERT model, and is fine-tuned for the biomedical question answering task. The article describes the results of the system in the competition and also presents related work on question answering tasks using different models.", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "The baseline is a reference point or starting point for comparison in an experiment or analysis. In the context of the article, the baseline refers to the initial performance of the models before any improvements are made. The article does not provide specific details on the baseline performance of the models, as the focus is on the improvements achieved through various techniques.", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "The paper proposes to use a joint model that combines textual features with visual features to assess the quality of documents. The visual features are based on a fine-tuned Inception V3 model, while the textual features are based on a hierarchical biLSTM. The joint model is trained on a Wikipedia dataset and an arXiv dataset, and achieves state-of-the-art results in both datasets.", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "Yes.", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "Improved KB Relation Detection", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The article describes a scientific competition called BioASQ, which involves submitting answers to biomedical questions. The authors state that their system scored near the top for factoid questions in the third test batch set and achieved the highest recall score for List-type questions in the fourth test batch set. They also mention that they used a BioBERT BIBREF0 based system, which is a contextual word embedding model fine-tuned for the biomedical question answering task. The article also describes related work on question answering tasks, including the use of BERT BIBREF1, a contextual word embedding model,", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Diversity of keyphrases is typically measured by evaluating the number of unique keyphrases generated by a model, as well as the diversity of the generated sequences. One common metric for measuring diversity is the F1 score, which takes into account both precision and recall.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "The article discusses the effectiveness of Rouge, a widely used metric in text summarization evaluation, for assessing the quality of scientific summaries. The study reveals that Rouge scores do not correlate well with manual Pyramid scores, challenging its effectiveness for scientific summarization. The article proposes an alternative metric, Sera, based on relevance analysis, which shows higher and more consistent correlations with manual judgments.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "Yes.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "KAR is an end-to-end MRC model that utilizes general knowledge to assist its attention mechanisms.", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "The article describes a study on neural machine translation (NMT) and how the models decide the importance of output words. The study uses a gradient-based approach to estimate word importance by exploiting the intermediate gradients in NMT generation. The word importance is useful for understanding NMT by identifying under-translated words, and the gradient-based approach is found to be superior to other methods in estimating word importance. The study also analyzes the linguistic behaviors of words with importance and finds that certain syntactic categories have higher importance while the categories vary across language pairs.", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "The article discusses the use of dedicated question answering (QA) systems to address the challenge of personalization in large-scale information retrieval. The authors propose a new approach called Recognizing Question Entailment (RQE) to retrieve answers to a premise question by retrieving inferred or entailed questions. The effectiveness of the RQE-based QA approach is evaluated using open-domain and clinical datasets. The authors also present two machine learning and deep learning methods for RQE and compare their performance. The article concludes with a discussion of the limitations of the current approach and future", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The accuracy merits of the approach are demonstrated through empirical evaluation on multiple real-world datasets, showing an average improvement of 24.3% in AUC compared to the state of the art.", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "The article does not provide the size of the real-world civil case dataset.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "The article mentions several publicly available datasets that have been used in previous studies on online hate speech and offensive language detection, including Twitter, Reddit, YouTube, and Wikipedia. However, it does not provide specific details on which datasets are used in the proposed transfer learning approach.", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "Unanswerable.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "The authors used twitter to build a large-scale dataset of ironic and non-ironic sentences for their study on irony generation.", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "The proposed task of concept-map-based MDS is defined as follows: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. The evaluation of this task involves creating a new dataset, developing a crowdsourcing scheme, and providing an annotation tool. The concept map is defined as a labeled graph showing concepts as nodes and relationships between them as edges. The task is complex and consists of several interdependent subtasks, including extracting appropriate labels for concepts and relations, recognizing different expressions that refer to the same concept across multiple documents, selecting the", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "Yes.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "They compared the performance of various LSTM models, including layer-wise trained models, transfer learning models, and models trained with different loss functions and optimization techniques.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "Future work will investigate the applications of dynamic balancing methods between RL and MLE in text generation, and improve the sensationalism scorer.", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "The article reports several evaluation metrics, including accuracy, F1 score, and performance improvement over previous methods.", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The evaluation criteria and metrics used to evaluate the generated text include factual correctness, fluency, and product-readiness. The factual correctness was measured by the number of factual errors in the generated text compared to the corrected variant, and the fluency was measured by the word error rate (WER) of the generated text compared to its corrected variant. The product-readiness was evaluated by two journalists from the STT agency, who edited the generated text to a form ready for production use or direct publication.", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the hate speech detection task and comparing them to the annotations made by human annotators. They found that the model was able to correctly identify some samples that were misclassified by the annotators, indicating that it was able to capture some of the biases present in the data.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "The article proposes Human Level Attributes (HLAs) as a way to model character profiles in dialogue agents. HLAs are based on tropes collected from TV Tropes and are used to trace dialogue data back to both its context and associated human-like qualities. The authors also propose a system called ALOHA that maps characters to a latent space based on their HLAs and determines which are most similar in profile and identity, allowing for the recovery of language styles of specific characters. The article demonstrates the effectiveness of ALOHA in character language style recovery and shows that it is stable across", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "The article does not provide the absolute accuracy of the system.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "The strong baseline is Seq2Seq with attention.", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "On the SimpleQuestions and WebQuestions benchmarks, they achieve the state of the art.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "The data was collected through crowdsourcing on Amazon Mechanical Turk, with workers asked to formulate questions and answers based on script scenarios rather than specific texts.", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "They show there is space for further improvement by conducting a human study and finding that a majority of questions the system couldn't answer were actually answerable. This suggests that the system is not yet achieving its full potential and there is still room for improvement.", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "KBQA stands for Knowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "The article discusses a novel approach to extractive summarization for long documents, which captures both global and local context. The global context refers to the overall structure of the document, while the local context refers to the specific topic being discussed within a section or subsection of the document. The proposed model integrates recent findings on neural extractive summarization and incorporates a parameter-lean and modular architecture. It is evaluated on two large scientific paper datasets and achieves state-of-the-art performance. The benefits of the model come exclusively from modeling the local context, even for the longest documents.", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "Unanswerable.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The article does not provide information on which stock market sector achieved the best performance.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "The authors show that their learned policy generalizes better than existing solutions to unseen games by training a Seq2Seq model on the high-performing trajectories found by the Go-Explore algorithm, which allows the agent to operate in the full, unconstrained action space of natural language and systematically generalize to new text-based games with no or few interactions with the environment.", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "The baseline model used in the BioASQ 7 competition is a BioBERT BIBREF0 based system that is fine-tuned for the biomedical question answering task. The system scored near the top for factoid questions in all test batches and achieved the highest 'MRR' score for Factoid Question Answering task in the third test batch set.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "Unanswerable.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Unanswerable.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The state-of-the-art system is not mentioned in the article.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "The article mentions that several real-world datasets are used for the experiments, including CyberAttack and PoliticianDeath.", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "The state of the art described in the paper is the development of a hybrid conceptual architecture and its implementation with a finance advisory system for multi-party conversational systems (MPCS). The paper also discusses the challenges of engineering MPCS and explores the possibility of decoupled interaction norms specification and a multi-party governance service to enforce compliant utterances.", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "The article does not mention the type of classifiers used.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "The proposed model, ALOHA, outperforms the baselines by a noticeable margin, achieving a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The article discusses the use of monolingual data in Neural Machine Translation (NMT) systems and proposes various techniques to integrate such data. The study focuses on the Europarl domain, which consists of two language pairs: English-German and English-French. The NMT systems are trained on out-of-domain parallel corpora and limited in-domain data. The article presents several baseline NMT systems and compares their performance with and without the use of monolingual data. The study also explores the effects of Back-Translation (BT) and the use", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "The article describes the use of neural machine translation (NMT) for the low-resourced language pair of Japanese INLINEFORM0 Vietnamese. The authors attempt to build NMT systems for this pair and conduct experiments using advanced methods to improve the quality of the systems. They use subword units, byte-pair encoding (BPE), and a variant of BPE for Vietnamese, and experiment with different segmentation methods for both languages. The article also describes the use of back-translated texts and mix-source data to increase the amount of training data. The main contributions of the paper are the description of the", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "The data in the new corpus is sourced from various domains and registers, including comments to articles, discussion forum posts, blog posts, and professional newswire articles. The data come from a variety of sources and no assumptions about its actual content with respect to argumentation can be drawn.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "They build a predictive model of dogmatism using statistical analysis of linguistic features in social media posts.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to understand the impact of gender representation in media on speech recognition systems and to question the fairness of these systems in terms of gender.", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "The model is applied to two datasets: the highly curated `Conversations Gone Awry` dataset and the in-the-wild moderation of the popular subreddit ChangeMyView.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "The paper explores various embedding techniques, including word embeddings and second-order co-occurrence vectors.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The article discusses the challenges of engineering multi-party conversational systems (MPCS) and presents a hybrid conceptual architecture and its implementation with a finance advisory system. The authors are evolving the architecture to support decoupled interaction norms specification and developing a multi-party governance service to enforce exchange of compliant utterances. They are also exploring a micro-service implementation of SABIA to increase its scalability and performance. The article does not provide any specific evaluation metrics for the system.", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "A second order co-occurrence matrix is a measure of semantic similarity and relatedness that incorporates pairwise similarity scores derived from a taxonomy, reducing noise by selecting only the most semantically similar co-occurrences.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "The article describes a study on neural machine translation (NMT) and how to better understand and improve NMT models. The study focuses on exploiting gradients in NMT generation to estimate word importance better. The authors find that word importance is useful for understanding NMT by identifying under-translated words, and they provide empirical support for the design principle of NMT architectures. The study also analyzes the linguistic behaviors of words with importance and shows potential to improve NMT models. The authors contribute to the field by demonstrating the necessity and effectiveness of exploiting intermediate gradients for estimating word", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "This work accounts for various argumentation phenomena in actual data, including refutation, rebuttal, and rhetorical questions.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "The size of the dataset is not mentioned in the article.", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "The article discusses the use of BERT, a deep bidirectional transformer architecture, for natural language processing tasks, particularly in the context of imbalanced classification. The authors participated in a shared task on fine-grained propaganda detection and achieved second place on sentence-level classification. They also addressed the issue of class imbalance and cost-sensitivity in BERT, which enables models to adapt to dissimilar datasets. The article provides statistical methods for establishing dataset similarity and discusses the challenges of automated propaganda detection.", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "The baseline methods are not explicitly mentioned in the article.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "Yes.", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "The article describes a study on the use of crowdsourcing platforms like Amazon Mechanical Turk to collect a sufficiently large corpus of annotated natural language examples in specialized domains like biomedicine. The study focuses on the prediction of annotation difficulty and the use of lay annotators and expert annotators to examine the quality of annotations. The article does not provide any information on how the annotators are matched to the instances.", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "The article does not provide information on the size of the data set used in the study.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The article discusses a new approach to building chatbots that can learn and improve their conversation ability over time. The approach is based on the idea of open-world knowledge base completion (OKBC), which allows chatbots to learn new facts and knowledge from user inputs even if they are not explicitly stated. The authors propose a lifelong interactive learning and inference (LiLi) approach to solve the OKBC problem, which involves using reinforcement learning to formulate query-specific inference strategies and then executing them to solve the problem by interacting with the user. The effectiveness of the LiLi approach is evaluated through", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "ROGUE", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "The article does not mention the two large-scale datasets used.", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "No.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "The article discusses the use of computational methods to study social and cultural phenomena, with a focus on the example of eliminating \"echo chambers\" on Reddit. The authors highlight the opportunities and challenges of using text data for such analyses, and provide a general overview of the research process involved. They aim to shed light on thorny issues, provide best practices for working with thick social and cultural concepts, and promote interdisciplinary collaborations. The authors have diverse backgrounds in computer science, social sciences, and humanities.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "The article discusses the challenges and limitations of argumentation mining, a field that aims to analyze and understand argumentation in user-generated content on the Web. The researchers propose a new corpus of annotated data and experiment with various machine learning methods to identify argument components. They find that the best-performing system achieves a macro-F1 score of 0.30 using Krippendorff's inter-annotator agreement measure, which is considered poor. The article also discusses the difficulties in detecting boundaries of argument components and the challenges of applying existing argumentation theories to user", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "The article discusses various approaches towards text analysis, including computational methods to measure social and cultural concepts such as hate speech. The research process involves identifying research questions, selecting data, conceptualizing and operationalizing the concepts, and analyzing and interpreting the results. The analysis phase often feeds back into the original research questions, and choices regarding how to operationalize and analyze concepts can raise serious concerns about conceptual validity. The article highlights the challenges and opportunities of using computational text analysis for exploring complex social and cultural phenomena.", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "A node in the network approach represents a country or state in the study of international politics.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "Yes.", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "The article introduces a new approach to estimate preference polarization in multidimensional settings using texts and votes. The authors use a distributional representation of textual data and community detection in multiplex networks to identify densely connected communities based on preference similarity. They illustrate the utility of this approach with an empirical test of a core hypothesis in International Relations (IR): militarized conflict is less likely between states with more similar preferences. The authors find that multiplex clusters based on country speeches and votes - which they refer to as affinity blocs - outperform clusters based on votes or speeches alone.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "Unanswerable.", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "Unanswerable.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The datasets were annotated using a carefully designed translation and annotation protocol that ensures consistency across languages.", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "The article describes a methodology that uses deep neural network high-dimensional text-embedding algorithms to detect content-based groups of records in a given dataset in an unsupervised manner. The text embedding is based on the free and unstructured textual description of the incidents reported to the National Reporting and Learning System (NRLS) in England and Wales. The method combines text-embedding, geometric graph construction, and multi-resolution community detection to identify content-based clusters from free, unstructured text in an unsupervised manner.", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "The article does not provide a clear answer to this question. It mentions that the study uses a computational linguistic framework to analyze dehumanizing language, with a focus on lexical signals of dehumanization, but does not specify how the discussions of LGBTQ people in the New York Times are identified.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "No.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "The 12 languages covered in the article are not specified.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Unanswerable.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "The baselines in the article are not explicitly mentioned. However, the authors propose a new method called Cell-aware Stacked LSTM (CAS-LSTM) and evaluate it on multiple benchmark datasets.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Facebook pages", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The article discusses the use of cognitive features extracted from eye-tracking data to improve sarcasm detection in NLP processing systems. The authors created a novel framework that combines traditional linguistic features with cognitive features obtained from readers' eye-movement data, resulting in an improved success rate for sarcasm detection. The article provides examples of how the system performs on different sentences and compares its classification accuracy to the best available system. The authors also discuss the limitations of the approach and suggest future work in exploring deeper graph and gaze features.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The article does not provide specific information on the datasets used in evaluation. It only mentions that the experiments were conducted on four languages, which were part of SemEval-2016 Challenge Task 5. The article does not provide any details on the specific datasets used for evaluation. Therefore, the answer to this question is \"unanswerable\".", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Yes.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Yes.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "Unanswerable.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "The article discusses the use of neural models to predict instructor intervention in MOOC forums. The models use a thread structure and the sequence of posts to make predictions. The article proposes attention models to infer the latent context of the thread, which is defined as a series of posts that trigger an intervention. The article also discusses the use of context in discussions and the role of structure and sequence in predicting intervention. The article evaluates the performance of the proposed models over a diverse dataset of 12 MOOC iterations on Coursera.org.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "Pivot-based baselines are methods that translate source language to a pivot language, and then translate the pivot language to the target language. This approach is commonly used in transfer learning for neural machine translation (NMT).", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "They use datasets with transcribed text.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention used in a Chinese word segmentation model. It captures localness and directional information by adding a directional mask and a Gaussian mask to the standard self-attention mechanism. The model uses only unigram features and achieves new state-of-the-art performance on the SIGHAN Bakeoff 2005 benchmark dataset.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Unanswerable.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "The article describes the DeepMine database, a large speech corpus collected using crowdsourcing and designed for text-dependent and text-prompted speaker verification, as well as text-independent speaker verification evaluation. The database contains speech data from 1969 speakers, with 1149 male and 820 female speakers, and includes fixed common phrases, random sequences of words, and phrases with word- and phoneme-level transcription. The article provides details on the data collection process, including the use of an Android application and post-processing steps to", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "The baseline is a neural network-based Named Entity Recognition (NER) system for the Nepali language, which uses grapheme-level representations and POS one-hot encoding vectors.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Unanswerable.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "The article describes the use of a Recurrent Neural Network (RNN) for end-to-end speech recognition.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "The article mentions two datasets, EmotionLines and Friends. EmotionLines is a dialogue dataset composed of two subsets, Friends and EmotionPush, and includes $1,000$ English dialogues, each divided into a few consecutive utterances. The dataset is annotated by five annotators on a crowd-sourcing platform, and the labeling work is based on the textual content. Friends is a speech-based dataset that includes $1,000$ English dialogues from the TV sitcom Friends, and EmotionPush is a chat-based dataset that includes $1", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The article proposes a neural-based Named Entity Recognition (NER) model for Nepali language using the latest state-of-the-art architecture. The model uses grapheme-level features and does not require any hand-crafted features or data pre-processing. The authors compare their model with various baselines, including Support Vector Machine (SVM) and Hidden Markov Model (HMM) with rule-based approaches, and show that their model performs better in terms of entity recognition accuracy. They also release a dataset to support future research and perform empirical evaluation of their model with state-of-", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Attention is different from alignment in cases where the attention model uses the most relevant parts of the source sentence at each translation step, and where the attention mechanism captures information beyond alignment in the case of verbs.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "The article discusses the automatic summarization of discharge summary notes, which are critical to ensuring continuity of care after hospitalization. The authors explore the upper bound on extractive summarization and develop a classifier for labeling the topics of history of present illness notes. They use the MIMIC-III database, which contains electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012. The dataset contains 55,177 discharge reports and 4,475 discharge", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "The article describes the use of a hierarchical three-level annotation model to create a new large publicly available dataset of English tweets. The article also discusses related work on abusive language identification, including aggression identification, bullying detection, hate speech, toxic comments, and offensive language. The article proposes a new hierarchical annotation model that captures the similarities among prior work on abusive language and distinguishes between whether the language is directed towards a specific individual or entity or towards a generalized group and whether the abusive content is explicit or implicit. The article also describes the use of", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "Unanswerable.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "Yes.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "BERT", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Attention captures useful information beyond traditional word alignment, such as syntactic and morphological information, which contributes to the translation quality.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The baselines were BOW, TFIDF, and various neural-based word embedding methods, as well as Logistic Regression, Random Forest, TextCNN with initial word embedding as GloVe, and the proposed model.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The article does not provide information on the length of the dataset for each step of the hierarchy.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The article discusses the use of tag recommendations to annotate e-books, specifically focusing on the use of editor tags and user-generated search terms from Amazon to improve the quality of tag recommendations. The authors evaluate various tag recommendation approaches, including popularity-based, similarity-based, and hybrid approaches, using a dataset containing editor tags, Amazon search terms, and Amazon review keywords. The study finds that combining both data sources enhances the quality of tag recommendations and that approaches that train on Amazon search terms provide poor performance in terms of accuracy but deliver good results in terms of semantic similarity and recommendation diversity.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "The article does not provide specific information on the data used by the authors. However, it mentions that the Pyramidal Recurrent Unit (PRU) was evaluated on the Penn Treebank (PTB) and WikiText2 (WT-2) datasets.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "Unanswerable.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The article does not provide a specific architecture for the models mentioned. It only mentions that they use an end-to-end approach and provide baseline results for various language pairs.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The article does not mention a specific baseline method.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The more predictive set of features to detect fake news is a combination of words embeddings, style, and morality features.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The ancient Chinese dataset comes from the parallel corpus crawling and cleaning, paragraph alignment, clause alignment based on aligned paragraphs, and augmenting data by merging aligned adjacent clauses.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The article does not mention which datasets are used for evaluation. Therefore, the answer is \"unanswerable\".", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "The article does not provide information on the size of the collection of COVID-19 literature.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Macaw has a modular design with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "Unanswerable.", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "The proposed model is compared to several baseline models, including traditional features such as TF-IDF, as well as deep features such as Doc2vec, LSTM, and ELMo.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "The article does not mention the specific datasets used for experiments.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Yes.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The article discusses the relation classification task, where the goal is to assign sentences with two marked entities to a predefined set of relations. The authors propose using neural networks (NNs) for this task, specifically convolutional neural networks (CNNs) and recurrent neural networks (RNNs), and present a new context representation for CNNs. They also introduce a ranking loss function for RNNs and combine CNNs and RNNs using a voting scheme. The article is based on the SemEval 2010 shared task 8 dataset, which consists of manually annotated data for relation classification.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The article describes experiments in finding out whether lemmatization helps modern contextualised embeddings, specifically the ELMo model, on the example of English and Russian languages. The authors trained their own ELMo models from scratch on English and Russian corpora, consisting of the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018, respectively, with the RNC added to the English corpus. The texts were tokenized and lemmatized with the UDPipe models for the respective languages trained on the Universal Dependencies 2.3 treebanks. The ELMo models", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "Unanswerable.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Unanswerable.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The encoder in the system described in the article is an LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "The article proposes a neural probabilistic model that captures word embeddings in a lower dimensional space, considering both the word's semantic similarity and its context. The model is trained using a variant of max-margin objective based on the asymmetric KL divergence energy function. The article also presents a KL approximation between Gaussian mixtures to capture textual entailment. The proposed approach is evaluated on benchmark word similarity and entailment datasets, showing better performance than existing models.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "Unanswerable.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "The Random Kitchen Sink (RKS) approach is a method used in natural language processing tasks, including offensive language detection, by explicitly mapping data vectors to a space where linear separation is possible. It provides an approximate kernel function via explicit mapping and has been explored for sentiment analysis, background-foreground separation, load forecasting, and saliency detection.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "They compare their approach to a baseline model that does not use pre-trained language model representations.", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The article describes a scientific article that proposes a new task of generating plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences. The authors release a new dataset of 180K+ recipes and 700K+ user reviews for this task and introduce new evaluation strategies for generation quality in instructional texts. They explore a hierarchical approach that condition", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "The article describes a new dataset called ReviewQA, which is a question-answering dataset for hotel reviews. The dataset contains 8 competencies that evaluate the reader's ability to understand and reason with relevant information extracted from a document. The questions are natural language questions over the 500 most popular articles of Wikipedia, and the answers are spans of text extracted from the source documents. The dataset is designed to challenge models to perform strongest reasoning tasks.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "Yes.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "Unanswerable.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "The article does not provide a clear answer to this question. It mentions that the dataset was collected from 35 mobile applications and their privacy policies, but it does not provide any information about the experts who were used for annotation. It is possible that the authors of the article did not include this information in the final version of the article, or that the information was not available at the time of writing. Therefore, the question is unanswerable.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "They addressed cyberbullying on social media platforms such as Formspring, Twitter, and Wikipedia.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The results show that error detection performance is improved by using artificially generated data created by the proposed methods. The combination of the pattern-based approach with the machine translation system gave the best overall performance on all datasets.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "The article does not mention which baseline model is used.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "IMDb dataset", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The article describes an experiment in which the authors incorporated subword information into word embedding models and evaluated the resulting word embeddings using intrinsic and extrinsic evaluations. The intrinsic evaluations consisted of word similarity and word analogy tasks, while the extrinsic evaluations included downstream tasks from the VecEval suite. The authors found that incorporating subword information into the LexVec model resulted in similar improvements as observed in moving from Skip-gram to fastText, and that unsupervised morphemes offered no advantage over n-grams. They also found that subword information improved syntactic results while pres", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "The article does not provide information on the sources of the datasets used in the study.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "Unanswerable.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "The article describes the creation of a corpus for training machine learning models, the development of a gold-standard test corpus, and the evaluation of established approaches for named entity recognition in the Armenian language. The authors used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. They also developed GloVe word embeddings and baseline results for 3 different models on the proposed benchmark data set. The article does not mention any specific named entity recognition models.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "Unanswerable.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "The article discusses the challenges of language identification (LID) in South Africa, where there are 11 official languages. The authors propose a hierarchical naive Bayesian and lexicon-based classifier for LID of short pieces of text. The algorithm is evaluated against recent approaches using existing test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) 2015 and 2017 shared tasks. The article also reviews existing works on the topic and summarizes the remaining research problems. The authors suggest that data harvesting, building", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "The best performing model among the author's submissions is ranked 3rd in the Fragment-level Classification (FLC) task with a score of 0.673 on the test set.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "The article does not mention which datasets are used to evaluate the proposed method.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "PolyResponse is a restaurant search and booking system that is currently available in 8 languages: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The article does not provide information on the performance of the system.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "Yes.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "The new context representation is proposed in the article and is based on the idea of using all parts of the sentence, including the relation arguments, left of the relation arguments, between the arguments, and right of the arguments, and paying special attention to the middle part.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "KNN, RF, SVM, and MLP have been trained.", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "The article describes a study that uses a dataset of Twitter tweets annotated with depression-related classes. The dataset is constructed based on a hierarchical model of depression-related symptoms, and it contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. The dataset is encoded with 7 feature groups with associated feature values binarized to represent potentially inform", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The article proposes the use of context tweets, which provide additional information about the context in which an abusive tweet was made, to improve the accuracy of abusive language detection. The article also explores the use of various features and models, including traditional machine learning classifiers and neural network models, to detect abusive language.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Unanswerable.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The article does not mention the specific metrics used for evaluation.", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "The Adversarial-neural Event Model (AEM) outperforms K-means, LEM, and DPEMM in event extraction from online texts.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "The article discusses various approaches to grammar induction, including probabilistic context-free grammars (PCFGs) and compound PCFGs. PCFGs are grammars with rule probabilities that are specified by a probabilistic grammar. Compound PCFGs are a type of PCFG that use a compound probability distribution to model the generative process. The article states that the compound PCFG approach is able to induce linguistically meaningful grammars by optimizing log likelihood, and that this can be done through a collapsed approach using amortized inference", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19, which consists of subparts of DTA corpus and contains texts from the 16th to the 20th century.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "InferSent, Universal Sentence Encoder, and SBERT are evaluated in the article.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The data in the article is in the Europarl language pair, which consists of English (INLINEFORM0 and INLINEFORM1) and French.", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "Multi-granularity and multi-tasking neural architecture design involve combining different levels of granularity and different tasks in a single model, allowing for more efficient and effective propaganda detection.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "The state-of-the-art models for automatic judgment prediction in civil cases are based on the Legal Reading Comprehension (LRC) framework, which incorporates the reading mechanism to better model the complementary inputs of fact descriptions, plaintiff's pleas, and related law articles. The LRC framework has been shown to improve the performance of judgment prediction compared to previous text classification-based approaches.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "Two metrics are proposed to evaluate keyphrase generation: F1 score and diversity of generated sequences.", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The size of the TweetQA dataset is not specified in the article.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "No.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "The backoff strategies in the article involve falling back on a generic word recognizer trained on a larger corpus when the word recognizer used in the main defense model encounters unobserved and rare words. The goal is to handle these unseen words and improve the defense model's robustness against adversarial attacks.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "Unanswerable.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "The CORD-19 dataset is a collection of scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. It contains over 45,000 articles contributed by hospitals and medical institutes all over the world and is used to study the correlation between radiological findings and COVID-19.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "The article discusses the use of natural language processing (NLP) and machine learning (ML) techniques for detecting online hate speech. The authors propose a transfer learning approach using the pre-trained language model BERT and introduce new fine-tuning strategies to improve the performance of hate speech detection. The article also discusses previous studies on online hate speech and offensive language, and transfer learning. The authors conclude that their model outperforms previous works and can detect some biases in the process of collecting or annotating datasets.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "The Neural User Simulator (NUS) is introduced, which outputs natural language and learns from a corpus of recorded dialogues.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "The article discusses the use of automatic diacritic recovery in Arabic language processing, specifically focusing on the recovery of both core-word diacritics (CW) and case-endings (CE). The authors propose two separate Deep Neural Network (DNN) architectures for recovering both types of diacritic types, using character-level and word-level bidirectional Long-Short Term Memory (biLSTM) based recurrent neural models for CW diacritic and CE recovery respectively. They train models for both Modern Standard Arabic (MSA) and Classical Arabic (CA", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "The article describes a method for performing both grammatical error correction and style transfer with a single trained model, using zero-shot neural machine translation. The model is trained on bilingual examples in both directions, and then used to translate the input into the same language for both tasks. The success of the model is evaluated in terms of performance in grammatical error correction and style transfer, and the model is shown to be effective in correcting errors and adapting the output into a desired style. The approach is applicable to multiple languages within the same model, and the paper presents thorough quantitative and qualitative manual evaluation of the model", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "Disinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "The baseline is not mentioned in the article.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The document-level encoder proposed in the paper is based on the pretrained Bert model, which is trained on a large corpus of text and uses a masked language modeling objective. The encoder is designed to encode a document and obtain representations for its sentences. This is a novel approach in text summarization, as previous models have focused on sentence-level encodings. The document-level encoder is a key component of the proposed summarization models, which achieve state-of-the-art results in both extractive and abstractive summarization tasks.", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "The article discusses a method to learn more interpretable dense word embeddings by modifying the objective function of the GloVe algorithm. The proposed method aligns the dimensions of the embedding vector with predefined concepts, resulting in an increase in interpretability. The article demonstrates the effectiveness of the method through quantitative and qualitative evaluations and shows that the semantic structure of the original embedding is not harmed. The method can be applied to other word embedding algorithms with a similar learning routine. The proposed methodology can also be helpful in computational cross-lingual studies.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "Unanswerable.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "The article proposes a new approach that enables learning for different manipulation schemes with the same single algorithm. The authors adapt an off-the-shelf reward learning algorithm from the reinforcement learning literature for data manipulation in the supervised setting. The algorithm is used to learn data augmentation and weighting, and it shows improved performance over strong base models and previous manipulation methods.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "The article proposes a new structured-data encoder that assumes the data structure should be hierarchically captured. The decoder is chosen to be a classical module as used in previous models. The encoder introduces a hierarchical attention mechanism to compute the hierarchical context fed into the decoder. The experiments are conducted on the RotoWire benchmark, which contains around $5K$ statistical tables of NBA basketball games paired with human-written descriptions. The proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics. The article also mentions that the proposed approach", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "Improvement over the best performing state-of-the-art is not mentioned in the article.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "The state of the art models for sarcasm detection are unanswerable.", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "Yes.", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "The article discusses the use of reinforcement learning to solve text-based games. The authors propose a methodology that uses the Go-Explore algorithm to extract high-performing trajectories in these games, and then trains a Seq2Seq model on these trajectories to play the game. The experiments are performed on the CoinCollector game and the \"First TextWorld Problems\" dataset, which consists of 4,440 games generated using the Z-Machine. The results show that the proposed methodology is effective in solving text-based games, with the Seq2Seq model trained on Go-", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "Yes.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "Reuters database", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "The article discusses a scientific article that introduces a new model called the Transformer, which is a sequence-to-sequence (seq2seq) model used in natural language processing (NLP). The Transformer uses multi-head attention mechanisms to map an input sequence to an output sequence, allowing for the dynamic, context-dependent propagation of information within and across sentences. The article also introduces a new model called the \"sparse attention\" Transformer, which allows for the attention to be limited to a contiguous span of past tokens, rather than the entire sequence. The article concludes with a discussion of", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The additive modification to the objective function is the introduction of an additional term that favors an increase in the value of the embedding vector dimension corresponding to the concept that the particular word belongs to.", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "The article discusses the use of a deep learning framework for detecting fake news on social media. The authors propose a classification framework based on a multi-layer formulation of Twitter diffusion networks, which allows them to disentangle different social interactions on Twitter, such as tweets, retweets, mentions, replies, and quotes, to build a diffusion network composed of multiple layers. They compute structural features separately for each layer and use a logistic regression model to classify news articles as either credible or non-credible. The article presents experimental results showing that the use of a multi-layer, disentang", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "The model is more reliable for spelling, word order, and grammatical errors in grammatical error correction. It is less reliable for lexical choice errors.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "Yes.", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "The article discusses the use of computational approaches to assist human moderators in online conversation platforms. The model introduced in the article is capable of forecasting the derailment of a conversation based on early warning signs, and can capture the dynamics of a conversation as it develops. The article also discusses the challenges of forecasting conversational events, including the need to capture inter-comment dynamics and deal with an unknown horizon. The article concludes with a discussion of future work, including the need to address questions about the usefulness of early warnings to moderators and the ethical implications of such a system.", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "The article describes a scientific competition called BioASQ, which is a biomedical document classification, document retrieval, and question answering competition. The competition is in its seventh year and the article describes the system used by the authors to participate in the question answering task. The system is based on BioBERT, a biomedical version of the BERT model, and is fine-tuned for the biomedical question answering task. The article describes the results of the system in the competition and also presents related work on question answering tasks using different models.", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "The article discusses the use of RNNs in Statistical Machine Translation (SMT) and their application to morphologically rich languages. The use of RNNs has revolutionized the approaches to automated translation, but the main downside is the heavy corpus requirement. The efficiency of an MT system depends on the availability and size of parallel corpora, and the syntactic divergence between the languages. Morphologically rich languages are difficult to translate due to the structural and semantic discordance with English. The article also discusses the use of word2vec and morph2vec models for", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "Unanswerable.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes.", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "The article does not mention the specific datasets used in the experiment.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "The article describes a scientific competition called BioASQ, which involves submitting answers to biomedical questions. The authors state that their system scored near the top for factoid questions in the third test batch set and achieved the highest recall score for List-type questions in the fourth test batch set. They also mention that they used a BioBERT BIBREF0 based system, which is a contextual word embedding model fine-tuned for the biomedical question answering task. The article also describes related work on question answering tasks, including the use of BERT BIBREF1, a contextual word embedding model,", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Diversity of keyphrases is typically measured by evaluating the number of unique keyphrases generated by a model, as well as the diversity of the generated sequences. One common metric for measuring diversity is the F1 score, which takes into account both precision and recall.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "The baseline is a reference point or starting point for comparison in an experiment or analysis. In the context of the article, the baseline refers to the initial performance of the models before any improvements are made. The article does not provide specific details on the baseline performance of the models, as the focus is on the improvements achieved through various techniques.", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "The paper proposes to use a joint model that combines textual features with visual features to assess the quality of documents. The visual features are based on a fine-tuned Inception V3 model, while the textual features are based on a hierarchical biLSTM. The joint model is trained on a Wikipedia dataset and an arXiv dataset, and achieves state-of-the-art results in both datasets.", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "KAR is an end-to-end MRC model that utilizes general knowledge to assist its attention mechanisms.", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "Improved KB Relation Detection", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The article discusses the use of dedicated question answering (QA) systems to address the challenge of personalization in large-scale information retrieval. The authors propose a new approach called Recognizing Question Entailment (RQE) to retrieve answers to a premise question by retrieving inferred or entailed questions. The effectiveness of the RQE-based QA approach is evaluated using open-domain and clinical datasets. The authors also present two machine learning and deep learning methods for RQE and compare their performance. The article concludes with a discussion of the limitations of the current approach and future", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The article discusses the effectiveness of Rouge, a widely used metric in text summarization evaluation, for assessing the quality of scientific summaries. The study reveals that Rouge scores do not correlate well with manual Pyramid scores, challenging its effectiveness for scientific summarization. The article proposes an alternative metric, Sera, based on relevance analysis, which shows higher and more consistent correlations with manual judgments.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "The accuracy merits of the approach are demonstrated through empirical evaluation on multiple real-world datasets, showing an average improvement of 24.3% in AUC compared to the state of the art.", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "Yes.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "The article does not provide the size of the real-world civil case dataset.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "The authors used twitter to build a large-scale dataset of ironic and non-ironic sentences for their study on irony generation.", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "Unanswerable.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "The article describes a study on neural machine translation (NMT) and how the models decide the importance of output words. The study uses a gradient-based approach to estimate word importance by exploiting the intermediate gradients in NMT generation. The word importance is useful for understanding NMT by identifying under-translated words, and the gradient-based approach is found to be superior to other methods in estimating word importance. The study also analyzes the linguistic behaviors of words with importance and finds that certain syntactic categories have higher importance while the categories vary across language pairs.", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "Yes.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "The proposed task of concept-map-based MDS is defined as follows: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. The evaluation of this task involves creating a new dataset, developing a crowdsourcing scheme, and providing an annotation tool. The concept map is defined as a labeled graph showing concepts as nodes and relationships between them as edges. The task is complex and consists of several interdependent subtasks, including extracting appropriate labels for concepts and relations, recognizing different expressions that refer to the same concept across multiple documents, selecting the", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The article mentions several publicly available datasets that have been used in previous studies on online hate speech and offensive language detection, including Twitter, Reddit, YouTube, and Wikipedia. However, it does not provide specific details on which datasets are used in the proposed transfer learning approach.", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "The article reports several evaluation metrics, including accuracy, F1 score, and performance improvement over previous methods.", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "The evaluation criteria and metrics used to evaluate the generated text include factual correctness, fluency, and product-readiness. The factual correctness was measured by the number of factual errors in the generated text compared to the corrected variant, and the fluency was measured by the word error rate (WER) of the generated text compared to its corrected variant. The product-readiness was evaluated by two journalists from the STT agency, who edited the generated text to a form ready for production use or direct publication.", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "Future work will investigate the applications of dynamic balancing methods between RL and MLE in text generation, and improve the sensationalism scorer.", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "They compared the performance of various LSTM models, including layer-wise trained models, transfer learning models, and models trained with different loss functions and optimization techniques.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The strong baseline is Seq2Seq with attention.", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "The article does not provide the absolute accuracy of the system.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the hate speech detection task and comparing them to the annotations made by human annotators. They found that the model was able to correctly identify some samples that were misclassified by the annotators, indicating that it was able to capture some of the biases present in the data.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "The data was collected through crowdsourcing on Amazon Mechanical Turk, with workers asked to formulate questions and answers based on script scenarios rather than specific texts.", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "They show there is space for further improvement by conducting a human study and finding that a majority of questions the system couldn't answer were actually answerable. This suggests that the system is not yet achieving its full potential and there is still room for improvement.", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "The article proposes Human Level Attributes (HLAs) as a way to model character profiles in dialogue agents. HLAs are based on tropes collected from TV Tropes and are used to trace dialogue data back to both its context and associated human-like qualities. The authors also propose a system called ALOHA that maps characters to a latent space based on their HLAs and determines which are most similar in profile and identity, allowing for the recovery of language styles of specific characters. The article demonstrates the effectiveness of ALOHA in character language style recovery and shows that it is stable across", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "On the SimpleQuestions and WebQuestions benchmarks, they achieve the state of the art.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "The authors show that their learned policy generalizes better than existing solutions to unseen games by training a Seq2Seq model on the high-performing trajectories found by the Go-Explore algorithm, which allows the agent to operate in the full, unconstrained action space of natural language and systematically generalize to new text-based games with no or few interactions with the environment.", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "The article does not provide information on which stock market sector achieved the best performance.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "The article discusses a novel approach to extractive summarization for long documents, which captures both global and local context. The global context refers to the overall structure of the document, while the local context refers to the specific topic being discussed within a section or subsection of the document. The proposed model integrates recent findings on neural extractive summarization and incorporates a parameter-lean and modular architecture. It is evaluated on two large scientific paper datasets and achieves state-of-the-art performance. The benefits of the model come exclusively from modeling the local context, even for the longest documents.", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "KBQA stands for Knowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "The state-of-the-art system is not mentioned in the article.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "Unanswerable.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "Unanswerable.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The state of the art described in the paper is the development of a hybrid conceptual architecture and its implementation with a finance advisory system for multi-party conversational systems (MPCS). The paper also discusses the challenges of engineering MPCS and explores the possibility of decoupled interaction norms specification and a multi-party governance service to enforce compliant utterances.", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "The baseline model used in the BioASQ 7 competition is a BioBERT BIBREF0 based system that is fine-tuned for the biomedical question answering task. The system scored near the top for factoid questions in all test batches and achieved the highest 'MRR' score for Factoid Question Answering task in the third test batch set.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "Unanswerable.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "The article discusses the use of monolingual data in Neural Machine Translation (NMT) systems and proposes various techniques to integrate such data. The study focuses on the Europarl domain, which consists of two language pairs: English-German and English-French. The NMT systems are trained on out-of-domain parallel corpora and limited in-domain data. The article presents several baseline NMT systems and compares their performance with and without the use of monolingual data. The study also explores the effects of Back-Translation (BT) and the use", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "The article mentions that several real-world datasets are used for the experiments, including CyberAttack and PoliticianDeath.", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "The data in the new corpus is sourced from various domains and registers, including comments to articles, discussion forum posts, blog posts, and professional newswire articles. The data come from a variety of sources and no assumptions about its actual content with respect to argumentation can be drawn.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "The article does not mention the type of classifiers used.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "The article describes the use of neural machine translation (NMT) for the low-resourced language pair of Japanese INLINEFORM0 Vietnamese. The authors attempt to build NMT systems for this pair and conduct experiments using advanced methods to improve the quality of the systems. They use subword units, byte-pair encoding (BPE), and a variant of BPE for Vietnamese, and experiment with different segmentation methods for both languages. The article also describes the use of back-translated texts and mix-source data to increase the amount of training data. The main contributions of the paper are the description of the", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "The paper explores various embedding techniques, including word embeddings and second-order co-occurrence vectors.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The proposed model, ALOHA, outperforms the baselines by a noticeable margin, achieving a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to understand the impact of gender representation in media on speech recognition systems and to question the fairness of these systems in terms of gender.", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "The article discusses the challenges of engineering multi-party conversational systems (MPCS) and presents a hybrid conceptual architecture and its implementation with a finance advisory system. The authors are evolving the architecture to support decoupled interaction norms specification and developing a multi-party governance service to enforce exchange of compliant utterances. They are also exploring a micro-service implementation of SABIA to increase its scalability and performance. The article does not provide any specific evaluation metrics for the system.", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "They build a predictive model of dogmatism using statistical analysis of linguistic features in social media posts.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "This work accounts for various argumentation phenomena in actual data, including refutation, rebuttal, and rhetorical questions.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "The model is applied to two datasets: the highly curated `Conversations Gone Awry` dataset and the in-the-wild moderation of the popular subreddit ChangeMyView.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "The article discusses the use of BERT, a deep bidirectional transformer architecture, for natural language processing tasks, particularly in the context of imbalanced classification. The authors participated in a shared task on fine-grained propaganda detection and achieved second place on sentence-level classification. They also addressed the issue of class imbalance and cost-sensitivity in BERT, which enables models to adapt to dissimilar datasets. The article provides statistical methods for establishing dataset similarity and discusses the challenges of automated propaganda detection.", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "The article describes a study on neural machine translation (NMT) and how to better understand and improve NMT models. The study focuses on exploiting gradients in NMT generation to estimate word importance better. The authors find that word importance is useful for understanding NMT by identifying under-translated words, and they provide empirical support for the design principle of NMT architectures. The study also analyzes the linguistic behaviors of words with importance and shows potential to improve NMT models. The authors contribute to the field by demonstrating the necessity and effectiveness of exploiting intermediate gradients for estimating word", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "A second order co-occurrence matrix is a measure of semantic similarity and relatedness that incorporates pairwise similarity scores derived from a taxonomy, reducing noise by selecting only the most semantically similar co-occurrences.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "The size of the dataset is not mentioned in the article.", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "The article discusses a new approach to building chatbots that can learn and improve their conversation ability over time. The approach is based on the idea of open-world knowledge base completion (OKBC), which allows chatbots to learn new facts and knowledge from user inputs even if they are not explicitly stated. The authors propose a lifelong interactive learning and inference (LiLi) approach to solve the OKBC problem, which involves using reinforcement learning to formulate query-specific inference strategies and then executing them to solve the problem by interacting with the user. The effectiveness of the LiLi approach is evaluated through", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "The baseline methods are not explicitly mentioned in the article.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "The article describes a study on the use of crowdsourcing platforms like Amazon Mechanical Turk to collect a sufficiently large corpus of annotated natural language examples in specialized domains like biomedicine. The study focuses on the prediction of annotation difficulty and the use of lay annotators and expert annotators to examine the quality of annotations. The article does not provide any information on how the annotators are matched to the instances.", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "Yes.", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "ROGUE", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "The article does not provide information on the size of the data set used in the study.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The article discusses the use of computational methods to study social and cultural phenomena, with a focus on the example of eliminating \"echo chambers\" on Reddit. The authors highlight the opportunities and challenges of using text data for such analyses, and provide a general overview of the research process involved. They aim to shed light on thorny issues, provide best practices for working with thick social and cultural concepts, and promote interdisciplinary collaborations. The authors have diverse backgrounds in computer science, social sciences, and humanities.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "The article does not mention the two large-scale datasets used.", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "No.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "The article introduces a new approach to estimate preference polarization in multidimensional settings using texts and votes. The authors use a distributional representation of textual data and community detection in multiplex networks to identify densely connected communities based on preference similarity. They illustrate the utility of this approach with an empirical test of a core hypothesis in International Relations (IR): militarized conflict is less likely between states with more similar preferences. The authors find that multiplex clusters based on country speeches and votes - which they refer to as affinity blocs - outperform clusters based on votes or speeches alone.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "The article discusses various approaches towards text analysis, including computational methods to measure social and cultural concepts such as hate speech. The research process involves identifying research questions, selecting data, conceptualizing and operationalizing the concepts, and analyzing and interpreting the results. The analysis phase often feeds back into the original research questions, and choices regarding how to operationalize and analyze concepts can raise serious concerns about conceptual validity. The article highlights the challenges and opportunities of using computational text analysis for exploring complex social and cultural phenomena.", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "Unanswerable.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The article discusses the challenges and limitations of argumentation mining, a field that aims to analyze and understand argumentation in user-generated content on the Web. The researchers propose a new corpus of annotated data and experiment with various machine learning methods to identify argument components. They find that the best-performing system achieves a macro-F1 score of 0.30 using Krippendorff's inter-annotator agreement measure, which is considered poor. The article also discusses the difficulties in detecting boundaries of argument components and the challenges of applying existing argumentation theories to user", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Yes.", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "A node in the network approach represents a country or state in the study of international politics.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "The article does not provide a clear answer to this question. It mentions that the study uses a computational linguistic framework to analyze dehumanizing language, with a focus on lexical signals of dehumanization, but does not specify how the discussions of LGBTQ people in the New York Times are identified.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "Unanswerable.", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "No.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "The article describes a methodology that uses deep neural network high-dimensional text-embedding algorithms to detect content-based groups of records in a given dataset in an unsupervised manner. The text embedding is based on the free and unstructured textual description of the incidents reported to the National Reporting and Learning System (NRLS) in England and Wales. The method combines text-embedding, geometric graph construction, and multi-resolution community detection to identify content-based clusters from free, unstructured text in an unsupervised manner.", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "The datasets were annotated using a carefully designed translation and annotation protocol that ensures consistency across languages.", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "The 12 languages covered in the article are not specified.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "Unanswerable.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "Facebook pages", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The baselines in the article are not explicitly mentioned. However, the authors propose a new method called Cell-aware Stacked LSTM (CAS-LSTM) and evaluate it on multiple benchmark datasets.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Yes.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "The article does not provide specific information on the datasets used in evaluation. It only mentions that the experiments were conducted on four languages, which were part of SemEval-2016 Challenge Task 5. The article does not provide any details on the specific datasets used for evaluation. Therefore, the answer to this question is \"unanswerable\".", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "The article discusses the use of cognitive features extracted from eye-tracking data to improve sarcasm detection in NLP processing systems. The authors created a novel framework that combines traditional linguistic features with cognitive features obtained from readers' eye-movement data, resulting in an improved success rate for sarcasm detection. The article provides examples of how the system performs on different sentences and compares its classification accuracy to the best available system. The authors also discuss the limitations of the approach and suggest future work in exploring deeper graph and gaze features.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "Pivot-based baselines are methods that translate source language to a pivot language, and then translate the pivot language to the target language. This approach is commonly used in transfer learning for neural machine translation (NMT).", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Yes.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "The article discusses the use of neural models to predict instructor intervention in MOOC forums. The models use a thread structure and the sequence of posts to make predictions. The article proposes attention models to infer the latent context of the thread, which is defined as a series of posts that trigger an intervention. The article also discusses the use of context in discussions and the role of structure and sequence in predicting intervention. The article evaluates the performance of the proposed models over a diverse dataset of 12 MOOC iterations on Coursera.org.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Unanswerable.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "Unanswerable.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "They use datasets with transcribed text.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "Unanswerable.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention used in a Chinese word segmentation model. It captures localness and directional information by adding a directional mask and a Gaussian mask to the standard self-attention mechanism. The model uses only unigram features and achieves new state-of-the-art performance on the SIGHAN Bakeoff 2005 benchmark dataset.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The baseline is a neural network-based Named Entity Recognition (NER) system for the Nepali language, which uses grapheme-level representations and POS one-hot encoding vectors.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "The article describes the DeepMine database, a large speech corpus collected using crowdsourcing and designed for text-dependent and text-prompted speaker verification, as well as text-independent speaker verification evaluation. The database contains speech data from 1969 speakers, with 1149 male and 820 female speakers, and includes fixed common phrases, random sequences of words, and phrases with word- and phoneme-level transcription. The article provides details on the data collection process, including the use of an Android application and post-processing steps to", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "The article mentions two datasets, EmotionLines and Friends. EmotionLines is a dialogue dataset composed of two subsets, Friends and EmotionPush, and includes $1,000$ English dialogues, each divided into a few consecutive utterances. The dataset is annotated by five annotators on a crowd-sourcing platform, and the labeling work is based on the textual content. Friends is a speech-based dataset that includes $1,000$ English dialogues from the TV sitcom Friends, and EmotionPush is a chat-based dataset that includes $1", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The article discusses the automatic summarization of discharge summary notes, which are critical to ensuring continuity of care after hospitalization. The authors explore the upper bound on extractive summarization and develop a classifier for labeling the topics of history of present illness notes. They use the MIMIC-III database, which contains electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012. The dataset contains 55,177 discharge reports and 4,475 discharge", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "The article describes the use of a Recurrent Neural Network (RNN) for end-to-end speech recognition.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "Attention is different from alignment in cases where the attention model uses the most relevant parts of the source sentence at each translation step, and where the attention mechanism captures information beyond alignment in the case of verbs.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "The article proposes a neural-based Named Entity Recognition (NER) model for Nepali language using the latest state-of-the-art architecture. The model uses grapheme-level features and does not require any hand-crafted features or data pre-processing. The authors compare their model with various baselines, including Support Vector Machine (SVM) and Hidden Markov Model (HMM) with rule-based approaches, and show that their model performs better in terms of entity recognition accuracy. They also release a dataset to support future research and perform empirical evaluation of their model with state-of-", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Unanswerable.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "BERT", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "The article describes the use of a hierarchical three-level annotation model to create a new large publicly available dataset of English tweets. The article also discusses related work on abusive language identification, including aggression identification, bullying detection, hate speech, toxic comments, and offensive language. The article proposes a new hierarchical annotation model that captures the similarities among prior work on abusive language and distinguishes between whether the language is directed towards a specific individual or entity or towards a generalized group and whether the abusive content is explicit or implicit. The article also describes the use of", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "The article discusses the use of tag recommendations to annotate e-books, specifically focusing on the use of editor tags and user-generated search terms from Amazon to improve the quality of tag recommendations. The authors evaluate various tag recommendation approaches, including popularity-based, similarity-based, and hybrid approaches, using a dataset containing editor tags, Amazon search terms, and Amazon review keywords. The study finds that combining both data sources enhances the quality of tag recommendations and that approaches that train on Amazon search terms provide poor performance in terms of accuracy but deliver good results in terms of semantic similarity and recommendation diversity.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The article does not provide information on the length of the dataset for each step of the hierarchy.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "Yes.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "Unanswerable.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The baselines were BOW, TFIDF, and various neural-based word embedding methods, as well as Logistic Regression, Random Forest, TextCNN with initial word embedding as GloVe, and the proposed model.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "Attention captures useful information beyond traditional word alignment, such as syntactic and morphological information, which contributes to the translation quality.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The article does not provide a specific architecture for the models mentioned. It only mentions that they use an end-to-end approach and provide baseline results for various language pairs.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "The more predictive set of features to detect fake news is a combination of words embeddings, style, and morality features.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "The article does not mention a specific baseline method.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The article does not provide information on the size of the collection of COVID-19 literature.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "The article does not provide specific information on the data used by the authors. However, it mentions that the Pyramidal Recurrent Unit (PRU) was evaluated on the Penn Treebank (PTB) and WikiText2 (WT-2) datasets.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "The article does not mention which datasets are used for evaluation. Therefore, the answer is \"unanswerable\".", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "The ancient Chinese dataset comes from the parallel corpus crawling and cleaning, paragraph alignment, clause alignment based on aligned paragraphs, and augmenting data by merging aligned adjacent clauses.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The proposed model is compared to several baseline models, including traditional features such as TF-IDF, as well as deep features such as Doc2vec, LSTM, and ELMo.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Macaw has a modular design with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Unanswerable.", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "The article describes experiments in finding out whether lemmatization helps modern contextualised embeddings, specifically the ELMo model, on the example of English and Russian languages. The authors trained their own ELMo models from scratch on English and Russian corpora, consisting of the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018, respectively, with the RNC added to the English corpus. The texts were tokenized and lemmatized with the UDPipe models for the respective languages trained on the Universal Dependencies 2.3 treebanks. The ELMo models", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "The article does not mention the specific datasets used for experiments.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The encoder in the system described in the article is an LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The article discusses the relation classification task, where the goal is to assign sentences with two marked entities to a predefined set of relations. The authors propose using neural networks (NNs) for this task, specifically convolutional neural networks (CNNs) and recurrent neural networks (RNNs), and present a new context representation for CNNs. They also introduce a ranking loss function for RNNs and combine CNNs and RNNs using a voting scheme. The article is based on the SemEval 2010 shared task 8 dataset, which consists of manually annotated data for relation classification.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The article proposes a neural probabilistic model that captures word embeddings in a lower dimensional space, considering both the word's semantic similarity and its context. The model is trained using a variant of max-margin objective based on the asymmetric KL divergence energy function. The article also presents a KL approximation between Gaussian mixtures to capture textual entailment. The proposed approach is evaluated on benchmark word similarity and entailment datasets, showing better performance than existing models.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "Unanswerable.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "Unanswerable.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "The article describes a scientific article that proposes a new task of generating plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences. The authors release a new dataset of 180K+ recipes and 700K+ user reviews for this task and introduce new evaluation strategies for generation quality in instructional texts. They explore a hierarchical approach that condition", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "Unanswerable.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "Unanswerable.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "The article describes a new dataset called ReviewQA, which is a question-answering dataset for hotel reviews. The dataset contains 8 competencies that evaluate the reader's ability to understand and reason with relevant information extracted from a document. The questions are natural language questions over the 500 most popular articles of Wikipedia, and the answers are spans of text extracted from the source documents. The dataset is designed to challenge models to perform strongest reasoning tasks.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "The Random Kitchen Sink (RKS) approach is a method used in natural language processing tasks, including offensive language detection, by explicitly mapping data vectors to a space where linear separation is possible. It provides an approximate kernel function via explicit mapping and has been explored for sentiment analysis, background-foreground separation, load forecasting, and saliency detection.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "They compare their approach to a baseline model that does not use pre-trained language model representations.", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "The results show that error detection performance is improved by using artificially generated data created by the proposed methods. The combination of the pattern-based approach with the machine translation system gave the best overall performance on all datasets.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "Yes.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "They addressed cyberbullying on social media platforms such as Formspring, Twitter, and Wikipedia.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The article does not provide a clear answer to this question. It mentions that the dataset was collected from 35 mobile applications and their privacy policies, but it does not provide any information about the experts who were used for annotation. It is possible that the authors of the article did not include this information in the final version of the article, or that the information was not available at the time of writing. Therefore, the question is unanswerable.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The article describes an experiment in which the authors incorporated subword information into word embedding models and evaluated the resulting word embeddings using intrinsic and extrinsic evaluations. The intrinsic evaluations consisted of word similarity and word analogy tasks, while the extrinsic evaluations included downstream tasks from the VecEval suite. The authors found that incorporating subword information into the LexVec model resulted in similar improvements as observed in moving from Skip-gram to fastText, and that unsupervised morphemes offered no advantage over n-grams. They also found that subword information improved syntactic results while pres", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "IMDb dataset", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "The article does not mention which baseline model is used.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "The article describes the creation of a corpus for training machine learning models, the development of a gold-standard test corpus, and the evaluation of established approaches for named entity recognition in the Armenian language. The authors used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. They also developed GloVe word embeddings and baseline results for 3 different models on the proposed benchmark data set. The article does not mention any specific named entity recognition models.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "The article does not provide information on the sources of the datasets used in the study.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "Unanswerable.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "Unanswerable.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "The article does not mention which datasets are used to evaluate the proposed method.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "The article does not provide information on the performance of the system.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "The best performing model among the author's submissions is ranked 3rd in the Fragment-level Classification (FLC) task with a score of 0.673 on the test set.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The article discusses the challenges of language identification (LID) in South Africa, where there are 11 official languages. The authors propose a hierarchical naive Bayesian and lexicon-based classifier for LID of short pieces of text. The algorithm is evaluated against recent approaches using existing test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) 2015 and 2017 shared tasks. The article also reviews existing works on the topic and summarizes the remaining research problems. The authors suggest that data harvesting, building", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "The new context representation is proposed in the article and is based on the idea of using all parts of the sentence, including the relation arguments, left of the relation arguments, between the arguments, and right of the arguments, and paying special attention to the middle part.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "PolyResponse is a restaurant search and booking system that is currently available in 8 languages: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The article proposes the use of context tweets, which provide additional information about the context in which an abusive tweet was made, to improve the accuracy of abusive language detection. The article also explores the use of various features and models, including traditional machine learning classifiers and neural network models, to detect abusive language.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "The article describes a study that uses a dataset of Twitter tweets annotated with depression-related classes. The dataset is constructed based on a hierarchical model of depression-related symptoms, and it contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. The dataset is encoded with 7 feature groups with associated feature values binarized to represent potentially inform", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "Yes.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19, which consists of subparts of DTA corpus and contains texts from the 16th to the 20th century.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The Adversarial-neural Event Model (AEM) outperforms K-means, LEM, and DPEMM in event extraction from online texts.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "KNN, RF, SVM, and MLP have been trained.", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "Unanswerable.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The article discusses various approaches to grammar induction, including probabilistic context-free grammars (PCFGs) and compound PCFGs. PCFGs are grammars with rule probabilities that are specified by a probabilistic grammar. Compound PCFGs are a type of PCFG that use a compound probability distribution to model the generative process. The article states that the compound PCFG approach is able to induce linguistically meaningful grammars by optimizing log likelihood, and that this can be done through a collapsed approach using amortized inference", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "InferSent, Universal Sentence Encoder, and SBERT are evaluated in the article.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Multi-granularity and multi-tasking neural architecture design involve combining different levels of granularity and different tasks in a single model, allowing for more efficient and effective propaganda detection.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "The article does not mention the specific metrics used for evaluation.", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "The size of the TweetQA dataset is not specified in the article.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "The data in the article is in the Europarl language pair, which consists of English (INLINEFORM0 and INLINEFORM1) and French.", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "The backoff strategies in the article involve falling back on a generic word recognizer trained on a larger corpus when the word recognizer used in the main defense model encounters unobserved and rare words. The goal is to handle these unseen words and improve the defense model's robustness against adversarial attacks.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The state-of-the-art models for automatic judgment prediction in civil cases are based on the Legal Reading Comprehension (LRC) framework, which incorporates the reading mechanism to better model the complementary inputs of fact descriptions, plaintiff's pleas, and related law articles. The LRC framework has been shown to improve the performance of judgment prediction compared to previous text classification-based approaches.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "No.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "Two metrics are proposed to evaluate keyphrase generation: F1 score and diversity of generated sequences.", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The CORD-19 dataset is a collection of scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. It contains over 45,000 articles contributed by hospitals and medical institutes all over the world and is used to study the correlation between radiological findings and COVID-19.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "Unanswerable.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "The article discusses the use of automatic diacritic recovery in Arabic language processing, specifically focusing on the recovery of both core-word diacritics (CW) and case-endings (CE). The authors propose two separate Deep Neural Network (DNN) architectures for recovering both types of diacritic types, using character-level and word-level bidirectional Long-Short Term Memory (biLSTM) based recurrent neural models for CW diacritic and CE recovery respectively. They train models for both Modern Standard Arabic (MSA) and Classical Arabic (CA", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "The Neural User Simulator (NUS) is introduced, which outputs natural language and learns from a corpus of recorded dialogues.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "The article discusses the use of natural language processing (NLP) and machine learning (ML) techniques for detecting online hate speech. The authors propose a transfer learning approach using the pre-trained language model BERT and introduce new fine-tuning strategies to improve the performance of hate speech detection. The article also discusses previous studies on online hate speech and offensive language, and transfer learning. The authors conclude that their model outperforms previous works and can detect some biases in the process of collecting or annotating datasets.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "Disinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "The article describes a method for performing both grammatical error correction and style transfer with a single trained model, using zero-shot neural machine translation. The model is trained on bilingual examples in both directions, and then used to translate the input into the same language for both tasks. The success of the model is evaluated in terms of performance in grammatical error correction and style transfer, and the model is shown to be effective in correcting errors and adapting the output into a desired style. The approach is applicable to multiple languages within the same model, and the paper presents thorough quantitative and qualitative manual evaluation of the model", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "The article discusses a method to learn more interpretable dense word embeddings by modifying the objective function of the GloVe algorithm. The proposed method aligns the dimensions of the embedding vector with predefined concepts, resulting in an increase in interpretability. The article demonstrates the effectiveness of the method through quantitative and qualitative evaluations and shows that the semantic structure of the original embedding is not harmed. The method can be applied to other word embedding algorithms with a similar learning routine. The proposed methodology can also be helpful in computational cross-lingual studies.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "The document-level encoder proposed in the paper is based on the pretrained Bert model, which is trained on a large corpus of text and uses a masked language modeling objective. The encoder is designed to encode a document and obtain representations for its sentences. This is a novel approach in text summarization, as previous models have focused on sentence-level encodings. The document-level encoder is a key component of the proposed summarization models, which achieve state-of-the-art results in both extractive and abstractive summarization tasks.", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "The baseline is not mentioned in the article.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The article proposes a new structured-data encoder that assumes the data structure should be hierarchically captured. The decoder is chosen to be a classical module as used in previous models. The encoder introduces a hierarchical attention mechanism to compute the hierarchical context fed into the decoder. The experiments are conducted on the RotoWire benchmark, which contains around $5K$ statistical tables of NBA basketball games paired with human-written descriptions. The proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics. The article also mentions that the proposed approach", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "Unanswerable.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "Improvement over the best performing state-of-the-art is not mentioned in the article.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "The article proposes a new approach that enables learning for different manipulation schemes with the same single algorithm. The authors adapt an off-the-shelf reward learning algorithm from the reinforcement learning literature for data manipulation in the supervised setting. The algorithm is used to learn data augmentation and weighting, and it shows improved performance over strong base models and previous manipulation methods.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "The article discusses the use of reinforcement learning to solve text-based games. The authors propose a methodology that uses the Go-Explore algorithm to extract high-performing trajectories in these games, and then trains a Seq2Seq model on these trajectories to play the game. The experiments are performed on the CoinCollector game and the \"First TextWorld Problems\" dataset, which consists of 4,440 games generated using the Z-Machine. The results show that the proposed methodology is effective in solving text-based games, with the Seq2Seq model trained on Go-", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "The state of the art models for sarcasm detection are unanswerable.", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "The article discusses a scientific article that introduces a new model called the Transformer, which is a sequence-to-sequence (seq2seq) model used in natural language processing (NLP). The Transformer uses multi-head attention mechanisms to map an input sequence to an output sequence, allowing for the dynamic, context-dependent propagation of information within and across sentences. The article also introduces a new model called the \"sparse attention\" Transformer, which allows for the attention to be limited to a contiguous span of past tokens, rather than the entire sequence. The article concludes with a discussion of", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The additive modification to the objective function is the introduction of an additional term that favors an increase in the value of the embedding vector dimension corresponding to the concept that the particular word belongs to.", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "Yes.", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "Yes.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "The article discusses the use of a deep learning framework for detecting fake news on social media. The authors propose a classification framework based on a multi-layer formulation of Twitter diffusion networks, which allows them to disentangle different social interactions on Twitter, such as tweets, retweets, mentions, replies, and quotes, to build a diffusion network composed of multiple layers. They compute structural features separately for each layer and use a logistic regression model to classify news articles as either credible or non-credible. The article presents experimental results showing that the use of a multi-layer, disentang", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "Reuters database", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "The article discusses the use of computational approaches to assist human moderators in online conversation platforms. The model introduced in the article is capable of forecasting the derailment of a conversation based on early warning signs, and can capture the dynamics of a conversation as it develops. The article also discusses the challenges of forecasting conversational events, including the need to capture inter-comment dynamics and deal with an unknown horizon. The article concludes with a discussion of future work, including the need to address questions about the usefulness of early warnings to moderators and the ethical implications of such a system.", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "The article discusses the use of RNNs in Statistical Machine Translation (SMT) and their application to morphologically rich languages. The use of RNNs has revolutionized the approaches to automated translation, but the main downside is the heavy corpus requirement. The efficiency of an MT system depends on the availability and size of parallel corpora, and the syntactic divergence between the languages. Morphologically rich languages are difficult to translate due to the structural and semantic discordance with English. The article also discusses the use of word2vec and morph2vec models for", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "The model is more reliable for spelling, word order, and grammatical errors in grammatical error correction. It is less reliable for lexical choice errors.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "Unanswerable.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "The article does not mention the specific datasets used in the experiment.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "Yes.", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "The article describes a scientific competition called BioASQ, which is a biomedical document classification, document retrieval, and question answering competition. The competition is in its seventh year and the article describes the system used by the authors to participate in the question answering task. The system is based on BioBERT, a biomedical version of the BERT model, and is fine-tuned for the biomedical question answering task. The article describes the results of the system in the competition and also presents related work on question answering tasks using different models.", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "Yes.", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "The baseline is a reference point or starting point for comparison in an experiment or analysis. In the context of the article, the baseline refers to the initial performance of the models before any improvements are made. The article does not provide specific details on the baseline performance of the models, as the focus is on the improvements achieved through various techniques.", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "The paper proposes to use a joint model that combines textual features with visual features to assess the quality of documents. The visual features are based on a fine-tuned Inception V3 model, while the textual features are based on a hierarchical biLSTM. The joint model is trained on a Wikipedia dataset and an arXiv dataset, and achieves state-of-the-art results in both datasets.", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "Diversity of keyphrases is typically measured by evaluating the number of unique keyphrases generated by a model, as well as the diversity of the generated sequences. One common metric for measuring diversity is the F1 score, which takes into account both precision and recall.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "The article describes a scientific competition called BioASQ, which involves submitting answers to biomedical questions. The authors state that their system scored near the top for factoid questions in the third test batch set and achieved the highest recall score for List-type questions in the fourth test batch set. They also mention that they used a BioBERT BIBREF0 based system, which is a contextual word embedding model fine-tuned for the biomedical question answering task. The article also describes related work on question answering tasks, including the use of BERT BIBREF1, a contextual word embedding model,", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Improved KB Relation Detection", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "KAR is an end-to-end MRC model that utilizes general knowledge to assist its attention mechanisms.", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "The article discusses the effectiveness of Rouge, a widely used metric in text summarization evaluation, for assessing the quality of scientific summaries. The study reveals that Rouge scores do not correlate well with manual Pyramid scores, challenging its effectiveness for scientific summarization. The article proposes an alternative metric, Sera, based on relevance analysis, which shows higher and more consistent correlations with manual judgments.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "Yes.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "The accuracy merits of the approach are demonstrated through empirical evaluation on multiple real-world datasets, showing an average improvement of 24.3% in AUC compared to the state of the art.", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "The article describes a study on neural machine translation (NMT) and how the models decide the importance of output words. The study uses a gradient-based approach to estimate word importance by exploiting the intermediate gradients in NMT generation. The word importance is useful for understanding NMT by identifying under-translated words, and the gradient-based approach is found to be superior to other methods in estimating word importance. The study also analyzes the linguistic behaviors of words with importance and finds that certain syntactic categories have higher importance while the categories vary across language pairs.", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "The article discusses the use of dedicated question answering (QA) systems to address the challenge of personalization in large-scale information retrieval. The authors propose a new approach called Recognizing Question Entailment (RQE) to retrieve answers to a premise question by retrieving inferred or entailed questions. The effectiveness of the RQE-based QA approach is evaluated using open-domain and clinical datasets. The authors also present two machine learning and deep learning methods for RQE and compare their performance. The article concludes with a discussion of the limitations of the current approach and future", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The authors used twitter to build a large-scale dataset of ironic and non-ironic sentences for their study on irony generation.", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "The article does not provide the size of the real-world civil case dataset.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "The proposed task of concept-map-based MDS is defined as follows: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. The evaluation of this task involves creating a new dataset, developing a crowdsourcing scheme, and providing an annotation tool. The concept map is defined as a labeled graph showing concepts as nodes and relationships between them as edges. The task is complex and consists of several interdependent subtasks, including extracting appropriate labels for concepts and relations, recognizing different expressions that refer to the same concept across multiple documents, selecting the", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The article mentions several publicly available datasets that have been used in previous studies on online hate speech and offensive language detection, including Twitter, Reddit, YouTube, and Wikipedia. However, it does not provide specific details on which datasets are used in the proposed transfer learning approach.", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "Unanswerable.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "Yes.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "Future work will investigate the applications of dynamic balancing methods between RL and MLE in text generation, and improve the sensationalism scorer.", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "They compared the performance of various LSTM models, including layer-wise trained models, transfer learning models, and models trained with different loss functions and optimization techniques.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "The evaluation criteria and metrics used to evaluate the generated text include factual correctness, fluency, and product-readiness. The factual correctness was measured by the number of factual errors in the generated text compared to the corrected variant, and the fluency was measured by the word error rate (WER) of the generated text compared to its corrected variant. The product-readiness was evaluated by two journalists from the STT agency, who edited the generated text to a form ready for production use or direct publication.", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "The article reports several evaluation metrics, including accuracy, F1 score, and performance improvement over previous methods.", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "The strong baseline is Seq2Seq with attention.", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the hate speech detection task and comparing them to the annotations made by human annotators. They found that the model was able to correctly identify some samples that were misclassified by the annotators, indicating that it was able to capture some of the biases present in the data.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The article proposes Human Level Attributes (HLAs) as a way to model character profiles in dialogue agents. HLAs are based on tropes collected from TV Tropes and are used to trace dialogue data back to both its context and associated human-like qualities. The authors also propose a system called ALOHA that maps characters to a latent space based on their HLAs and determines which are most similar in profile and identity, allowing for the recovery of language styles of specific characters. The article demonstrates the effectiveness of ALOHA in character language style recovery and shows that it is stable across", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "The data was collected through crowdsourcing on Amazon Mechanical Turk, with workers asked to formulate questions and answers based on script scenarios rather than specific texts.", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "The article does not provide the absolute accuracy of the system.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "On the SimpleQuestions and WebQuestions benchmarks, they achieve the state of the art.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "They show there is space for further improvement by conducting a human study and finding that a majority of questions the system couldn't answer were actually answerable. This suggests that the system is not yet achieving its full potential and there is still room for improvement.", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "The article discusses a novel approach to extractive summarization for long documents, which captures both global and local context. The global context refers to the overall structure of the document, while the local context refers to the specific topic being discussed within a section or subsection of the document. The proposed model integrates recent findings on neural extractive summarization and incorporates a parameter-lean and modular architecture. It is evaluated on two large scientific paper datasets and achieves state-of-the-art performance. The benefits of the model come exclusively from modeling the local context, even for the longest documents.", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "KBQA stands for Knowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "The authors show that their learned policy generalizes better than existing solutions to unseen games by training a Seq2Seq model on the high-performing trajectories found by the Go-Explore algorithm, which allows the agent to operate in the full, unconstrained action space of natural language and systematically generalize to new text-based games with no or few interactions with the environment.", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "Unanswerable.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The state-of-the-art system is not mentioned in the article.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "Unanswerable.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The article does not provide information on which stock market sector achieved the best performance.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "The baseline model used in the BioASQ 7 competition is a BioBERT BIBREF0 based system that is fine-tuned for the biomedical question answering task. The system scored near the top for factoid questions in all test batches and achieved the highest 'MRR' score for Factoid Question Answering task in the third test batch set.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "Unanswerable.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The article mentions that several real-world datasets are used for the experiments, including CyberAttack and PoliticianDeath.", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "The state of the art described in the paper is the development of a hybrid conceptual architecture and its implementation with a finance advisory system for multi-party conversational systems (MPCS). The paper also discusses the challenges of engineering MPCS and explores the possibility of decoupled interaction norms specification and a multi-party governance service to enforce compliant utterances.", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "The article discusses the use of monolingual data in Neural Machine Translation (NMT) systems and proposes various techniques to integrate such data. The study focuses on the Europarl domain, which consists of two language pairs: English-German and English-French. The NMT systems are trained on out-of-domain parallel corpora and limited in-domain data. The article presents several baseline NMT systems and compares their performance with and without the use of monolingual data. The study also explores the effects of Back-Translation (BT) and the use", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "The article does not mention the type of classifiers used.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "The article describes the use of neural machine translation (NMT) for the low-resourced language pair of Japanese INLINEFORM0 Vietnamese. The authors attempt to build NMT systems for this pair and conduct experiments using advanced methods to improve the quality of the systems. They use subword units, byte-pair encoding (BPE), and a variant of BPE for Vietnamese, and experiment with different segmentation methods for both languages. The article also describes the use of back-translated texts and mix-source data to increase the amount of training data. The main contributions of the paper are the description of the", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "The proposed model, ALOHA, outperforms the baselines by a noticeable margin, achieving a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The data in the new corpus is sourced from various domains and registers, including comments to articles, discussion forum posts, blog posts, and professional newswire articles. The data come from a variety of sources and no assumptions about its actual content with respect to argumentation can be drawn.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to understand the impact of gender representation in media on speech recognition systems and to question the fairness of these systems in terms of gender.", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "The paper explores various embedding techniques, including word embeddings and second-order co-occurrence vectors.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They build a predictive model of dogmatism using statistical analysis of linguistic features in social media posts.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "The model is applied to two datasets: the highly curated `Conversations Gone Awry` dataset and the in-the-wild moderation of the popular subreddit ChangeMyView.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Facebook pages", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The article discusses the use of neural models to predict instructor intervention in MOOC forums. The models use a thread structure and the sequence of posts to make predictions. The article also discusses the use of attention models to infer the latent context of the thread, which is defined as a series of posts that trigger an intervention. The article concludes that the proposed models generalize over the assumptions made by previous works and improve the state-of-the-art for instructor intervention in MOOC forums.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "The article discusses the use of cognitive features extracted from eye-tracking data to improve sarcasm detection in NLP processing systems. The authors created a novel framework that combines traditional linguistic features with cognitive features obtained from readers' eye-movement data, resulting in an improved success rate for sarcasm detection. The article provides examples of how the system performs on different sentences and compares its classification accuracy to the best available system. The authors also discuss the limitations of the approach and suggest future work in exploring deeper graph and gaze features.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The baselines in the article are not explicitly mentioned. However, the authors propose a new method called Cell-aware Stacked LSTM (CAS-LSTM) and evaluate it on multiple benchmark datasets.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Yes.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "Unanswerable.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "The article does not provide specific details on the datasets used in evaluation. It only mentions that the experiments were conducted on four languages, which were part of SemEval-2016 Challenge Task 5. The article does not provide any further information on the datasets used for evaluation. Therefore, the answer to this question is \"unanswerable\".", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "They use datasets with transcribed text.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "Pivot-based baselines are methods that translate source language to a pivot language, and then translate the pivot language to the target language. This approach is commonly used in transfer learning for neural machine translation (NMT).", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Yes.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "The article describes the DeepMine database, a large speech corpus collected using crowdsourcing and designed for text-dependent and text-prompted speaker verification, as well as text-independent speaker verification evaluation. The database contains speech data from 1969 speakers, with 1149 male and 820 female speakers, and includes fixed common phrases, random sequences of words, and phrases with word- and phoneme-level transcription. The article provides details on the data collection process, including the use of an Android application and post-processing steps to", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "The article mentions two datasets, EmotionLines and Friends. EmotionLines is a dialogue dataset composed of two subsets, Friends and EmotionPush, and includes $1,000$ English dialogues, each divided into a few consecutive utterances. The dataset is annotated by five annotators on a crowd-sourcing platform, and the labeling work is based on the textual content. Friends is a speech-based dataset that includes $1,000$ English dialogues from the TV sitcom Friends, and EmotionPush is a chat-based dataset that includes $1", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "Unanswerable.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "The article describes the use of a Recurrent Neural Network (RNN) for end-to-end speech recognition.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention used in a Chinese word segmentation model. It captures localness and directional information by adding a directional mask and a Gaussian mask to the standard self-attention mechanism. The model uses only unigram features and achieves new state-of-the-art performance on the SIGHAN Bakeoff 2005 benchmark dataset.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Attention is different from alignment in cases where the attention model uses the most relevant parts of the source sentence at each translation step, and where the attention mechanism captures information beyond alignment in the case of verbs.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "The article describes the use of a hierarchical three-level annotation model to create a new large publicly available dataset of English tweets. The article also discusses related work on abusive language identification, including aggression identification, bullying detection, hate speech, toxic comments, and offensive language. The article proposes a new hierarchical annotation model that captures the similarities among prior work on abusive language and distinguishes between whether the language is directed towards a specific individual or entity or towards a generalized group and whether the abusive content is explicit or implicit. The article also describes the use of", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "The baseline is a neural network-based Named Entity Recognition (NER) system for the Nepali language, which uses grapheme-level representations and POS one-hot encoding vectors.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Unanswerable.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "Unanswerable.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "Yes.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "The article proposes a neural-based Named Entity Recognition (NER) model for Nepali language using the latest state-of-the-art architecture. The model uses grapheme-level features and does not require any hand-crafted features or data pre-processing. The authors compare their model with various baselines, including Support Vector Machine (SVM) and Hidden Markov Model (HMM) with rule-based approaches, and show that their model performs better in terms of entity recognition accuracy. They also release a dataset to support future research and perform empirical evaluation of their model with state-of-", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "The article discusses the automatic summarization of discharge summary notes, which are critical to ensuring continuity of care after hospitalization. The authors explore the upper bound on extractive summarization and develop a classifier for labeling the topics of history of present illness notes. They use the MIMIC-III database, which contains electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012. The dataset contains 55,177 discharge reports and 4,475 discharge", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "BERT", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Attention captures useful information beyond traditional word alignment, such as syntactic and semantic relationships between words.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "The article does not provide information on the length of the dataset for each step of the hierarchy.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "The baselines were BOW, TFIDF, and various neural-based word embedding methods, as well as Logistic Regression, Random Forest, TextCNN with initial word embedding as GloVe, and the proposed model.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The article discusses the use of tag recommendations to annotate e-books, specifically focusing on the use of editor tags and user-generated search terms from Amazon to improve the quality of tag recommendations. The authors evaluate various tag recommendation approaches, including popularity-based, similarity-based, and hybrid approaches, using a dataset containing editor tags, Amazon search terms, and Amazon review keywords. The study finds that combining both data sources enhances the quality of tag recommendations and that approaches that train on Amazon search terms provide poor performance in terms of accuracy but deliver good results in terms of semantic similarity and recommendation diversity.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The article does not provide a specific architecture for the models mentioned. It only mentions that they use an end-to-end approach and provide baseline results for various language pairs.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "The article does not provide specific information on the data used by the authors. However, they mention using the Penn Treebank and WikiText2 datasets for their experiments on language modeling.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "Unanswerable.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The article does not mention a specific baseline method.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "The ancient Chinese dataset comes from the parallel corpus crawling and cleaning, paragraph alignment, clause alignment based on aligned paragraphs, and augmenting data by merging aligned adjacent clauses.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The article does not mention which datasets are used for evaluation. Therefore, the answer is \"unanswerable\".", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "The more predictive set of features to detect fake news is a combination of words embeddings, style, and morality features.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "The proposed model is compared to several baseline models, including traditional features such as TF-IDF, as well as deep features such as Doc2vec, LSTM, and ELMo.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "The article does not provide information on the size of the collection of COVID-19 literature.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "Unanswerable.", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "The article does not mention the specific datasets used for experiments.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "Macaw has a modular design with the goal of making it easy to configure and add new modules such as a different user interface or different retrieval module. The overall setup follows a Model-View-Controller (MVC) like architecture. The design decisions have been made to smooth the Macaw's adoptions and extensions. Macaw is implemented in Python, thus machine learning models implemented using PyTorch, Scikit-learn, or TensorFlow can be easily integrated into Macaw. The high-level overview of Macaw is depicted in FIGREF8. The user interacts with the interface and", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "The article describes experiments in finding out whether lemmatization helps modern contextualised embeddings, specifically the ELMo model, in word sense disambiguation. The authors trained ELMo models on English and Russian corpora, consisting of the English Wikipedia dump from February 2017 and a concatenation of the Russian Wikipedia dump from December 2018 and the full Russian National Corpus (RNC). The RNC texts were added to the Russian Wikipedia dump so as to make the Russian training corpus more comparable in size to the English one. The texts were tokenized and lemmatized with the UDP", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "Yes.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The article discusses the relation classification task, where the goal is to assign sentences with two marked entities to a predefined set of relations. The authors propose using neural networks (NNs) for this task, specifically convolutional neural networks (CNNs) and recurrent neural networks (RNNs), and present a new context representation for CNNs. They also introduce a ranking loss function for RNNs and combine CNNs and RNNs using a voting scheme. The article is based on the SemEval 2010 shared task 8 dataset, which consists of manually annotated data for relation classification.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The encoder in the system described in the article is an LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The article proposes a neural probabilistic model that captures word embeddings in a lower dimensional space, considering both the word's semantic similarity and its context. The model is trained using a variant of max-margin objective based on the asymmetric KL divergence energy function. The article also presents a KL approximation between Gaussian mixtures to capture textual entailment. The proposed approach is evaluated on benchmark word similarity and entailment datasets, showing better performance than existing models.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "Unanswerable.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The article describes a scientific article that proposes a new task of generating plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences. The authors release a new dataset of 180K+ recipes and 700K+ user reviews for this task and introduce new evaluation strategies for generation quality in instructional texts. They explore a hierarchical approach that condition", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "Unanswerable.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Unanswerable.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "The article describes a new dataset called ReviewQA, which is a question-answering dataset for hotel reviews. The dataset contains 8 competencies that evaluate the reader's ability to understand and reason with relevant information extracted from a document. The questions are natural language questions over the 500 most popular articles of Wikipedia, and the answers are spans of text extracted from the source documents. The dataset is designed to challenge models to perform strongest reasoning tasks.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "The Random Kitchen Sink (RKS) approach is a method used in natural language processing tasks, including offensive language detection, by explicitly mapping data vectors to a space where linear separation is possible. It provides an approximate kernel function via explicit mapping and has been explored for sentiment analysis, background-foreground separation, load forecasting, and saliency detection.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "The results show that error detection performance is improved by using artificially generated data created by the proposed methods. The combination of the pattern-based approach with the machine translation system gave the best overall performance on all datasets.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "The article describes an experiment in which the authors incorporated subword information into word embedding models and evaluated the resulting word embeddings using intrinsic and extrinsic evaluations. The authors used the LexVec model, which is a word embedding model that factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent, and modified it to learn word representations as the sum of a unique vector and a set of shared character n-gram vectors. The authors evaluated the impact of subword information on word embeddings by comparing the use of n-gram subwords and unsupervised morphemes", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Unanswerable.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "The article does not provide a clear answer to this question. It mentions that the dataset was collected from 35 mobile applications and their privacy policies, but it does not provide any information about the experts who were used for annotation. It is possible that the authors of the article did not include this information in the final version of the article, or that the information was not available at the time of writing. Therefore, the question is unanswerable.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "They compare their approach to a baseline model that does not use pre-trained language model representations.", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "Unanswerable.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "The article describes the creation of a corpus for training machine learning models, the development of a gold-standard test corpus, and the evaluation of established approaches for named entity recognition in the Armenian language. The authors used Sysoev and Andrianov's modification of the Nothman et al. approach to automatically generate data for training a named entity recognizer. They also developed GloVe word embeddings and baseline results for 3 different models on the proposed benchmark data set. The article does not mention any specific named entity recognition models.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "The article does not mention which datasets are used to evaluate the proposed method.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "The article does not mention which baseline model is used.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "They addressed cyberbullying on social media platforms such as Formspring, Twitter, and Wikipedia.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The article discusses the challenges of language identification (LID) in South Africa, where there are 11 official languages. The authors propose a hierarchical naive Bayesian and lexicon-based classifier for LID of short pieces of text. The algorithm is evaluated against recent approaches using existing test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) 2015 and 2017 shared tasks. The article also reviews existing works on the topic and summarizes the remaining research problems. The authors suggest that data harvesting, building", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "The article describes a study that uses a dataset of Twitter tweets annotated with depression-related classes. The dataset is constructed based on a hierarchical model of depression-related symptoms, and it contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. The dataset is encoded with 7 feature groups with associated feature values binarized to represent potentially inform", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The article does not provide information on the performance of the system.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "IMDb dataset is used for sentiment analysis.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "PolyResponse is a restaurant search and booking system that is currently available in 8 languages: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "The Adversarial-neural Event Model (AEM) outperforms K-means, LEM, and DPEMM in event extraction from online texts.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "The article does not provide information on the sources of the datasets used in the study.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "Yes.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "InferSent, Universal Sentence Encoder, and SBERT are evaluated in the article.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Unanswerable.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "The article proposes the use of context tweets, which provide additional information about the context in which an abusive tweet was made, to improve the accuracy of abusive language detection. The article also explores the use of various features and models, including traditional machine learning classifiers and neural network models, to detect abusive language.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The best performing model among the author's submissions is ranked 3rd in the Fragment-level Classification (FLC) task with a score of 0.673 on the test set.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19, which consists of subparts of DTA corpus and contains texts from the 16th to the 20th century.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "The size of the TweetQA dataset is not specified in the article.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "KNN, RF, SVM, and MLP have been trained.", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "Multi-granularity and multi-tasking neural architecture design involve combining different levels of granularity and different tasks in a single model, allowing for more efficient and effective propaganda detection.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "The new context representation is proposed in the article and is based on the idea of using all parts of the sentence, including the relation arguments, left of the relation arguments, between the arguments, and right of the arguments, and paying special attention to the middle part.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "Unanswerable.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The backoff strategies in the article involve falling back on a generic word recognizer trained on a larger corpus when the word recognizer used in the main defense model encounters unobserved and rare words. The goal is to handle these unseen words and improve the defense model's robustness against adversarial attacks.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The CORD-19 dataset is a collection of scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. It contains over 45,000 articles contributed by hospitals and medical institutes all over the world and is used to study the correlation between radiological findings and COVID-19.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "The article does not mention the specific metrics used for evaluation.", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "The article discusses various approaches to grammar induction, including probabilistic context-free grammars (PCFGs) and compound PCFGs. PCFGs are grammars with rule probabilities that are specified by a probabilistic grammar. Compound PCFGs are a type of PCFG that use a compound probability distribution to model the generative process. The article states that the compound PCFG approach is able to induce linguistically meaningful grammars by optimizing log likelihood, and that this can be done through a collapsed approach using amortized inference", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The article discusses the use of automatic diacritic recovery in Arabic language processing, specifically focusing on the recovery of both core-word diacritics (CW) and case-endings (CE). The authors propose two separate Deep Neural Network (DNN) architectures for recovering both types of diacritic types, using character-level and word-level bidirectional Long-Short Term Memory (biLSTM) based recurrent neural models for CW diacritic and CE recovery respectively. They train models for both Modern Standard Arabic (MSA) and Classical Arabic (CA", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "The Neural User Simulator (NUS) is introduced, which outputs natural language and learns from a corpus of recorded dialogues.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
