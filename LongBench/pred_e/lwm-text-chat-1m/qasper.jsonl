{"pred": "They looked at pages such as FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The article mentions that the classification accuracy of the system is evaluated using F-scores and Kappa statistics.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "Yes, jiant is compatible with models in any programming language.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "No, this method does not help in sentiment classification task improvement.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Improvements of small-scale unbalanced datasets can be significant when sentence representation is enhanced with topic information. The article reports that the accuracy and F1 score of a model that applies ELMo and topic-attention has increased by 7.36% on accuracy and 9.69% on F1 score compared to the base model. Additionally, the model achieved an average AUC of 0.8196 compared to the base model's average AUC of 0.7189. The article also shows that the model can benefit from ELMo, which considers contextual", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "Carrot2 Lingo and CLUTO were used for clustering.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "The latent context used to predict instructor intervention is a series of posts that trigger an intervention.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "They looked at pages such as FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The baseline was a model that used plain stacked LSTMs.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "The article mentions that the classification accuracy of the system is evaluated using F-scores and Kappa statistics.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "Yes, the experts are comparable to real-world users.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "Yes, jiant is compatible with models in any programming language.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "The two datasets used in evaluation were Amazon reviews and restaurant reviews from four different languages (Spanish, Turkish, Dutch, and Russian).", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "No, this method does not help in sentiment classification task improvement.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "They use datasets with transcribed text.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "BERT", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Improvements of small-scale unbalanced datasets can be significant when sentence representation is enhanced with topic information. The article reports that the accuracy and F1 score of a model that applies ELMo and topic-attention has increased by 7.36% on accuracy and 9.69% on F1 score compared to the base model. Additionally, the model achieved an average AUC of 0.8196 compared to the base model's average AUC of 0.7189. The article also shows that the model can benefit from ELMo, which considers contextual", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "The article mentions that the EmotionLines dataset was used for the multi-utterance emotion recognition task.", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "Carrot2 Lingo and CLUTO were used for clustering.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "The pivot-based baselines are pivoting and pivot-synthetic.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention that replaces the standard self-attention in the Transformer encoder. It expects to pay attention to the adjacent characters of each position and cast the localness relationship between characters as a fix Gaussian weight for attention. The Gaussian weight only relies on the distance between characters, and the larger distance between characters, the smaller the weight is, which makes one character can affect its adjacent characters more compared with other characters. To combine the Gaussian weight to the self-attention, we produce the Hadamard product of Gaussian weight matrix $G$", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The evaluation protocols provided in the article allow researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration, and phrase text on the performance.", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Attention is different from alignment in cases where the attention model captures information beyond alignment, such as in the case of verbs where the attention is more distributed compared to nouns. This allows for a better translation of verbs, but also means that training the attention with explicit alignment labels may not be useful.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "The system uses a limited amount of training data from the non-English language.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "They use a RNN with two 2D-convolutional layers, followed by seven bi-directional recurrent layers and a fully-connected layer with softmax activation.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "The baseline is a model that is used as a reference to compare the performance of other models.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "The paper is published in 2021.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Around 500 different workers were involved in the annotation.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The models used in the experiment are a linear SVM and a bidirectional Long Short-Term-Memory (BiLSTM) model.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "They explore neural network models, specifically BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS.", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "The vocabulary of word-like or phoneme-like units is automatically discovered using acoustic models trained from the transcribed speech of its evaluation corpus.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "The keyphrase extraction models that were reassessed are not specified in the given article.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "They test BERTBase, BERTBase with whole word masking, ERNIE BIBREF4, and ERNIE 2.0 BIBREF5.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "The MIMIC-III database was used as a source of electronic health records for patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Yes, pre-training is effective in their evaluation as it offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "The dataset for each step of hierarchy is not specified in the given article.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "Attention captures other relevant information beyond alignments, which is already being captured by the attention model.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The baselines mentioned in the article are BOW and TFIDF.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "They look at 19 different algorithms to recommend tags for annotating e-books.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The model architecture is not specified in the given article.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "They used a scientific article and a question.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "No, they do not mention using graphical models in their work.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The baseline method used is word2vec.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "Yes, the answered questions measure for the usefulness of the answer.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "Prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models because they require the student and teacher models to share the same vocabulary and output space, which limits their potential to further reduce model sizes.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The article mentions using the XNLI dataset for classification task and the Universal Dependencies v2.4 dataset for parsing task.", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "The ancient Chinese dataset comes from collecting 1.7K bilingual ancient-modern Chinese articles from the internet, including articles written by celebrities of that era and history records in several dynasties (about 1000BC-200BC).", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "No, the dataset is not balanced between speakers of different L1s.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "The more predictive set of features to detect fake news is a rich set of features borrowed from previous works in fake news, bias, and rumors detection.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "The proposed model is compared to traditional models such as TF-IDF features and deep models such as Doc2vec and CNN.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "They build word embeddings for English and German.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "\nMacaw consists of multiple actions, each action is a module that can satisfy the information needs of users for some requests. For example, search and question answering can be two actions in Macaw. Even multiple search algorithms can be also seen as multiple actions. Each action can produce multiple outputs (e.g., multiple retrieved documents).", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "The collection of COVID-19 literature contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "The number of electrodes used on the subject in EEG sessions is not specified in the given article.", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "The article mentions that the corpus contains Persian and English accents.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Yes, their indexing-based method can be applied to create other QA datasets in other domains, and not just Wikipedia.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "They train their models on the relation classification dataset of the SemEval 2010 task 8 BIBREF8 .", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The experiments are performed on the SQuAD dataset BIBREF3.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The qualitative experiments were performed on benchmark datasets such as SL, WS, WS-R, WS-S, MEN, MC, RG, YP, MTurk-287, MTurk-771, and RW. The metric used for comparison is 'AvgCos'. The results show that GM$\\_$KL achieves significantly better correlation score than w2g and w2gm approaches for most of the datasets.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "The corpora they trained ELMo on were the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018, which were added to the Russian National Corpus (RNC) to make the Russian training corpus more comparable in size to the English one.", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "They calculate variance from the model outputs by selecting a subset of the training data and choosing another distribution INLINEFORM3 so that the INLINEFORM4 pairs are drawn from INLINEFORM5.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The dataset contains a large panel of human-generated questions.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "They looked at pages such as FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The latent context used to predict instructor intervention is a series of posts that trigger an intervention.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "The article mentions that the classification accuracy of the system is evaluated using F-scores and Kappa statistics.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "The baseline was a model that used plain stacked LSTMs.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Yes, jiant is compatible with models in any programming language.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "No, this method does not help in sentiment classification task improvement.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Yes, the experts are comparable to real-world users.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The two datasets used in evaluation were Amazon reviews and restaurant reviews from four different languages (Spanish, Turkish, Dutch, and Russian).", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "They use datasets with transcribed text.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "Improvements of small-scale unbalanced datasets can be significant when sentence representation is enhanced with topic information. The article reports that the accuracy and F1 score of a model that applies ELMo and topic-attention has increased by 7.36% on accuracy and 9.69% on F1 score compared to the base model. Additionally, the model achieved an average AUC of 0.8196 compared to the base model's average AUC of 0.7189. The article also shows that the model can benefit from ELMo, which considers contextual", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "BERT was used as a neural model.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Carrot2 Lingo and CLUTO were used for clustering.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "The article mentions that the EmotionLines dataset was used for the multi-utterance emotion recognition task.", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The pivot-based baselines are pivoting and pivot-synthetic.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "The evaluation protocols provided in the article allow researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration, and phrase text on the performance.", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention that replaces the standard self-attention in the Transformer encoder. It expects to pay attention to the adjacent characters of each position and cast the localness relationship between characters as a fix Gaussian weight for attention. The Gaussian weight only relies on the distance between characters, and the larger distance between characters, the smaller the weight is, which makes one character can affect its adjacent characters more compared with other characters. To combine the Gaussian weight to the self-attention, we produce the Hadamard product of Gaussian weight matrix $G$", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The system uses a limited amount of training data from the non-English language.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "They use a Recurrent Neural Network (RNN) for end-to-end speech recognition.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "The paper is published in 2021.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "Attention is different from alignment in cases where the attention model captures information beyond alignment, such as in the case of verbs. The large portion of attention being paid to words other than alignment points is already capturing other relevant information, and training attention with alignments in this case will force the attention model to forget these useful information. This explains the mixed results reported when guiding attention to comply with alignments BIBREF9 , BIBREF7 , BIBREF8 .", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "The models used in the experiment are a linear SVM and a bidirectional Long Short-Term-Memory (BiLSTM) model.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "The baseline is a model that is used as a reference to compare the performance of other models.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "The vocabulary of word-like or phoneme-like units is automatically discovered using acoustic models trained from the transcribed speech of its evaluation corpus.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "Around 500 different workers were involved in the annotation.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The keyphrase extraction models that were reassessed are not specified in the given article.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "The MIMIC-III database was used as the source of the discharge summaries.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "They explore neural network models, specifically BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS.", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "Attention captures other relevant information beyond alignments, which is already being captured by the attention model.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "They test BERTBase, BERTBase with whole word masking, ERNIE BIBREF4, and ERNIE 2.0 BIBREF5.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "They look at 19 different algorithms to recommend tags for annotating e-books.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "Yes, pre-training is effective in their evaluation as it offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "They used a scientific article and a question.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "The dataset for each step of hierarchy is not specified in the given article.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "The baselines mentioned in the article are BOW and TFIDF.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "Prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models because they require the student and teacher models to share the same vocabulary and output space, which limits their potential to further reduce model sizes.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "The model architecture is not specified in the given article.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "No, they do not mention using graphical models in their work.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The ancient Chinese dataset comes from collecting 1.7K bilingual ancient-modern Chinese articles from the internet, including articles written by celebrities of that era and history records in several dynasties (about 1000BC-200BC).", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The baseline method used is word2vec.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "The more predictive set of features to detect fake news is a rich set of features borrowed from previous works in fake news, bias, and rumors detection.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "Yes, the answered questions measure for the usefulness of the answer.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "They build word embeddings for English and German.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "The article mentions using the XNLI dataset for classification task and the Universal Dependencies v2.4 dataset for parsing task.", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "The collection of COVID-19 literature contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "No, the dataset is not balanced between speakers of different L1s.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "14 electrodes were used on the subject in EEG sessions.", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "The proposed model is compared to traditional models such as TF-IDF features and deep models such as Doc2vec and CNN.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Yes, their indexing-based method can be applied to create other QA datasets in other domains, and not just Wikipedia.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "\nMacaw consists of multiple actions, each action is a module that can satisfy the information needs of users for some requests. For example, search and question answering can be two actions in Macaw.", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "The experiments are performed on the SQuAD dataset BIBREF3.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The article mentions that the corpus contains Persian and English accents.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "They train their models on the relation classification dataset of the SemEval 2010 task 8 BIBREF8 .", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The corpora they trained ELMo on were the English Wikipedia dump from February 2017 and the Russian Wikipedia dump from December 2018, which were added to the Russian National Corpus (RNC) to make the Russian training corpus more comparable in size to the English one.", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "The proposed method introduces performance boost for both BERT and XLNet, improving the F1 score by +0.58 for MRPC and +0.73 for QQP.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The qualitative experiments were performed on benchmark datasets such as SL, WS, WS-R, WS-S, MEN, MC, RG, YP, MTurk-287, MTurk-771, and RW. The metric used for comparison is 'AvgCos'. The results show that GM$\\_$KL achieves significantly better correlation score than w2g and w2gm approaches for most of the datasets.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "The encoder has an LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "They calculate variance from the model outputs by selecting a subset of the training data and choosing another distribution INLINEFORM3 so that the INLINEFORM4 pairs are drawn from INLINEFORM5.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The dataset contains a large panel of human-generated questions.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "No, other baselines were not tested to compare with the neural baseline.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The new corpus consists of 53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "They get the recipes from Food.com, which covers a period of 18 years (January 2000 to December 2018).", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "The 10 other phenotypes that are annotated are \"None\" and \"Unsure\".", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "They evaluate their resulting word embeddings using the WordSim-353 Similarity (WS-Sim) and Relatedness (WS-Rel) BIBREF28 and SimLex-999 (SimLex) BIBREF29 datasets, and the Rare Word (RW) BIBREF20 dataset to verify if subword information improves rare word representation.", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Textual patterns are extracted from an annotated corpus.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "The essays on average range from 150 to 250 tokens.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "The Random Kitchen Sink approach is a machine learning method that combines multiple features to improve the classification accuracy of offensive language detection.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "No, they do not study numerical properties of their obtained vectors.", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "They compare the baseline to a Big Transformer model and a variant that shares token embeddings between the encoder and decoder.", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "Their results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "They test their method on hashtag prediction for social media posts, which is annotated by millions of users in the form of hashtags which link posts about the same topic together.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "The experts used for annotation were legal professionals with training in privacy law.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "Maximum Matching, Hidden Markov Model, N-gram language modeling, Conditional Random Fields, and Support Vector Machines have been applied to solve word segmentation in Vietnamese.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "The models evaluated were Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 and BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "The two datasets used to evaluate this paper are WordNet BIBREF0 and Freebase BIBREF1.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "The baseline model used is the classic RNN model.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "They addressed cyberbullying topics such as personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "No, they report results on both English and Arabic data.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "The dataset used for this study is an annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF0 and BIBREF13.", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "The Nguni languages (zul, xho, nbl, ssw), Afrikaans (afr), and English (eng) are similar to each other.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "The IMDb dataset is used for sentiment analysis.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "Their system achieved precision of over 95% and recall of over 85% when considering only high confidence samples.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "The approach outperforms K-means, LEM, and DPEMM.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "The sources of the datasets are not mentioned in the given article.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "PolyResponse engine is currently used in 8 languages for restourant search and booking system, including English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade).", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "No, the lexicon is not the same for all languages.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "InferSent BIBREF4 and Universal Sentence Encoder BIBREF5 are also evaluated.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The transfer learning tasks evaluated are MR, CR, SUBJ, MPQA, SST, TREC, and MRPC.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "The vocabulary is not specified in the given article.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "The additional features and context proposed are context tweets, which are text sequences of tweets that provide significant contextual information. These are directly used as an additional feature of neural network models.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes, they evaluate their learned representations on downstream tasks such as sentiment classification and word similarity computation in sentiment lexicon.", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "Yes, they build a model to automatically detect demographic, linguistic, and psycholinguistic dimensions of people.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "The best performing model among author's submissions is the ensemble+ of (II and IV) from each of the folds 1-3, i.e., $|{\\mathcal {M}}|=6$ models. It had a performance of 0.673 F1 on dev (external).", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The size of this dataset is not specified in the given article.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19, which consists of subparts of DTA corpus BIBREF11, a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP)", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "They propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The CORD-19 dataset is a collection of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses contributed by hospitals and medical institutes all over the world.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "Multi-granularity and multi-tasking neural architecture design refers to the use of multiple granularities and tasks in a neural network to improve performance in fine-grained propaganda detection tasks. This approach involves combining different types of features such as linguistic, layout, and topical features to capture the nuances of propaganda in different contexts. Multi-tasking involves combining multiple tasks such as propaganda detection and fragment detection to improve the overall performance of the system. Overall, this approach aims to provide a more comprehensive and accurate solution to fine-grained propaganda detection tasks.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "The state of the art methods for grammar induction involve carefully-crafted auxiliary objectives, priors or non-parametric models, and manually-engineered features to encourage the desired structures to emerge.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The size of the real-life dataset is not specified in the given article.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The corpus used to learn behavior is a recorded dialogue corpus.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "The backoff strategies work by predicting a word recognition model that can handle rare and unobserved words, and then using it as a backoff alternative to the foreground model. This reduces the number of options available to attackers and increases the robustness of the model.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The state-of-the-art models mentioned in the article are r-net and AoA, which are leading models on SQuAD leaderboard.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "BLEU-4, NIST-4, and ROUGE-4 are used for evaluation.", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "The semantically related words take larger values along the 32nd dimension, which is encoded with the concept JUDGMENT.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "The linguistics features used in the article are morphological patterns, POS tags, gender and number features, and affixes.", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "No, their framework does not automatically optimize for hyperparameters.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "The data is in English.", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "The experiments are performed on CoinCollector and CookingWorld games, which require multiple words at each step to solve and have sparse rewards.", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "The two news domains that are country-independent are mainstream and disinformation news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "The model captures biases from data collection and rules of annotation, which can lead to misclassification of tweets as hate speech or offensive language.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "The two metrics proposed are F INLINEFORM1 @ INLINEFORM2 and F INLINEFORM3 @ INLINEFORM4.", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The additive modification to the objective function is an additional cost term that favors a selective increase for a pre-specified input of concept words along each dimension.", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "The article mentions that the model can still lead to erroneous facts or even hallucinations, and suggests that further constraining the model on the data structure could prevent inaccurate or contradictory descriptions.", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "The novelty of their document-level encoder is that it can encode a document and obtain representations for its sentences, which is different from previous tasks that only focused on encoding individual sentences.", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "The use of morphological segmentation improves the efficacy of the attention mechanism by reducing the vocabulary size and increasing the number of input units per sentence, which makes it more demanding of the LSTM's memory units and the feed forward network of the attention model. Additionally, this behavior could also be attributed to the errors in the semi-supervised morphological segmentation due to the complexities of the Tamil language and the extent of the corpus.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "The dataset used in the experiment is a new one that was randomly collected from China Judgments Online, among which INLINEFORM0 cases are for training, INLINEFORM1 each for validation and testing. Among the original cases, INLINEFORM2 are granted divorce and others not. There are INLINEFORM3 valid pleas in total, with INLINEFORM4 supported and INLINEFORM5 rejected.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "This method achieves an absolute improvement of 18.2% over the best performing state-of-the-art, which is Pointer-Gen+ARL-SEN.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
