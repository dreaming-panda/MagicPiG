{"pred": " They did not provide information on which Facebook pages they looked at, so the answer is \"unanswerable\".", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": " Unanswerable.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": " Based on the article, the pivot-based baselines are:\n\n* BIBREF4: BILINGUAL PIVOT-TARGET MODEL\n* BIBREF5: PIVOT-TARGET MODEL\n* BIBREF12: PIVOTING BILINGUAL MODEL\n\nUnanswerable.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": " Unanswerable", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": " Yes. According to the article, the models of UPA, PPA, and APA use a structured representation of the complete thread as the context to predict intervention.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": " The paper was published in 2020, so the answer is \"2020\".", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": " Unanswerable", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": " According to the article, they use both datasets with transcribed text and determine text from the audio. Therefore, the answer is \"yes\".", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": " According to the article, the vocabulary of word-like or phoneme-like units is automatically discovered using the following steps:\n\n1. Construct the sparse acoustic similarity matrix between each pair of speech utterances.\n2. Identify word repetitions using the fast DTW algorithm.\n3. Threshold the resulting matrix to obtain a set of word-like or phoneme-like units.\n\nTherefore, the answer is: UTD.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": " Unanswerable. The article does not provide information on any other evaluation metrics beyond accuracy.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": " The article mentions the following datasets were used:\n\n* EmotionLines: a dataset of dialogues consisting of 1,000 utterances from 500 different speakers.\n* Friends: a dataset of dialogues from the TV show Friends.\n* EmotionPush: a dataset of dialogues from anonymous Facebook messenger chats.\n\nUnanswerable.", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": " No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": " The baselines were:\n\n* (i) plain stacked LSTMs\n* (ii) models that use different values of INLINEFORM0\n* (iii) models that integrate lower contexts via peephole connections\n\nUnanswerable: What is the effect of INLINEFORM0 on performance?", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": " Yes. According to the article, clustering algorithms were used to discover broad categories among the strengths, weaknesses, and suggestions for improvements found in the supervisor assessments.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": " Yes.", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": " According to the article, the following datasets were used:\n\n* MIMIC-III: a freely available, deidentified database containing electronic health records of patients admitted to the intensive care unit (ICU) at Beth Israel Deaconess Medical Center.\n* Date of Birth: a deidentified database containing electronic health records of patients admitted to the ICU at Beth Israel Deaconess Medical Center.\n* HIPAA: a deidentified database containing electronic health records of patients admitted to the ICU at Beth Israel Deaconess Medical Center.\n\nUnanswerable.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": " The article mentions that the evaluation is conducted on four datasets:\n\n* SemEval-2016 Task 5 datasets BIBREF7\n* Four datasets from the Semeval-2016 challenge BIBREF8\n\nTherefore, the answer is:\n\nYes", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": " According to the article, attention is different from alignment in the following cases:\n\n* When translating verbs, attention is more dispersed and does not always follow alignments, as shown in Figure FIGREF15.\n* For nouns, attention is more concentrated and follows alignments to some extent, as shown in Figure FIGREF15.\n\nTherefore, the answer to the question is \"unanswerable\" because the article does not provide information on how to determine when attention differs from alignment in other cases.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": " Yes. According to the article, the model used for end-to-end speech recognition is a Recurrent Neural Network (RNN) with a softmax output layer.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": " Yes.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": " Based on the article, the answer is:\n\nUnanswerable. The article does not provide information on the number of tags looked at.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": " Unanswerable. The article does not provide any information about how Gaussian-masked directional multi-head attention works.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " According to the article, the number of people who participated in their evaluation study of table-to-text models is unanswerable based on the information provided in the article.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": " The models used in the experiment are:\n\n* Linear SVM\n* Bidirectional LSTM\n* Convolutional Neural Network (CNN)\n\nUnanswerable.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " Yes. According to the article, they test the BERTBase-PN model.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": " Unanswerable. The article does not provide information on why prior knowledge distillation techniques are ineffective in producing student models with vocabularies different from the original teacher models.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": " Based on the article, the baseline is BiLSTM+CRF.\n\nUnanswerable: What is the volume of the dataset used for training the model?", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": " Yes. According to the article, the following keyphrase extraction models were reassessed: TopicRank, WFST, KP-Miner, KP-Rank, and TF-IDF.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": " They explore BiLSTM, CNN, and other neural network models.\n\nUnanswerable: What are the specific machine learning models used in the article?", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": " Based on the article, the more predictive set of features to detect fake news is:\n\nMorality, Emotion, Style, and Words Embeddings.\n\nUnanswerable: Can the article provide any explanation for why these features are more predictive?", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": " Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": " According to the article, the collection of COVID-19 literature contains over 45,000 scholarly articles. Therefore, the answer is:\n\nYes", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": " Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": " According to the article, the dataset for each step of the hierarchy is as follows:\n\n* Level A: Offensive language detection - 14,100 tweets\n* Level B: Categorization of offensive language - 3,077 tweets\n* Level C: Identification of the target of the offensive language - 1,095 tweets\n\nTherefore, the answer to your question is:\n\nLevel A: 14,100 tweets\nLevel B: 3,077 tweets\nLevel C: 1,", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": " Based on the article, the baselines are:\n\n* BOW (bag-of-words)\n* TF-IDF (term frequency-inverse document frequency)\n* TextCNN (with initial word embedding as GloVe)\n* Random Forest\n\nSo, the answer is \"yes\".", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequencyâ€“inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": " The answer to this question can be found in the article as follows:\n\n\"Here, we have shown that attention has different patterns based on the POS tag of the target word. The most attended roles when translating to nouns include adjectives, while for verbs, the most attended roles include auxiliary verbs, subjects, and objects.\"\n\nTherefore, the answer to the question is: useful information captured by attention includes adjectives for nouns and auxiliary verbs, subjects, and objects for verbs.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": " The size of the corpora used to train ELMo was not provided in the article. Therefore, the answer is \"unanswerable\".", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": " Unanswerable", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " No.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": " Yes. According to the article, the model architecture for the end-to-end speech-to-text system is based on the Transformer architecture, but with 3 decoder layers like that in the Transformer-XL model.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": " Based on the article, the answer to the question \"what data did they use?\" is:\n\nUnanswerable.\n\nThe article does not provide any information about the data used in the study.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": " The encoder has an LSTM architecture.\n\nUnanswerable: What is the number of LSTM layers?", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " The different modules in Macaw are:\n\n* Model-Controller\n* Action-Controller\n* User-Interface\n\nUnanswerable.", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": " The baseline method used in the study is the random baseline method.\n\nYes.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": " Based on the article, the answer is: unanswerable. The article does not provide information on where the recipes are obtained from.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": " According to the article, the ancient Chinese dataset is created by the proposed method which aligns parallel corpus, merges the data, and combines the information from the two languages. Therefore, the answer is \"created by the proposed method\".", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " The article does not provide information on the datasets used for evaluation. Therefore, the answer is \"unanswerable\".", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": " According to the article, the models are trained on the SemEval 2010 task 8 benchmark data set.\n\nYes.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": " The article does not provide information on what textual patterns are extracted. Therefore, the answer is \"unanswerable\".", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": " Unanswerable. The article does not provide any information on how to calculate variance from the model outputs.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": " Based on the article, the answer is:\n\nUnanswerable.\n\nThe article does not provide any information about the languages for which word embeddings are built.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": " Yes. According to the article, the proposed model is compared to several baseline models, including \"classic models\" and \"deep models\".", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": " Unanswerable", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": " Yes. According to the article, the corpus contains 53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": " Yes. According to the article, the results show that the error detection performance is improved by using artificial data, and the combination of the pattern-based method and the machine translation approach provides the best performance on all datasets.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": " Unanswerable. The article does not provide any information about the accents present in the corpus.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Yes. According to the article, the experiments are performed on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " The average length of the essays in the dataset is not provided in the article, so the answer is \"unanswerable\".", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": " Yes. According to the article, the authors perform qualitative experiments on benchmark datasets to evaluate the performance of the proposed approach. The article mentions that the results are shown in Section 3.1 of the paper.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": " According to the article, they compare to BIBREF26 who use a task-specific architecture compared to their generic sequence to sequence baseline. Therefore, the answer is \"yes\".", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": " The questions in the dataset are natural language questions over a document.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": " According to the article, the proposed method achieves +1.24 in terms of F1 score for paraphrase identification on MRPC and +0.76 on QQP. Therefore, the method improvements of F1 for paraphrase identification are +1.24 and +0.76.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " Yes. According to the article, the following NER models were evaluated:\n\n1. Recurrent models using bidirectional LSTM cells and CRF top layers.\n2. SpaCy's transition-based named entity recognition system.\n3. The recurrent model using a bidirectional LSTM cell and GloVe word embeddings.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": " Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": " Yes. According to the article, several approaches have been applied to solve word segmentation in Vietnamese, including machine learning-based methods (such as SVM and CRF) and lexical-based methods.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": " Unanswerable. The article does not provide information on the 10 other phenotypes that are annotated.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": " Unanswerable. The article does not provide any information about the system's performance.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": " Unanswerable.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": " Based on the article, they evaluate their resulting word embeddings using intrinsic evaluation tasks such as word similarity and word analogy, as well as downstream tasks such as word chunking, sentiment analysis, and question sentiment analysis. Therefore, the answer is \"yes\".", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": " They addressed three cyberbullying topics: personal attack, racism, and sexism.\n\nUnanswerable: Can you provide more information about the severity of the posts?", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": " The answer to your question is: IMDb dataset.\n\nUnanswerable: Can you provide more information about the question?", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " Yes. According to the article, the Random Kitchen Sink (RKS) approach is a method used in natural language processing tasks, including offensive language detection, to improve the evaluation scores by explicitly mapping the data vectors to a higher dimensional space where the data becomes linearly separable.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": " unanswerable", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " According to the article, they test their method on the following tasks:\n\n* Named entity recognition\n* Part-of-speech tagging\n* Text classification\n* Sentiment analysis\n* Document recommendation\n* Language modeling\n\nSo, the answer is \"yes\".", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " Unanswerable. The article does not mention the baseline model used.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": " Unanswerable.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " The article proposes the following additional features and context:\n\n* Context tweets: The article suggests using context tweets to improve the accuracy of abusive language detection.\n* Character-level representations: The article explores the use of character-level representations for abusive language detection, but notes that they do not improve the performance of traditional machine learning models.\n* Hybrid models: The article proposes a hybrid model that combines different neural network models to improve the accuracy of abusive language detection.\n\nUnanswerable: Can the proposed models improve the accuracy of abusive language detection", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " Yes. According to the article, the datasets used are:\n\n* Event Registry BIBREF7\n* TRECVID BIBREF8\n* Gardner's News BIBREF9\n* BIBREF10\n\nThese sources are mentioned in the article as the places where the datasets were obtained.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": " According to the article, the datasets used for evaluation are WN18 and FB15K. Therefore, the answer is: WN18 and FB15K.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": " The corpus used for the task is \"Diachronic corpus of German\" (BIBREF11).", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " The dataset used for this study is the \"Twitter dataset\" which contains 9,473 annotations for 9,300 tweets.", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": " According to the article, the PolyResponse engine is used in 8 languages for restaurant search and booking: English, German, Spanish, Mandarin, Russian, Korean, Japanese, and French. Therefore, the answer is:\n\nYes.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": " According to the article, the vocabulary of e-book readers is typically different from the one of editors. Therefore, the answer to the question \"how large is the vocabulary?\" is \"unanswerable\" based on the information provided in the article.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": " Multi-granularity and multi-tasking neural architecture design are specific to the article because the authors explore these techniques in their system for propaganda detection.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": " According to the article, AEM outperforms the following baseline approaches:\n\n* K-means\n* LEM\n* DPEMM\n\nTherefore, the answer to the question is \"yes\".", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": " Yes. According to the article, the best performing model among the author's submissions is the ensemble of (r4, r7, r12) with a F1 score of 0.673 on the test set.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " Yes, the article evaluates the following transfer learning tasks:\n\n1. Question-answering\n2. Sentence-pair regression\n3. Clustering of 10,000 sentences\n4. Semantic search via hierarchical clustering\n\nUnanswerable: The article does not provide information on other transfer learning tasks beyond these four.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": " Other sentence embeddings methods evaluated in the article are:\n\n* InferSent\n* Universal Sentence Encoder\n* XLNet\n\nUnanswerable: What is the impact of the different setup on the performance of SBERT?", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": " The new context representation is proposed by extending middle context, a new context combination of left context, middle context, and right context.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " The backoff strategies are discussed in the article as follows:\n\n* Pass-through backoff: The attacker adds or replaces one character in the input word, and the word recognition model predicts an UNK.\n* Neutral backoff: The attacker adds or replaces one character in the input word, and the word recognition model predicts a neutral word.\n\nTherefore, the answer is \"yes\".", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": " The size of this dataset is 3000.\n\nUnanswerable.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": " Yes. According to the article, the following classifiers have been trained:\n\n1. KNN (K Nearest Neighbors)\n2. RF (Random Forest)\n3. MLP (Multi-Layer Perceptron)\n4. SVM (Support Vector Machine)", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": " The article mentions the following linguistics features are used:\n\n* Surface features\n* Morphological features\n* Syntactic features\n* POS tags\n* Gender\n* Number\n\nTherefore, the answer is \"yes\".", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": " Based on the article, the answer to the question \"what are the state of the art methods?\" is:\n\nUnanswerable.\n\nThe article does not provide information on the state of the art methods for grammar induction.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": " Yes. According to the article, the real-life dataset contains 4597 employees and 26972 sentences in the supervisor assessment.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": " The CORD-19 dataset is a collection of 45,000 scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses, contributed by hospitals and medical institutions.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": " Yes. According to the article, the two news domains that are country-independent are disinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": " The answer to your question is:\n\nYes.\n\nAccording to the article, the corpus used to learn behavior is a recorded dialogue corpus.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": " Based on the article, the state-of-the-art models for judgment prediction in civil cases are:\n\n* AutoJudge: a novel neural model that incorporates law articles for judgment prediction.\n* r-net: an attention-based model that uses law articles for judgment prediction.\n\nTherefore, the answer is \"yes\".", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": " Unanswerable based on the provided article. The article does not mention any future possible improvements or new research directions related to the field of structured data encoding.", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": " Unanswerable. The article does not provide information on which dimension the semantically related words take larger values.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": " According to the article, the following metrics are used for evaluation:\n\n* Adequacy (i.e., captures the relevant information from the infobox)\n* Fluency (i.e., captures the grammaticality of the generated descriptions)\n* Preference (i.e., the relative ranking of the generated descriptions)\n\nSo, the answer is \"yes\".", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": " Unanswerable. The article does not provide information on whether their framework automatically optimizes for hyperparameters.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": " English", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": " Unanswerable.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": " Yes. According to the article, experiments are performed on text-based games called CoinCollector, CookingWorld, and Zork.", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": " The novel document-level encoder is shown to achieve state-of-the-art results in text summarization.", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": " The two metrics proposed are:\n\n1. Average unique predictions\n2. t-SNE clustering", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": " The article states that the attention distribution of each head is predicted using the softmax normalizing transform, which may yield non-zero attention weights for irrelevant words. The authors propose an adaptive version of sparse attention, where the attention heads learn to attend to a contiguous or non-contiguous set of words, leading to improved interpretability and novel behaviors. Therefore, the answer is \"yes\".", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": " Unanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " Unanswerable based on the information provided in the article. The article does not provide any information about instances or IE tuples, and does not mention sentence structure or IE tuples at all.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": " The additive modification to the objective function is: For a word belonging to any one of the word-groups, a corresponding concept word-group is associated with the word, and a cost term is added to the value of the word's embedding vector dimension corresponding to the concept word-group.\n\n(Unanswerable)", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": " The article does not provide enough information to answer this question. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": " Yes. According to the article, two datasets are available for the task of forecasting derailment of online conversations: the highly curated Wikipedia Talk Page conversations crowd-labeled according to the goal of signaling civil starting points, and the in-the-wild moderation of the subreddit ChangeMyView, where the aim is to signal whether a conversation will lead to personal attacks.", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": " Yes. According to the article, \"Our model achieves statistically significant improvement over the best performing state-of-the-art models, with a p-value less than 0.05 by a Chi-square test.\"", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": " Based on the article, the success of style transfer is measured through the following:\n\n* \"grammatical error correction\"\n* \"output fluency\"\n* \"style transfer\"\n\nSo, the answer is:\n\nYes", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": " According to the article, the human judgements were assembled using a combination of parallel corpora and manual annotation.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " Yes, the baseline is mentioned in the article as \"a weak baseline model (M2M)\".", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " According to the article, the following datasets are used in the experiment:\n\n* INLINEFORM0 case documents that the Supreme People's Court of the People's Republic of China has made publicly available.\n\nUnanswerable.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": " Yes.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": " Yes. According to the article, the baseline is the first fully automated system to extract MR tags from doctor-patient conversations.", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": " According to the article, the model used is a \"joint model\" that combines textual and visual features. Therefore, the answer is \"yes\".", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": " The article mentions the following global network features:\n\n* LWCC (Largest Weighted Clustering Coefficient)\n* SCC (Strongest Connectivity Component)\n* DCC (Deepest Connectivity Component)\n* MWCC (Maximum Weighted Clustering Coefficient)\n* SNR (Network Density)\n* U (number of strong/weak connected components)\n\nTherefore, the answer is \"yes\".", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": " Unanswerable. The article does not provide information on the manual Pyramid scores used.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": " The core component for KBQA is hierarchical matching between questions and relations.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": " Based on the article, the state of the art models for sarcasm detection are:\n\n* CNN-SVM on the Amazon review dataset\n* The method proposed by BIBREF9\n\nTherefore, the answer is \"yes\".", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": " Yes.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": " The dataset used in the article is BioASQ.\n\nUnanswerable.", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": " According to the article, the models use integrated gradients to estimate the word importance. Therefore, the answer is:\n\nYes.", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": " The proposed evaluation for this task is \"an evaluation protocol and baseline\" (SECREF7). Therefore, the answer is \"unanswerable\" because the article does not provide information on how the evaluation will be conducted.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " According to the article, the authors test their word importance approach on different architectures, including:\n\n* English-French BIBREF7\n* English-Japanese BIBREF8\n* French-English BIBREF9\n* Chinese-English BIBREF10\n\nTherefore, the answer is:\n\nYes", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": " Yes.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": " Yes, future work is planned to improve the sensationalism scorer and investigate the applications of dynamic balancing methods between MLE and RL in textGAN.", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": " Reuters database.", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": " Yes, the article provides information on the improvement in classification performance in experiments for low data regime and class-imbalance problems. According to the article, the data weighting approach consistently improves over the base model and BIBREF4, and the augmentation approach outperforms other manipulation methods in both text and image classification tasks. The article also shows that the proposed approach outperforms a variety of dedicated reward learning methods for text classification tasks. Therefore, the answer to the question is \"yes\".", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": " Yes. According to the article, the following publicly available datasets are used:\n\n1. Twitter dataset (BIBREF9)\n2. Reddit dataset (BIBREF12)\n3. YouTube dataset (BIBREF7)\n4. Waseem dataset (BIBREF5)\n5. Davidson dataset (BIBREF13)\n\nThese datasets are used for training and evaluating the hate speech detection model.", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": " Based on the information provided in the article, the model is more reliable for correcting spelling errors and grammatical errors. Therefore, the answer is:\n\nYes.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": " Yes. According to the article, the authors present evidence that the model can capture some biases in data annotation and collection by examining more samples and understanding the misclassifications made by the model. They also mention that the adverse classifications made by the model do not confirm the low performance of the classifier, and that the biases in the data may be the reason for these misclassifications.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " They utilized LDA and Gibbs sampling to evaluate ISWC and WWW publications. (yes)", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": " Yes. According to the article, the authors compared the performance of different LSTM models, but the details are not provided. Therefore, the question is unanswerable based on the information provided in the article.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " Based on the article, keyphrase diversity can be measured using the following metrics:\n\n* Quantitative metric: average unique predictions\n* Qualitative metric: t-SNE clustering of decoder states on the last layer of the target encoder.\n\nTherefore, the answer to the question is: unanswerable.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": " Yes. According to the article, KAR is an end-to-end MRC model named as Knowledge Aided Reader (KAR).", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": " According to the article, global context refers to the representation of the whole document, while local context refers to the representation of the local topic information. Therefore, the answer to the question is \"unanswerable\" as the article does not provide a clear definition or explanation of these terms.", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": " Yes. According to the article, their system achieved a highest â€˜MRRâ€™ score of 0.6103 in one of the test batches for the factoid question answering task.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " Yes. According to the article, the dataset models character's profiles by collecting dialogue data for various characters and using it to construct a large dialogue dataset that is traceable back to the character's HLAs.", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": " The imbalance in the analyzed corpora is significant, with a ratio of 65% male speakers to 35% female speakers.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " According to the article, the proposed HR-BiLSTM model achieves the state-of-the-art on both SimpleQuestions and WebQSP benchmarks. Therefore, the answer is:\n\nYes.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": " unanswerable", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " According to the article, the accuracy merits of the approach are demonstrated through extensive experimental evaluation on multiple real-world datasets, which shows that the proposed approach significantly outperforms the state-of-the-art by an average of 24.3% AUC. Therefore, the answer to the question is \"yes\".", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": " KBQA abbreviates to Knowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": " The invertibility condition is unanswerable based on the information provided in the article.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " Based on the article, the answer is:\n\nYes, the authors found ironic data on Twitter by crawling over 2 million tweets.", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": " The article mentions that the RQE-based QA approach uses two machine learning (ML) and deep learning (DL) methods for RQE:\n\n* ML methods: open-ended and closed-ended question classification\n* DL methods: Recurrent Neural Networks (RNNs), Long Short Term Memory (LSTM) cells, and Convolutional Neural Networks (CNNs)\n\nTherefore, the answer is:\n\nYes, the article mentions two ML and DL methods used for RQE.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " The baseline model was BioBERT.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": " Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " The article mentions the following methods to reduce data sparsity effects:\n\n* Back Translation\n* Mix-Source\n\nTherefore, the answer is \"yes\".", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": " According to the article, the size of the real-world civil case dataset is INLINEFORM0.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": " Yes.", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": " According to the article, the following real-world datasets are used:\n\n* CyberAttack\n* PoliticianDeath\n\nTherefore, the answer is \"CyberAttack and PoliticianDeath\".", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": " Unanswerable. The article does not provide information on the distinctive characteristics of how Arabic speakers use offensive language.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": " According to the article, the classifiers used are deep neural networks.\n\nYes.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " Yes.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": " Yes.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": " The evaluation criteria and metrics used to evaluate the generated text are:\n\n* Word error rate (WER) to measure the number of errors in the generated text.\n* Edit distance to measure the effort required to correct the generated text.\n* Factual correctness to measure the accuracy of the generated text.\n\nUnanswerable: What is the level of factual errors in the generated text?", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": " They use the integrated gradients to estimate the word importance, which consists of two parts: (1) word importance is estimated using a gradient-based approach, and (2) the importance is used to identify under-translated words.\n\nUnanswerable: Can they provide more details on the language-specific syntactic categories?", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": " According to the article, the difference in performance between the proposed model (ALOHA) and baselines is significant, with ALOHA achieving a notable improvement in performance (see Section SECREF17). Therefore, the answer is \"yes\".", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " The article reports the following evaluation metrics:\n\n* Accuracy\n* F1 score\n* Performance in terms of support and reject\n\nSo, the answer is:\n\nYes", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": " According to the article, the strong baseline is the standard sequence-to-sequence model with a content selection mechanism, which is a type of neural data-to-text generation model. Therefore, the answer is \"yes\".", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " Yes. According to the article, the researchers built a predictive model to classify dogmatic posts based on linguistic features.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": " The data was collected through crowdsourcing on Amazon Mechanical Turk.", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": " Yes, their system's absolute accuracy is 59.4%.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": " According to the article, the system \"routes\" instances to annotators based on their predicted difficulty. Expert annotators are used for the most difficult instances, while crowd workers are used for the remaining instances. So, the answer is \"yes\".", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": " The model is applied to two datasets:\n\n1. \"Conversations Gone Awry\"\n2. \"in-moderation subreddit discussions\"", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " They show there is space for further improvement by observing a drop in accuracy when moving the model to a new domain.", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": " Yes, the article mentions several evaluation methods used in the study, including ROGUE.", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": " The authors show that their learned policy generalizes better than existing solutions to unseen games by training a Seq2Seq model on the high-performing trajectories found by Go-Explore and then evaluating its performance on a set of 4,440 test games.", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": " Second order co-occurrence matrix: unanswerable.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order coâ€“occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": " Yes. According to the article, the state-of-the-art system is MPAD.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": " Yes. According to the article, the best performance was achieved by the Healthcare sector.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " The two large-scale datasets used are:\n\n* Disinformation articles (size class: $[0, 100]$)\n* Mainstream news articles (size class: $[0, 100]$)", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": " The size of their dataset is approximately 2,100 texts and 14,000 questions.", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": " No.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": " Yes.", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": " The article uses the Europarl dataset for the experiments.\n\nUnanswerable: The article does not provide information on the specific dataset used.", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": " Based on the article, the baseline methods are:\n\n* NaiveNN\n* FANN\n\nYes.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": " Based on the article, the answer is:\n\nUnanswerable.\n\nThe article does not provide information on the state of the art described in the paper.", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": " No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": " Unanswerable.", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": " Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": " Yes. According to the article, \"we experimented with a wide range of machine learning methods in order to extract argumentation structure from the data, including n-gram, SVM, and random forest.\"", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": " The data in the new corpus is sourced from various domains across different registers of user-generated Web content, including comments to articles, discussion forum posts, and professional newswire articles.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": " The paper explores the following embedding techniques:\n\n* Word embeddings (neural network approach)\n* Second-order co-occurrence vectors (integrating pairwise semantic similarity scores)\n\nUnanswerable.", "answers": ["Skipâ€“gram, CBOW", "integrated vector-res, vector-faith, Skipâ€“gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " Yes. According to the article, \"our framework combines text-embedding, geometric graph construction, and multi-scale community detection to identify content-based clusters of records in an unsupervised manner.\" Therefore, text embedding methodologies are used in the framework.", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": " Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": " According to the article, the dataset used in the study consists of \"two datasets: a Wikipedia dataset novel to this paper and an arXiv dataset provided by BIBREF2\". The size of the Wikipedia dataset is not specified in the article, but the arXiv dataset is split into three sub-datasets: cs.ai, cs.lg, and cs.lg. The size of these sub-datasets is also not specified in the article. Therefore, the answer to the question of how large the data set is cannot be determined based on the information provided in the article, and is therefore unanswerable.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " According to the article, the following metrics are used in evaluation:\n\n* Macro-averaged F-score\n* F1-score\n\nSo, the answer is \"yes\".", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": " According to the article, the authors evaluated the performance of the chatbots in terms of the maximum time to respond to a user's utterance, and they looked at the time taken by each chatbot to respond to a specific utterance. Therefore, the answer is \"yes\".", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": " Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": " Unanswerable.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": " Unanswerable. The article does not provide information on the specific dataset used for analysis.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": " The article does not provide information on the metrics used to establish that the proposed approach makes chatbots more knowledgeable and better at learning and conversation. Therefore, the question is unanswerable based on the provided article.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " Yes. According to the article, the ML methods aim to identify argument components such as premise, claim, rebuttal, and conclusion.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " The answer is: Unanswerable. The article does not provide enough information to answer this question.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": " Yes, they use a newly released corpus of state speeches in the UN General Debate that provides the first dataset of textual output from states that reflects their positions on various issues in international politics.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": " Unanswerable.", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a â€œbig questionâ€ that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": " The article does not provide information on the datasets used. Therefore, the answer is \"unanswerable\".", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " The datasets were annotated using a carefully designed and rigorously guided protocol for manual annotation of semantic similarity judgments.\n\n(Unanswerable)", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": " Unanswerable. The article does not provide information on how the authors identified discussions of LGBTQ people in the New York Times.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": " Yes.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": " Yes. According to the article, the 12 languages covered are:\n\n1. Mandarin\n2. French\n3. German\n4. Russian\n5. Spanish\n6. Italian\n7. Portuguese\n8. Arabic\n9. Hebrew\n10. Japanese\n11. Korean\n12. Turkish.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
