{"pred": "Facebook pages", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "Yes.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "The baselines in the article are not explicitly mentioned. However, the authors propose a new method called Cell-aware Stacked LSTM (CAS-LSTM) and evaluate it on multiple benchmark datasets.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Pivot-based baselines are methods that translate a source language into a pivot language, which is then translated to the target language. This approach is commonly used in transfer learning for NMT.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "Unanswerable.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "The article does not provide specific details on the datasets used in evaluation. It only mentions that the experiments were conducted on four languages from the SemEval-2016 Challenge Task 5 dataset.", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "Unanswerable.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "The article discusses the use of neural models to predict instructor intervention in MOOC forums. The models use a sequence of posts to infer the context of the intervention and make predictions. The article compares the performance of various models and finds that the attention-based models outperform the baseline models. The article also investigates the effect of context length on the model's performance and finds that training the model with loss from threads of different lengths improves its generalization ability.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "Yes.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "The article discusses the use of sarcasm detection in various applications such as online review summarizers, dialogue systems, recommendation systems, and sentiment analyzers. The article highlights the challenges in detecting sarcasm in text, and the importance of considering both textual and cognitive features for sarcasm detection. The authors propose a novel framework that uses eye-tracking data to extract cognitive features, which are then combined with traditional textual features to improve the accuracy of sarcasm detection. The article presents experimental results that show significant improvement in classification accuracy using the proposed framework.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "They use datasets with transcribed text.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Unanswerable.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention used in a Chinese word segmentation model. It captures local and directional information by adding a directional mask and a Gaussian mask to the standard self-attention mechanism. The model uses only unigram features and achieves new state-of-the-art performance on the SIGHAN Bakeoff benchmark dataset.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The article discusses the development of a neural network architecture for extractive summarization of discharge summaries, which are critical for ensuring continuity of care after hospitalization. The authors explore the upper bound on extractive summarization and develop a classifier for labeling the topics of history of present illness notes. They use the MIMIC-III database, which contains electronic health records of patients admitted to the ICU at Beth Israel Deaconess Medical Center between 2001 and 2012. The dataset includes 55,177 discharge summaries and 4,47", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "The article mentions two datasets: EmotionLines and Friends. EmotionLines is a dialogue dataset composed of two subsets, Friends and EmotionPush. The Friends dataset includes $1,000$ English dialogues, while the EmotionPush dataset includes $1,000$ Facebook Messenger chat conversations. Both datasets include multiple consecutive utterances. The article uses the EmotionLines dataset for the EmotionX challenge, which involves recognizing emotions for all utterances in the dialogues.", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "The baseline is a neural network-based Named Entity Recognition (NER) model for the Nepali language, which uses grapheme-level features and does not require any hand-crafted features or data pre-processing.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "The article describes the DeepMine database, a large speech corpus collected using crowdsourcing for text-dependent and text-prompted speaker verification, as well as for text-independent speaker verification. The database contains over 190,000 utterances with full transcription and over 10,000 with sub-part alignment. The article reports the results of speech recognition experiments using the DeepMine database, including the performance of an i-vector/HMM based method and a deep neural network based method. The article also describes the data collection process and the post-", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Attention is different from alignment in cases where the attention model captures information beyond alignment, such as in the case of verbs.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "Unanswerable.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The article describes the use of a Recurrent Neural Network (RNN) for end-to-end speech recognition.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "BERT", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
