{"pred": "Facebook pages", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "The article discusses the use of neural models to predict instructor intervention in MOOC forums. The models use a thread structure and the sequence of posts to infer the latent context, which is defined as a series of posts that trigger an intervention. The article proposes attention models to infer the latent context, which improves the performance of the models. The primary problem is to predict instructor intervention, which is cast as a binary classification problem. The article evaluates the models over a diverse dataset of 12 MOOC iterations on Coursera.org. The models perform better on threads with more posts", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "The article discusses the use of cognitive features extracted from eye-tracking data to improve sarcasm detection in natural language processing. The authors propose a novel framework that combines traditional linguistic features with cognitive features derived from eye-movement patterns. The article presents experimental results that show significant improvement in classification accuracy compared to the state-of-the-art systems. The authors also discuss the effectiveness of considering reading time as a cognitive feature and analyze the importance of individual features in the classification process. The article concludes with a proposal to explore deeper graph and gaze features and to develop models for learning complex gaz", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "Yes.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "Unanswerable.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "The baselines in the article are not explicitly mentioned. However, the authors propose a new method called Cell-aware Stacked LSTM (CAS-LSTM) and evaluate it on various benchmark tasks. Therefore, the baseline can be assumed to be the existing state-of-the-art methods on those tasks.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "Pivot-based baselines are methods that translate a source language into a pivot language, which is then translated to the target language. This approach is commonly used in transfer learning for NMT.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "The article does not provide a specific question or statement that can be answered with a single phrase or sentence. It discusses the challenges of building sentiment analysis models for different languages and proposes a new approach that utilizes machine translation to reuse a model trained on one language in other languages. The article also describes the methodology used in the study and presents experimental results.", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "They use datasets with transcribed text.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "Yes.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "The article describes the DeepMine database, a large speech corpus collected using crowdsourcing and designed for text-dependent and text-prompted speaker verification, as well as text-independent speaker recognition. The database consists of three parts: fixed common phrases for text-dependent speaker verification, random sequences of words for text-prompted speaker verification, and phrases with word and phoneme transcriptions for text-independent speaker recognition. The article provides details on the data collection process, the post-processing steps, and the different parts of the database. It also reports baseline", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "Unanswerable.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "The article describes the use of a Recurrent Neural Network (RNN) for end-to-end speech recognition.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "The article mentions two datasets, EmotionLines and FriendsPush, which were used for the emotion recognition challenge. EmotionLines consists of two subsets, Friends and EmotionPush, each including $1,000$ English dialogues. The dialogues are annotated with seven emotions, including Ekman's six basic emotions and neutral. The Friends dataset is based on the TV show Friends, while the EmotionPush dataset is composed of Facebook messenger chats.", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention used in a Chinese word segmentation model. It captures local and directional information by using a Gaussian mask and directional multi-head attention. The model uses only unigram features and performs segmentation faster than previous models, achieving new state-of-the-art performance.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "The article describes the use of a hierarchical three-level annotation model to create a new publicly available dataset of English tweets. The dataset is used to evaluate various models for offensive language identification, including a linear SVM, a bidirectional LSTM, and a CNN. The performance of these models is evaluated using various metrics, including macro-averaged F1-score, precision, recall, and F1-score for each class. The article also discusses the challenges of offensive language identification and the potential for future research in this area.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Attention is different from alignment in cases where the attention model captures information beyond alignment, such as in the case of verbs.", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "Unanswerable.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "Yes.", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "The baseline is a neural network-based Named Entity Recognition (NER) system for the Nepali language, which uses grapheme-level embeddings and performs well compared to other approaches.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "Unanswerable.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "The article discusses the automatic summarization of discharge summary notes, which are critical for ensuring continuity of care after hospitalization. The authors explore the upper bound on extractive summarization and develop a classifier for labeling the topics of history of present illness notes. The data used in the study comes from the MIMIC-III database, which contains electronic health records of patients admitted to the ICU at Beth Israel Deaconess Medical Center between 2001 and 2012.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "Attention captures useful information beyond traditional word alignments, such as syntactic and semantic relationships between words.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "The article proposes a neural based NER for Nepali language using the latest state-of-the-art architecture. The model uses grapheme-level features and does not require any hand-crafted features or data pre-processing. The authors compare the performance of their model with other baseline models, including Support Vector Machine (SVM) with manual feature engineering and Hidden Markov Model (HMM) with part-of-speech (POS) tags. They find that their neural network model outperforms these baseline models.", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "BERT", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "The article does not provide information on the length of the dataset for each step of the hierarchy.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "The article does not provide a specific number of tags that are looked at for e-book annotation. However, the authors mention that their dataset contains editor tags and user-generated search terms from Amazon, which are used to generate tag recommendations for annotating e-books. The dataset contains metadata fields such as the ISBN, title, description text, author, and a list of BISAC identifiers. The authors evaluate their approaches on a rich set of 19 different algorithms, including popularity-based, similarity-based, and hybrid approaches.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "The baselines were BOW (bag-of-words), TF-IDF (term frequency-inverse document frequency), and TextCNN (a neural-based word embedding method).", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequencyâ€“inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "The article does not provide specific information on the data used. It mentions using the Penn Treebank (PTB) and WikiText-2 (WikiText2) datasets for language modeling tasks, but does not provide details on the data sources or specific data points. Therefore, the answer to this question is \"unanswerable\".", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "The article does not provide a clear answer to this question. It mentions that the authors used a multilingual ST corpus called CoVoST, which was created using the Common Voice speech recognition corpus. However, it does not mention the architecture of the models used in the study. Therefore, the answer to this question is \"unanswerable\".", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "Unanswerable.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "The article does not mention a specific baseline method.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "The ancient Chinese dataset comes from the proposed method which achieves 94.2 F1-score on Test set.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "The article does not mention which datasets are used for evaluation. Therefore, the answer to this question is \"unanswerable\".", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "The more predictive set of features to detect fake news is a combination of words embeddings, style, and morality features.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "The article does not provide information on the size of the collection of COVID-19 literature.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "The proposed model is compared to several baseline models, including traditional features such as TF-IDF, as well as deep features such as Doc2vec and LSTM.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "Unanswerable.", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "Macaw has a modular design with multiple actions that can satisfy different information seeking tasks, including conversational search, question answering, and recommendation.", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "The article does not mention the specific datasets used for experiments.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "Unanswerable.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "The corpora used to train ELMo were not specified in the article.", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "Yes.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The encoder in the system described in the article is an LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The article discusses the relation classification task and how neural networks, specifically convolutional neural networks (CNNs) and recurrent neural networks (RNNs), can be used to classify relations in sentences. The authors propose a new context representation for CNNs and present a connectionist bi-directional RNN model. They also combine CNNs and RNNs using a voting scheme and achieve new state-of-the-art results on the SemEval 2010 benchmark dataset. The article does not mention the specific dataset used for training the models.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "The article describes a scientific article that proposes a new task of generating personalized recipes from incomplete input specifications and user histories. The authors explore data-to-text generation and personalized recommendation in the context of recipe generation. They release a new dataset of 180K recipes and 700K user reviews for this task and introduce new evaluation strategies for recipe quality. The authors' contributions include exploring the task of generating plausible and personalized recipes, leveraging user profiles to generate recipes, and introducing new evaluation measures for recipe quality.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "Unanswerable.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "Unanswerable.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "The article proposes a KL divergence based energy function for learning multi-sense word embeddings, capturing both semantic similarity and polysemy. The model uses a variant of max-margin objective with an asymmetric KL divergence energy function. The article reports the effectiveness of the proposed approach on benchmark word similarity and entailment datasets, showing better performance than existing models.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "Unanswerable.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "The Random Kitchen Sink (RKS) approach is a method used in natural language processing tasks, including sentiment analysis, that explicitly maps data vectors to a space where linear separation is possible. It has been used to improve the evaluation scores of offensive language detection in social media platforms.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "The article discusses various datasets used to evaluate the performance of reading models, including the SQuAD dataset, which consists of natural questions over the 500 most popular articles of Wikipedia, and the ReviewQA dataset, which is proposed as a novel question-answering dataset regarding hotel reviews.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "The results show that using artificial data improves error detection performance, and the combination of multiple error-generated versions of the input file leads to a significant improvement in error detection. The proposed methods consistently outperform the previous method by Felice2014a, and the combination of the pattern-based method and the machine translation approach gives the best overall performance on all datasets.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "The article does not provide a clear answer to this question. It mentions that the dataset was collected from privacy policies of 35 mobile applications before April 2018, and that the answers were formulated by domain experts with legal training. However, it does not specify who these experts were or how they were selected. Therefore, the answer to this question is unknown.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The article describes an experiment in which the authors incorporated subword information into counting models using a strategy similar to fastText. They used the LexVec model as the counting model and modified it to use word vectors as the input. The authors evaluated the performance of the models on intrinsic and extrinsic tasks, including word similarity, word analogy, and downstream tasks. They found that incorporating subword information into the models resulted in similar gains as fastText, with subword models tending to capture more syntactic information. The authors also evaluated the ability of the models to represent out-of-vocabul", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "Unanswerable.", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "They compare their approach to a bi-directional language model and a uni-directional model, both of which use self-attention and are trained using the Big Transformer architecture. They also compare their approach to a variant where the token embeddings are shared between the encoder and decoder.", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "Unanswerable.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "The article describes the creation of a corpus for training machine learning models for named entity recognition in the Armenian language. The corpus was generated using Wikipedia articles and includes gold-standard test data for evaluation of named entity recognition models. The article also describes the use of GloVe word embeddings and popular entity recognition models for evaluation of the created corpus. The main contributions of the work are the silver-standard training corpus, the gold-standard test corpus, GloVe word embeddings, and baseline results for 3 different models on the proposed benchmark data set.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "The article does not mention which datasets are used to evaluate the proposed method. Therefore, the answer to this question is \"unanswerable\".", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "The article does not mention which baseline model is used.", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "They addressed cyberbullying on social media platforms such as Formspring, Twitter, and Wikipedia.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The article discusses the challenges of language identification (LID) in South Africa, where there are 11 official languages. The authors propose a hierarchical naive Bayesian and lexicon-based classifier for LID of short texts. The algorithm is evaluated against existing approaches using three datasets, and the results show that it performs well compared to other methods. The article also discusses existing LID research, datasets, and models, and highlights the need for more data and standardized datasets for LID in South Africa.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "The article does not provide information on the performance of the system.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "The article describes a study that uses a dataset of Twitter tweets annotated with depression-related symptoms. The dataset is constructed based on a hierarchical model of depression-related symptoms, and it contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as either having no evidence of depression or evidence of depression, and if it is annotated as evidence of depression, it is further annotated with one or more depressive symptoms. The dataset is encoded with 7 feature groups, and the study aims to assess the contribution", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "IMDb dataset is used for sentiment analysis.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "PolyResponse is a restaurant search and booking system that is currently available in 8 languages: English, German, Spanish, Mandarin, Polish, Russian, Korean, and Serbian.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "Yes.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "The Adversarial-neural Event Model (AEM) outperforms the K-means, LEM, and DPEMM baseline approaches in open-domain event extraction from online texts.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "The article does not provide information on the sources of the datasets used for the study.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "The article proposes the use of context tweets, which are tweets that reply to or quote another tweet, as additional features for detecting abusive language. The article also discusses the use of character-level representations and various neural network models for abusive language detection.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "InferSent, Universal Sentence Encoder, and XLNet are evaluated in the article.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Unanswerable.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF11, which is a freely available lemmatized, POS-tagged, and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "KNN, RF, SVM, and MLP have been trained.", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "The best performing model among the author's submissions is the ensemble of Logistic Regression, CNN, and BERT, which achieves a F1 score of 0.673 on the test set in the sentence-level propaganda detection task.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The size of the dataset is not mentioned in the article.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "Multi-granularity and multi-tasking neural architecture design involve combining different levels of granularity and different tasks in a single model, allowing for more efficient and effective propaganda detection.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "Unanswerable.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "The new context representation is proposed in the article and is based on the idea of using all parts of the sentence, including the relation arguments, left of the arguments, between the arguments, and right of the arguments, to create a more comprehensive representation for relation classification.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The backoff strategies in the article involve falling back on a generic word recognizer trained on a larger corpus when an RNN-based word recognizer fails to provide accurate predictions for unobserved and rare words.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "The article does not mention the specific metrics used for evaluation.", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "The CORD-19 dataset is a collection of scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. It contains over 45,000 articles contributed by hospitals and medical institutes worldwide and is used to study the correlation between radiological findings and COVID-19.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "The article discusses the use of automatic diacritic recovery in Arabic language processing, specifically focusing on the recovery of both core-word diacritics (CW) and case-endings (CE). The article presents two separate Deep Neural Network (DNN) architectures for recovering both types of diacritics, using character-level and word-level bidirectional Long-Short Term Memory (biLSTM) based recurrent neural models. The article also explores the effectiveness of different features, such as word segmentation information, POS tags, morphological patterns, and affixes", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "The article discusses various methods for inducing probabilistic context-free grammars (PCFGs) from data, including traditional methods that rely on parameterizing rule probabilities and more recent neural network-based approaches. The article proposes a new approach that uses compound PCFGs, which are PCFGs with per-sentence continuous latent vectors that allow different contexts in a derivation to coordinate. The article shows that this approach can perform favorably against recent neural network-based approaches to grammar induction.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "The data in the article is in the Europarl corpus, which is a large collection of parallel sentences in multiple languages. The specific languages mentioned in the article are English, French, and German.", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "Disinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "Two metrics are proposed to evaluate keyphrase generation: F1 score and diversity score.", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "The Neural User Simulator (NUS) is introduced, which outputs natural language and learns from a corpus of dialogues between users and a Spoken Dialogue System (SDS). The NUS is trained to optimize the policy of a reinforcement learning-based SDS, and is compared to the Agenda-Based User Simulator (ABUS) in terms of its ability to generate realistic user behavior. The NUS outperforms the ABUS in both simulated user and real user testing, and is shown to overfit to the NUS in some cases.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "The article proposes a new structured-data encoder that assumes a hierarchical structure, and reports experiments on the RotoWire benchmark. The proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics. The article also discusses related work on data-to-text literature, and the limitations of previous models. The authors integrate a hierarchical attention mechanism into the decoder to improve the encoding of data-structures.", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "The state-of-the-art models for automatic judgment prediction in civil cases are based on the Legal Reading Comprehension (LRC) framework, which incorporates the reading mechanism to better model the complementary inputs. The LRC framework has been applied to the task of automatic judgment prediction in civil cases, and has been shown to improve the performance of previous methods.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "The article discusses a new model called the \"sparse attention\" model, which is a variant of the Transformer architecture. The model uses multi-head attention mechanisms to represent words in a sentence, with each word being represented by a weighted average of its relevant context. The attention weights are computed using the softmax normalizing transform, which can yield non-zero attention weights for irrelevant words. The authors propose a new approach that uses a sparse normalizing transform called sparsemax, which can yield exactly zero attention weights for irrelevant words, improving interpretability. The article also discusses the use of adaptive span", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "Unanswerable.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "The article discusses a method to learn dense, interpretable word embeddings by modifying the objective function of the GloVe algorithm. The proposed method aligns the dimensions of the embedding vectors with predefined concepts, improving interpretability without harming the underlying semantic learning mechanism. The article demonstrates the effectiveness of the approach through quantitative and qualitative evaluations, showing comparable performance with the original GloVe embeddings and preserving semantic coherence. The method can be applied to other word embedding algorithms with a similar learning routine.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "No.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "The article discusses the use of computational models to assist human moderators in online conversation platforms. The models are designed to forecast the derailment of conversations, which is defined as the event when the conversation's trajectory deviates from its intended course. The article introduces a new model called CRAFT that can capture the dynamics of conversations as they unfold and make predictions based on this context. The article also presents two new datasets for evaluating the performance of the model, one based on Wikipedia talk pages and the other based on the subreddit \"ChangeMyView.\" The article concludes that the model out", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "The article discusses the use of reinforcement learning to solve text-based games. The authors propose a methodology that uses the Go-Explore algorithm to extract high-performing trajectories in these games, followed by the training of a Seq2Seq model to map observations to actions. The experiments are conducted on the CoinCollector game and the \"First TextWorld Problems\" dataset, which consists of 4,440 games generated using the CoinCollector game. The results show that the proposed methodology is effective in solving text-based games, with the Seq2Seq model trained on Go", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "The article describes a method for performing both grammatical error correction (GEC) and style transfer with a single trained model, without using any supervised training data for either task. The approach is based on zero-shot neural machine translation (NMT) and involves training the model on bilingual examples in both directions, and then translating the input into the same language. The model is evaluated on the task of GEC and style transfer using publicly available datasets, and the authors report on the performance of the model on these tasks. The article also discusses related work in the field of GEC and style transfer, and", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "Unanswerable.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "The article discusses the use of natural language processing (NLP) and machine learning (ML) to detect online hate speech. The authors propose a transfer learning approach using the pre-trained language model BERT and fine-tuning strategies to improve the performance of hate speech detection models. The article also discusses previous works on online hate speech detection and the use of different features and algorithms. The authors' experiments show that their model outperforms previous works in terms of precision, recall, and F1-score. The article concludes by highlighting the importance of addressing biases in hate speech datasets and using context", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "The baseline is not mentioned in the article.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "The additive modification to the objective function is the introduction of an additional term that favors an increase in the value of the embedding vector dimension for words belonging to a specific concept group, while leaving the cost term untouched for words not belonging to the group.", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "The baseline is a reference point or starting point for comparison in an experiment or analysis. In the context of the article, the baseline refers to the initial performance of the models before any improvements are made. The article discusses the improvement of the baseline performance through various techniques such as data preprocessing, model architecture changes, and training methods.", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "The document-level encoder proposed in the article is based on the pretrained Bert model, which encodes the entire document and generates representations for its sentences. This encoder is novel in that it aims to capture the overall meaning of a document beyond individual word and sentence representations, making it useful for text summarization tasks.", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "The article proposes a new approach for learning different data manipulation schemes with the same single algorithm. The authors adapt an off-the-shelf reward learning algorithm from the reinforcement learning literature for the supervised setting. The algorithm is used to learn data augmentation and weighting, and shows improved performance over strong base models and previous manipulation methods. The authors demonstrate the generality of the approach and its ability to be instantiated for different types of manipulation.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "The article discusses the use of RNNs in Statistical Machine Translation (SMT) and their application to morphologically rich languages. The use of RNNs has revolutionized SMT, but the main downside is the heavy corpus requirement. The efficiency of an MT system depends on the availability and size of parallel corpora, as well as the syntactic divergence between the languages. Morphologically rich languages are difficult to translate due to the heavy inflectional forms and the complexity of developing MT systems. The article discusses the trade-off between domain-specific and large corpor", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "Unanswerable.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "The article discusses the effectiveness of Rouge, a widely used metric in text summarization, for evaluating scientific summarization. The study raises the question of whether Rouge is an accurate evaluator of scientific summaries, and it compares Rouge scores with semi-manual evaluation scores (Pyramid) in the TAC 2014 scientific summarization dataset. The results show that Rouge scores have weak correlations with the Pyramid scores, indicating that Rouge is not an effective metric for evaluating scientific summaries. The article proposes an alternative metric, called Sera, which is based", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "The state of the art models for sarcasm detection are unsupervised pattern mining approach, semi-supervised approach, and n-grams based approach.", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "Yes.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "Yes.", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "The article describes the use of a neural network model called AutoJudge for automatic judgment prediction in civil cases in mainland China. The model is based on the Legal Reading Comprehension (LRC) framework, which incorporates the reading mechanism for better modeling of the complementary inputs of fact description, plaintiff's plea, and related law articles. The article compares the performance of AutoJudge with previous methods and shows significant improvements. The article also describes the construction of a large-scale real-world dataset of case documents and the implementation of previous methods as baselines.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "Improvement over the best performing state-of-the-art is not mentioned in the article.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "The model used in the article is a joint model that combines textual and visual features to predict the quality of a document.", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "The proposed task of concept-map-based MDS is a summarization task that requires creating a concept map that represents the most important content of a set of related documents, is structured and clear, and is connected. The task is complex and consists of several interdependent subtasks, such as extracting appropriate labels for concepts and relations, selecting the most important concepts and relations, and organizing them in a graph. The proposed corpus creation method combines automatic preprocessing, scalable crowdsourcing, and high-quality expert annotations to overcome the issues of traditional approaches. The created corpus consists of 30 topics", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "Yes.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "Future work will investigate the applications of dynamic balancing methods between RL and MLE in text generation, and improve the sensationalism scorer.", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "Reuters", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "The article discusses the use of Twitter diffusion networks to classify news articles as credible or not credible. The authors propose a classification framework based on a multi-layer formulation of Twitter diffusion networks, which allows them to disentangle different social interactions on Twitter, such as tweets, retweets, mentions, replies, and quotes, to build a diffusion network composed of multiple layers. They use off-the-shelf machine learning classifiers to classify news articles and find that a multi-layer, disentangled network yields a significant advance in terms of classification accuracy over a single-layer approach.", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "Improved KB relation detection.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the hate speech detection task and comparing them to the biases in the data collection process. They found that some tweets containing offensive language were misclassified as hate speech, and this was due to biases in the data collection process, such as the oversampling of certain language varieties and the use of a small ad-hoc set of keywords for annotation. They also found that the model was able to differentiate between hate speech and offensive language in some cases, which suggests that it was able", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "The article describes a scientific experiment in which a system was developed for answering biomedical questions. The system was based on a neural network model that used contextual word embeddings to generate answers. The article provides detailed results for the system's performance on the BioASQ 7b dataset, which consists of biomedical questions and documents. The system achieved high scores in factoid and list-type question answering tasks. The article also describes related work on question answering systems and contextual word embeddings.", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "The model is more reliable for spelling, word order, and grammatical errors in grammatical error correction. It is less reliable for lexical choice errors. In style transfer, the model is good at preserving meaning and output fluency, and reliably transfers style for English contractions, word choice, and grammatical constructions.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "The article discusses a novel approach to extractive summarization, especially for long documents, by incorporating local context within each topic and global context of the whole document.", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "The article describes a study on neural machine translation (NMT) and how it works. The study focuses on understanding the importance of representation vectors and input words in NMT models. The researchers use the integrated gradients method to attribute the output to the input words, and they find that attention scores do not provide meaningful explanations. The study contributes to the understanding of NMT models and provides empirical support for designing NMT architectures.", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "Yes.", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "Yes.", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "Unanswerable.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The article mentions several publicly available datasets that have been used in previous studies on online hate speech detection. These include datasets such as Twitter (BIBREF9), Reddit (BIBREF12), YouTube (BIBREF14), and a corpus of more than $16,000$ tweets labeled as racist, sexist, or neither (BIBREF5).", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "The article describes a scientific competition called BioASQ, which is a question answering task in the field of biomedicine. The authors submitted a system based on BioBERT, a biomedical document classification, document retrieval, and question answering system. The system achieved high scores in factoid and list-type question answering tasks. The authors also describe their approach to deriving lexical answer types from questions and provide additional details on their system's architecture and training.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Unanswerable.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Diversity of keyphrases can be measured by counting the number of unique keyphrases generated by a model, or by using metrics such as F1 score or precision-recall curve.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "They compared the performance of their deep LSTM RNN model with various other models, including shallow LSTM RNN models, linear classifier probe, and blockwise model-updating filter (BMUF).", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "KAR is an end-to-end MRC model that utilizes general knowledge to assist its attention mechanisms.", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "The article discusses the use of Recognizing Question Entailment (RQE) in medical question answering (QA) systems. RQE involves recognizing whether a given question entails a specific answer, and the article explores various machine learning and deep learning methods for this task. The article presents a collection of 47,000 medical question-answer pairs and evaluates the performance of RQE-based QA systems on TREC 2017 LiveQA medical questions. The results show that RQE-based QA systems outperform traditional information retrieval-based", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Unanswerable.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The article does not provide the size of the real-world civil case dataset.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "The article proposes Human Level Attributes (HLAs) as a way to model character profiles in dialogue agents. HLAs are based on tropes collected from TV Tropes and are used to trace dialogue data back to both its context and associated human-like qualities. The authors propose a system called ALOHA that maps characters to a latent space based on their HLAs and recovers language styles of specific characters. ALOHA outperforms baseline models and is stable regardless of the character's identity, genre, or context of the dialogue.", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "The accuracy merits of the approach are demonstrated through empirical evaluation on multiple real-world datasets, showing an average improvement of 24.3% in AUC compared to the state of the art.", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "The article describes the use of neural machine translation (NMT) for the low-resourced language pair of Japanese INLINEFORM0 Vietnamese. The authors attempt to build NMT systems for this pair and conduct experiments using various methods to improve the quality of the systems. They use subword units as translation units, experiment with different segmentation methods, and apply data augmentation techniques. The article also describes the architecture of NMT systems and the attention mechanism used in them. The authors conclude that their NMT systems achieve reasonable results and provide a dataset and associated training scripts for future research.", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "Unanswerable.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "On the SimpleQuestions and WebQuestions benchmarks, the proposed system achieves the state-of-the-art performance.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "The authors used twitter to build a large-scale dataset of ironic and non-ironic sentences for irony generation.", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to understand and address the representation of women in media and its impact on speech recognition technology.", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "Yes.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "KBQA stands for Knowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "The baseline model used in the BioASQ competition is a neural network model based on contextual word embeddings, fine-tuned on the BioASQ training data. The model is designed to generate exact and ideal answers to biomedical questions.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "The article describes a study that focuses on understanding Neural Machine Translation (NMT) models and how they work. The study uses a gradient-based approach to estimate word importance in NMT generation. The authors find that word importance is useful for understanding NMT and identifying under-translated words. They also provide empirical support for the design principle of NMT architectures, suggesting that essential inductive bias should be considered for model design. The article discusses several methods for interpreting Seq2Seq models, including interpreting internal representations and input-output behaviors. The authors focus on interpreting the input-", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "The article reports several evaluation metrics, including accuracy, F1 score, and a custom attention-based metric.", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "The evaluation criteria and metrics used to evaluate the generated text include factual correctness, fluency, and product-readiness. The factual correctness was measured by the number of factual errors in the generated text compared to the corrected version. The fluency was evaluated based on the grammaticality and overall flow of the generated text. The product-readiness was assessed by two journalists who edited the generated text to simulate the use of the output as a product or for direct publication. The word error rate (WER) was used as a measure of the edit distance of the generated text and its corrected variant,", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "The article mentions that several real-world datasets are used for the experiments, including CyberAttack and PoliticianDeath.", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "The strong baseline is a Seq2Seq model with attention and copy mechanism, which generates text summaries from tables. It is the current state-of-the-art model for table-to-text generation.", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "The article describes a study on the use of crowdsourcing platforms like Amazon Mechanical Turk to collect annotated data for biomedical natural language processing tasks. The authors explore the use of expert and lay annotators and investigate the quality of crowdsourced annotations. They also present a task difficulty prediction model and show how it can be used to improve information extraction models. The study concludes that expert annotations are preferable, but a combination of expert and lay annotations is better than using lay data alone.", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The article does not mention the type of classifiers used.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "ROGUE", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "The article does not provide the absolute accuracy of the system.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "The data was collected through crowdsourcing on Amazon Mechanical Turk, with workers being asked to formulate questions and answers based on script scenarios rather than specific texts.", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "The article does not mention the two large-scale datasets used.", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "The proposed model, ALOHA, outperforms the baselines by a significant margin, achieving a noticeable improvement in performance across all five evaluation characters.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "They show there is space for further improvement by conducting a human study and finding that a majority of questions that their system couldn't answer were actually answerable. This suggests that there is still room for improvement beyond the performance of their current model.", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "No.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "The authors show that their learned policy generalizes better than existing solutions to unseen games by training a Seq2Seq model on the high-performing trajectories extracted using the Go-Explore algorithm. This approach allows the agent to operate in the full, unconstrained action space of natural language and systematically generalize to new text-based games with no or few interactions with the environment.", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "They build a predictive model of dogmatism using linguistic features and psychological theories.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "The state-of-the-art system is not mentioned in the article.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "The model is applied to two datasets: the \"Conversations Gone Awry\" dataset and the \"ChangeMyView\" dataset.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "The article discusses the use of computational methods to analyze text data, with a focus on exploring social and cultural phenomena. The research process involves identifying research questions, selecting data, conceptualizing and operationalizing concepts, and analyzing and interpreting results. The authors emphasize the importance of interdisciplinary collaboration and the need to address challenges such as the cultural and social context of texts, the subjectivity of human interpretations, and the potential for bias in data collection and analysis. They also highlight the importance of validating results through multiple methods and approaches, and the need to consider the broader societal", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "The article does not provide information on which stock market sector achieved the best performance.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "Yes.", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "The state of the art described in the paper is the development of a hybrid conceptual architecture and its implementation with a finance advisory system, as well as the exploration of a multi-party governance service to enforce compliant interaction norms.", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "The article discusses the use of monolingual data in Neural Machine Translation (NMT) systems and proposes various techniques to improve the performance of these systems. The article focuses on the use of back-translation (BT) as a way to generate artificial parallel data and proposes several ways to improve the quality of BT data. The article also explores the use of target-side language models (LMs) in NMT systems and compares their performance to BT. The article concludes by discussing the limitations of BT and the need for further research in this area.", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "A second order co-occurrence matrix is a measure of semantic similarity and relatedness that incorporates pairwise similarity scores derived from a taxonomy, reducing noise by selecting only the most semantically similar co-occurrences.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order coâ€“occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "The article describes a new algorithmic methodology that detects content-based groups of records in a given dataset in an unsupervised manner, based on free text. The method combines deep neural network-based language models with network-theoretical methods to extract clusters of incidents at different levels of resolution. The framework is able to identify, rather than impose, content-based clusters from free, unstructured text in a more efficient and effective manner than traditional methods.", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "The size of the dataset is not mentioned in the article.", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "The baseline methods are not explicitly mentioned in the article.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "The paper explores various embedding techniques, including word embeddings and second-order co-occurrence vectors.", "answers": ["Skipâ€“gram, CBOW", "integrated vector-res, vector-faith, Skipâ€“gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "Yes.", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "The data in the new corpus is sourced from various domains and registers, including comments to articles, discussion forum posts, blog posts, and professional newswire articles. The data is sourced from different sources and no assumptions can be made about its actual content with respect to argumentation.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "The article discusses the challenges of engineering conversational systems, specifically multi-party conversational systems (MPCS). The authors present a hybrid conceptual architecture and its implementation with a finance advisory system. They are also working on evolving the architecture to support decoupled interaction norms specification and developing a multi-party governance service to enforce compliance. The article does not provide any information on evaluation metrics.", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "Unanswerable.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "The article discusses the use of BERT language model for transfer learning and its ability to handle imbalanced classification tasks. The authors participated in a shared task on fine-grained propaganda detection and achieved second place on the sentence-level classification task. They also studied the problem of class imbalance and cost-sensitive learning in NLP tasks. The article provides a statistical method for establishing the similarity of datasets and incorporates cost-sensitivity into BERT to enable models to adapt to dissimilar datasets. The authors release their program code on GitHub and Google Colaboratory for other researchers to benefit from their work", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "The article does not provide information on the size of the data set used in the study.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "This work accounts for various argumentation phenomena in actual data, including the use of rhetorical questions, figurative language, and narratives. The article also discusses the challenges of annotating user-generated content and the limitations of existing argumentation theories.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "The article discusses a new approach to building chatbots that can learn and improve their conversation skills over time. The approach is based on the idea of open-world knowledge base completion (OKBC), which allows chatbots to learn new facts and relationships in response to user input. The authors propose a lifelong interactive learning and inference (LiLi) approach to solve the OKBC problem, which involves using reinforcement learning to formulate query-specific inference strategies and then executing them in an interactive setting. The effectiveness of the LiLi approach is evaluated through a series of experiments using two datasets, and the results", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Unanswerable.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "The article discusses the use of computational text analysis to study social and cultural phenomena. The authors describe their experiences in conducting research using computational methods, and provide guidance for others who are interested in doing similar work. They emphasize the importance of identifying research questions, selecting appropriate data sources, and developing methods for measuring social and cultural concepts. The authors also highlight the challenges involved in conducting computational text analysis, including the need to operationalize concepts, deal with the limitations of machine learning algorithms, and interpret the results in a meaningful way. Overall, the article provides a useful overview of the opportunities", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "The article introduces a new approach to estimate preference polarization in multidimensional settings using texts and votes. The authors use a distributional representation of textual data and community detection in multiplex networks to identify preference affinity blocs across multiple layers of votes and speeches. They find that the proposed approach can improve the ability to model conflict onset in international politics and will be useful for any scholar interested in measuring preference polarization in multidimensional settings.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "The article discusses the field of argumentation mining, which involves analyzing argumentation in user-generated content on the Web. The article identifies several challenges and research questions in this field, including the need to analyze argumentation in different domains and registers, the gap between argumentation theories and computational linguistics, and the need for reliable annotation of argument components. The article also describes the use of machine learning techniques to identify argument components in user-generated content, and reports on the results of annotation studies to assess the reliability of these techniques. Overall, the article highlights the ongoing research and development needed to improve", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Unanswerable.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "A node in the network approach represents a country or state in the study of international politics.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "The article does not provide a clear answer to this question. It mentions that the authors developed a computational linguistic framework for analyzing dehumanizing language, with a focus on lexical signals of dehumanization. They used the New York Times as a case study to explore changing representations of LGBTQ groups over three decades. However, the article does not explain how they identified discussions of LGBTQ people in the New York Times.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "Unanswerable.", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a â€œbig questionâ€ that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "No.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "The datasets were annotated using a carefully designed and rigorous language-agnostic translation and annotation protocol.", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "The 12 languages covered in the article are not specified.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
