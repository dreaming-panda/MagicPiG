{"pred": " According to the article, they chose different Facebook pages (and therefore domains and stances) mainly based on intuition and with an eye to the nature of the datasets available for evaluation. They did not provide a specific list of pages.\n\nUnanswerable.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " Yes", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": " Unanswerable. The article does not provide information on the improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": " Unanswerable. The article does not provide information on the type of latent context used to predict instructor intervention.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": " Unanswerable", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": " Yes.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": " Unanswerable. The article does not provide information on any other evaluation metrics beyond F-score and Kappa.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": " No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": " Based on the article, the pivot-based baselines are:\n\n* BIBREF4: translates a sentence from the source language to the pivot language, and then from the pivot language to the target language.\n* BIBREF5: first translates a sentence from the source language to the pivot language, and then generates a parallel corpus of the pivot language by back-translation.\n\nTherefore, the answer is \"yes\".", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": " Yes, clustering algorithms were used. According to the article, \"we used clustering algorithms to discover broad categories among them\" (emphasis added).", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": " Unanswerable. The article does not provide information on the amount of training data from the non-English language used by the system.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": " The article mentions the following datasets were used:\n\n* EmotionLines dataset, which consists of dialogues.\n* Friends dataset, which includes $1,000$ English dialogues.\n* EmotionPush dataset, which includes $1,000$ English dialogues.\n\nSo, the answer is:\n\nYes", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": " Yes.", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": " The baselines were:\n\n* Plain stacked LSTMs\n* Models that use different values of INLINEFORM0\n* Models that do not use INLINEFORM1\n* Models that integrate lower contexts via peephole connections.\n\nUnanswerable: What is the purpose of introducing INLINEFORM0 and INLINEFORM1 in the proposed architecture?", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": " The paper was published in 2022, so the answer is \"2022\".", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": " Yes. According to the article, the model used for end-to-end speech recognition is a Recurrent Neural Network (RNN).", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": " Unanswerable. The article does not provide information on how the vocabulary of word-like or phoneme-like units is automatically discovered.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": " The models used in the experiment are:\n\n* Linear SVM\n* BiLSTM\n* CNN\n\nYes.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " The datasets used in evaluation are:\n\n* SemEval-2016 Challenge Task 5\n* BIBREF7\n* BIBREF8\n* BIBREF9\n* BIBREF10\n\nUnanswerable.", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": " According to the article, attention is different from alignment in the following cases:\n\n* When translating verbs: Attention loss varies substantially across different POS tags, and the attention of verbs is substantially more inconsistent with alignments than for nouns.\n* When translating other parts of speech: The attention distribution is more distributed compared to alignment points for most POS tags, with the most attended roles including adjectives, adverbs, and auxiliary verbs.\n\nUnanswerable: Can the attention model capture information beyond alignment?", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": " Yes. According to the article, a strong neural baseline was used, but the article does not provide more specific information about the type of neural model used.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": " Yes. According to the article, the following keyphrase extraction models were reassessed: \"the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based approach, and the last two were among the top-performing systems in the SemEval-2010 task\".", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": " According to the article, the following datasets were used:\n\n* MIMIC-III: a freely available, deidentified database containing electronic health records of patients admitted to an intensive care unit at Beth Israel Deaconess Medical Center between 2001 and 2012.\n* 515 annotated history of present illness (HPI) notes from the MIMIC-III dataset.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": " Unanswerable. The article does not provide any information on how Gaussian-masked directional multi-head attention works.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " Based on the article, the answer is: unanswerable. The article does not provide any information about the number of people who participated in the evaluation study of table-to-text models.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": " Yes. According to the article, they test the BERT model.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": " Based on the article, the answer is:\n\nUnanswerable. The article does not provide information on the number of tags looked at.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": " Based on the article, the baseline is \"BiLSTM+CNN(grapheme-level)\". Therefore, the answer is \"yes\".", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": " Based on the article, attention captures different patterns based on the POS tag of the target word, with nouns showing a concentrated pattern and verbs showing a more distributed pattern. Therefore, the answer to the question \"What useful information does attention capture?\" is:\n\nYes, attention captures useful information.", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": " Unanswerable. The article does not provide information on why prior knowledge distillation techniques are ineffective in producing student models with vocabularies different from the original teacher models.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": " They explore BiLSTM, CNN, and BiLSTM+CNN models.\n\nUnanswerable: What is the relative improvement of the model compared to other baselines?", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": " Based on the article, the answer to the question \"what data did they use?\" is:\n\nUnanswerable.\n\nThe article does not provide any information about the data used in the study.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": " Based on the article, the more predictive set of features to detect fake news is:\n\nEmotion, Sentiment, Morality, Words embeddings, Style.\n\nUnanswerable: Can the article answer the question? No.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": " According to the article, the size of the dataset for each step of the hierarchy is as follows:\n\nLevel A (Offensive language detection): 14,100 tweets\nLevel B (Categorization of offensive language): 8,500 tweets\nLevel C (Target identification of offensive language): 3,950 tweets\n\nTherefore, the total size of the dataset is 14,100 + 8,500 + 3,950 = 26,550 tweets.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": " Baselines: BOW, TFIDF, Logistic Regression, TextCNN, and Neural Network.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequencyâ€“inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": " According to the article, the collection of COVID-19 literature contains over 45,000 scholarly articles.\n\nYes.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": " Unanswerable", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " According to the article, the ancient Chinese dataset comes from the following sources:\n\n* Parallel corpus crawling and cleaning\n* Ancient-modern Chinese translation dataset: INLINEFORM0 1.24M bilingual sentence pairs\n\nTherefore, the answer is: The ancient Chinese dataset comes from these sources.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " No.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": " Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": " Yes, the article provides information on the model architecture used in the study. According to the article, the models used are based on the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": " The size of the corpora used to train ELMo is not provided in the article. Therefore, the answer is \"unanswerable\".", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": " The different modules in Macaw are:\n\n* Co-Reference Resolution\n* Query Generation\n* Retrieval Model\n* Result Generation\n\n(Answer: Yes)", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": " Based on the article, the answer is:\n\nUnanswerable.\n\nThe article does not provide any information about the languages for which word embeddings are built.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": " Yes. According to the article, the baseline method used is the linear kernel option of the LibSVM utility in the WEKA tool.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": " Unanswerable", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": " The encoder has an LSTM architecture.\n\nUnanswerable: Does the system use an auxiliary objective for morpho-syntactic awareness?", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " Yes. According to the article, experiments are performed on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " According to the article, the authors train their models on the SemEval 2010 task 8 dataset, which is a manually annotated dataset released in the context of a Semantic Evaluation (SemEval) shared task in 2010. Therefore, the answer is:\n\nYes", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": " The article mentions the following datasets for evaluation:\n\n* XNLI (zero-shot cross-lingual sentiment analysis)\n* UD (universal dependency parsing)\n* SOV (syntactic orientation of vocabulary)\n\nTherefore, the answer is:\n\nYes", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": " Based on the article, the answer is: unanswerable. The article does not provide any information on where the recipes are obtained from.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": " Unanswerable. The article does not provide any information on how to calculate variance from the model outputs.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": " Yes. The proposed model is compared to several baseline models, including traditional features, deep features, and ELMo.", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": " According to the article, the method improvements of F1 for paraphrase identification are:\n\n* +0.58 for BERT\n* +0.73 for XLNet\n\nSo, the answer is \"yes\".", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " Yes. According to the article, the corpus contains 53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": " Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " Yes. According to the article, textual patterns are extracted using a method described as follows: \"We also describe a method for AEG using patterns over words and part of speech (POS) tags, extracting known incorrect sequences from a corpus of annotated corrections.\" (emphasis added)", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": " The average length of the essays in the dataset is not provided in the article, so the answer is \"unanswerable\".", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": " Unanswerable. The article does not provide any information about the accents present in the corpus.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Yes.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": " Unanswerable. The article does not provide information on 10 other phenotypes that are annotated.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": " They compare to BIBREF26 who use a task-specific architecture compared to their generic sequence to sequence baseline.\n\nUnanswerable: Can they improve the decoder with pre-trained representations?", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": " Yes. According to the article, the authors perform qualitative experiments on benchmark datasets to evaluate the performance of the proposed approach. These experiments include comparing the performance of the GM$\\_$KL model with other existing approaches on various datasets.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": " Yes. According to the article, the Random Kitchen Sink (RKS) approach is a method used to improve the evaluation score in offensive language detection tasks. It involves explicitly mapping the data vectors to a space where linear separation is possible, and then using regularized least-square based classification.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": " Yes. According to the article, the following NER models were evaluated:\n\n1. Recurrent model with a batch size of 8 and Adam optimizer\n2. Char-biLSTM+biLSTM+CRF\n3. Stanford NER\n4. spaCy 2.0", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": " Yes. According to the article, several approaches have been applied to solve word segmentation in Vietnamese, including lexical analysis, morphology, word segmentation, and named entity recognition.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": " unanswerable", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": " The questions present in the dataset are natural language questions over the 500 most popular articles of Wikipedia.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": " They addressed three cyberbullying topics: personal attack, racism, and sexism.\n\nYes.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": " Unanswerable. The article does not provide any information about the system's performance.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": " Yes, according to the article, the baseline model used is \"LastStateRNN\" (Last State RNN).", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": " Unanswerable.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": " Yes.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " Based on the article, they evaluate their resulting word embeddings through intrinsic evaluation tasks such as word similarity and word analogy tasks, as well as downstream tasks from the VecEval suite. Therefore, the answer is \"yes\".", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": " The answer to your question is: IMDb dataset.\n\nUnanswerable: Can you provide more information about the project, such as the specific question you are trying to answer or the context in which you are asking the question?", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " Yes. The article proposes using context tweets as additional features for improving the accuracy of abusive language detection.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": " The corpus used for the task is \"DTA18 and DTA19\".", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " Yes, according to the article, PolyResponse engine is used for restaurant search and booking system in 8 languages: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), and Korean (Seoul).", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": " Multi-granularity and multi-tasking are specific to the neural architecture design in the article.\n\nYes.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": " According to the article, they test their method on the following tasks:\n\n* Named Entity Recognition (NER)\n* Part-of-Speech (POS) tagging\n* Text classification\n* Language modeling\n* Document recommendation\n\nSo, the answer is \"yes\".", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " Yes, the sources of the datasets are mentioned in the article as follows:\n\n* \"Event Registry\" (BIBREF7)\n* \"rupnik2016news\" (BIBREF8)\n* \"Steinberger et al. (BIBREF9)\"\n* \"Ahgar et al. (BIBREF10)\"\n* \"Sato et al. (BIBREF11)\"\n* \"Bloomberg\" (BIBREF12)\n\nNote: BIBREF7, BIBREF8, BIBREF9, B", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": " Yes, the article evaluates transfer learning tasks for sentence embeddings, including question-answering tasks (MR, CR, and SST) from SentEval, and the TREC dataset.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": " According to the article, the backoff strategies in the proposed defense work by mapping UNK predictions to a fixed neutral word, which can help mitigate the effects of adversarial attacks. This is done to reduce the sensitivity of the word recognition model to adversarial attacks. Therefore, the answer is \"yes\".", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": " According to the article, the datasets used to evaluate the paper are:\n\n* WN18\n* FB15K\n\nTherefore, the answer is: WN18 and FB15K.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": " According to the article, the vocabulary of users may differ from the one of editors, and thus, the tag recommendation system may struggle to annotate e-books with suitable tags. However, the article does not provide information on the size of the vocabulary. Therefore, the answer is \"unanswerable\".", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": " Yes. According to the article, the following classifiers have been trained: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP).", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": " The dataset used for this study is an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression symptoms.", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": " Yes. According to the article, the real-life dataset used in the study contains 4528 employees and 26972 sentences.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": " According to the article, the following features are used for Arabic diacritics recovery:\n\n* Word segmentation information\n* Unigram language model\n* Character-level RNN model\n* Surface, morphological, and syntactic features\n* POS tags\n* Gender and number features\n* Morphological patterns\n* Affixes\n\nTherefore, the answer is:\n\nYes, these features are used for Arabic diacritics recovery.", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": " Yes. According to the article, the best performing model among the author's submissions is the ensemble of (r4, r7, r12) with a F1 score of 0.673 on the test set for sentence-level propaganda detection.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " According to the article, AEM outperforms the following baseline approaches:\n\n* K-means\n* LEM\n* DPEMM\n\nTherefore, the answer to the question is \"yes\".", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": " Yes. According to the article, the two news domains that are country-independent are disinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": " Unanswerable. The article does not mention any future possible improvements for the proposed hierarchical encoder model.", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": " Other sentence embeddings methods evaluated in the article are:\n\n* InferSent\n* Universal Sentence Encoder\n* XLNet\n\nUnanswerable: What is the reason for the poor performance of BERT in computing sentence embeddings?", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " According to the article, the following metrics are used for evaluation:\n\n* Adequacy (i.e., does the generated text capture the relevant information from the input data?)\n* Fluency (i.e., how well-written and coherent is the generated text?)\n* Preference (i.e., which of the two generated texts is preferred by the annotators?)\n\nSo, the answer is \"yes\" (the article provides information on the metrics used for evaluation).", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": " English", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": " The new context representation is proposed by extending the middle context of the CNN with all parts of the sentence, including the relation arguments, left of the relation, and right of the relation. This is described in the article as follows: \"We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation, and right of the relation) and pays special attention to the middle part.\" Therefore, the answer is \"yes\".", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " The size of the dataset is 3000.\n\nUnanswerable.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": " Yes. According to the article, the authors' model improves interpretability compared to softmax transformers by using sparse attention mechanisms that can yield exactly zero probabilities for irrelevant words, leading to improved interpretability and slight accuracy gains.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " The two metrics proposed are:\n\n1. Average unique predictions produced by the models.\n2. t-SNE clustering results on the decoder hidden states.\n\nUnanswerable.", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": " Based on the article, the answer to the question \"what are the state of the art methods?\" is:\n\nUnanswerable.\n\nThe article does not provide information on the state of the art methods for grammar induction.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": " Unanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " The CORD-19 dataset is a collection of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses, containing over 45,000 sentences, including over 33,000 with full text, contributed by hospitals and medical institutes all over the world.\n\n(Yes)", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": " Yes. According to the article, two new datasets are developed and released publicly for the task of forecasting derailment of online conversations, with one dataset consisting of triplets of civil-starting Wikipedia Talk Page conversations that are crowd-labeled according to whether they eventually lead to personal attacks, and the other dataset relying on in-the-wild moderation of the popular subreddit ChangeMyView, with the goal of forecasting whether a given conversation will lead to moderator intervention due to toxic or offensive content.", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": " Based on the article, the success of style transfer is measured by the model's ability to adapt the output into a desired style while preserving the meaning of the input. The article does not provide any specific metrics or measures for evaluating style transfer success, so the answer is \"unanswerable\".", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": " Based on the article, the state-of-the-art models for judgment prediction in civil cases are:\n\n* AutoJudge (a novel neural model that incorporates law articles)\n* GRU+Attention (a combination of a GRU model and attention mechanism)\n* CNN+Attention (a combination of a CNN model and attention mechanism)\n\nUnanswerable: What are the directions for future research?", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": " Unanswerable based on the information provided in the article.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " The answer to your question is:\n\nYes. According to the article, the Neural User Simulator (NUS) learns behavior from a corpus of recorded dialogues.", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": " No.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": " Based on the article, the baseline is \"a strong baseline\" (yes).", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " Unanswerable. The article does not provide information on which dimension the semantically related words take larger values.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": " Unanswerable. The article does not provide information on the biases captured by the model.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": " Yes.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": " Yes. According to the article, the baseline is \"ELMo with encoder multi-decoder (one each for dosage and frequency) and BERT with encoder-decoder\" (see the last sentence of the article).", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": " Unanswerable. The article does not provide information on the manual Pyramid scores used.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": " Yes, according to the article, experiments are performed on three text-based games: CoinCollector, CookingWorld, and First TextWorld Problems.", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": " The novel document-level encoder is described as \"a new way of representing documents in a compact and efficient manner\" (unanswerable).", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": " Yes.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": " Unanswerable based on the information provided in the article. The article does not provide any information about instances being sentences or IE tuples, and does not define what an instance is in the context of the article.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": " The additive modification to the objective function is \"an increase for a pre-specified input of concept words along each dimension\".\n\nUnanswerable: Can the proposed method preserve the semantic structure of the original embedding without any modification to the objective function?", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": " Based on the article, the state of the art models for sarcasm detection are:\n\n* BIBREF6 (using a kNN-classifier over a punctuation-based feature set)\n* BIBREF9 (using a combination of lexical and pragmatic features)\n* BIBREF8 (using a combination of language-independent and language-dependent features)\n\nTherefore, the answer is \"yes\".", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": " The evaluation proposed for this task is \"an evaluation protocol and baseline\" (SECREF7). Therefore, the answer is \"unanswerable\" because the article does not provide any information about the type of evaluation proposed.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " Future work: unanswerable", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": " They test their word importance approach on different architectures, including English-French, English-Japanese, and Chinese-English.\n\n(Yes)", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": " Yes.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " The article does not provide information on how much the use of morphological segmentation improves the efficacy of the attention mechanism. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": " Yes. According to the article, the proposed method achieves 65% accuracy in predicting sensationalism, which is an improvement over the best performing state-of-the-art.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": " Yes.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": " Reuters database", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": " According to the article, global context refers to the representation of the whole document, while local context refers to the representation of each topic or section within the document. Therefore, the answer is \"yes\".", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": " The global network features which quantify different aspects of the sharing process are:\n\n* Network density\n* Number of strong/weak connected components\n* Diameter\n* Main K-core number\n* Structural virality\n\nYes.", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": " Based on the information provided in the article, the model is more reliable for correcting spelling errors, word order errors, and grammatical errors. Therefore, the answer is:\n\nYes", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": " The imbalance in the analyzed corpora is significant, with a proportion of 65% of male speakers and 35% of female speakers.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " According to the article, the experiment uses a real-world dataset of civil law case documents that the Supreme People's Court of People's Republic of China has made publicly available. The dataset consists of case descriptions, pleas, and results, which are easily extractable with regular expressions. The article does not provide further information on the specific datasets used in the experiment.\n\nUnanswerable: Does the article mention any other datasets used in the experiment?", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": " unanswerable", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " They used the BioASQ dataset.\n\nUnanswerable: What is the ideal answer generation task?", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": " They utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications. (yes)", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": " The invertibility condition is unanswerable based on the information provided in the article.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " According to the article, the model used is a \"joint model\" that combines textual and visual features. Therefore, the answer is \"yes\".", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": " Yes, their highest recall score was 4th for List-type questions and 3rd for Factoid-type questions.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " The core component for KBQA is hierarchical matching between questions and KB relations.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": " The article mentions that the following methods were used to reduce data sparsity effects:\n\n* Back-translation\n* Mix-source\n\nTherefore, the answer is \"yes\".", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": " Yes.", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": " Yes, the article provides information on the improvement in classification performance in experiments for low data regime and class-imbalance problems. According to the article, the data weighting approach consistently improves over the base model and a previous method in the low-data setting, and the improvement increases as the data gets more imbalanced. Similarly, on imbalanced CIFAR10 classification, the method outperforms other comparison approaches. These results suggest that the proposed approach can improve classification performance in low data regime and class-imbalance problems.", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": " The article mentions that two machine learning (ML) and deep learning (DL) methods are used for RQE:\n\n* Logistic Regression\n* Recurrent Neural Networks (RNNs)\n\nTherefore, the answer is:\n\nYes", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " They measure which words are under-translated by NMT models by exploiting the gradients in NMT generation.", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": " Based on the article, the answer to the question is:\n\nUnanswerable.\n\nThe article does not provide information on how the models decide how much importance to give to the output words.", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": " According to the article, the size of the real-world civil case dataset is INLINEFORM0 case documents.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": " Unanswerable based on the information provided in the article.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": " Based on the article, the answer to the question \"How do they match annotators to instances?\" is:\n\nUnanswerable.\n\nThe article does not provide any information on how the authors match annotators to instances.", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": " Yes.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": " Based on the article, keyphrase diversity can be measured using the following metrics:\n\n1. Average unique predictions produced by the model (Section SECREF3).\n2. Clustering of hidden states in the decoder (Section SECREF11).\n\nTherefore, the answer to the question is:\n\nAnswer: 1 (measured using average unique predictions).", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": " Yes. According to the article, the following publicly available datasets are used:\n\n* Twitter dataset (Waseem et al.)\n* Reddit dataset (Davidson et al.)\n* YouTube dataset (Gao et al.)\n\nThese datasets are used for training and evaluating the hate speech detection model proposed in the article.", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": " Yes.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": " Yes, the article mentions several evaluation methods used in the field of text summarization, including ROGUE.", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": " Yes. According to the article, KAR is an end-to-end MRC model named as Knowledge Aided Reader (KAR), which is an instance of a neural network-based model.", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": " Yes, they compared the performance of layer-wise trained LSTM models with Xavier initialization and sMBR loss function with different numbers of layers.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " The two large-scale datasets used are:\n\n1. Disinformation articles, published by websites known for producing low-credibility content, and flagged as such by reputable sources.\n2. Mainstream news articles from various sources, including news outlets with different political biases.", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": " The article reports the following evaluation metrics:\n\n* Accuracy\n* F1-score\n* INLINEFORM0 and INLINEFORM1 (not specified what these are)\n* Performance on real-world data (not specified what this is)\n\nSo, the answer to your question is:\n\nYes, the article reports evaluation metrics.", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": " No.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": " Yes, the article mentions that the proposed dataset, ALOHA, models character's profiles by collecting dialogues from a variety of characters and linking them to their HLAs (Human Level Attributes).", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " According to the article, the accuracy merits of the approach are demonstrated through experimental evaluation on real-world datasets, which shows that the proposed approach significantly outperforms the state of the art and is particularly useful for detecting events where relevant microposts are semantically complex, such as the death of a politician. Therefore, the answer is \"yes\".", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": " Yes, their system's absolute accuracy is 59.4%.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": " According to the article, the proposed KBQA system achieves state-of-the-art results on both single-relation and multi-relation KBQA tasks on the WebQSP and SimpleQuestions benchmarks. Therefore, the answer is \"yes\".", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": " Based on the article, the authors found ironic data on Twitter by crawling over 2 million tweets and building a dataset of 262,755 ironic and 112,330 non-ironic tweets. Therefore, the answer is \"yes\".", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " KBQA abbreviates to Knowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": " Yes. They show there is space for further improvement by conducting a human study that tested the model's performance on a random subset of questions and found that there were still some questions that the model could not answer correctly.", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": " The baseline model was a neural network model based on contextual word embeddings.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": " Yes. According to the article, the researchers use a variety of approaches towards text analysis, including:\n\n1. Measuring the distribution of hate speech in online communities.\n2. Analyzing the connection between online discourse and offline events.\n3. Examining the role of social media in shaping public opinion.\n4. Investigating the impact of cultural and historical context on text analysis.\n5. Using machine learning algorithms to identify patterns in text data.\n\nThese approaches are not mutually exclusive, and the researchers may combine them in various ways to achieve their research goals", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": " According to the article, the following real-world datasets are used:\n\n* CyberAttack\n* PoliticianDeath\n\nTherefore, the answer is \"yes\".", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": " Yes, the article provides information on the performance of the model across different sectors. According to the article, the best performance was achieved by the Healthcare sector.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " Yes. According to the article, \"we experimented with a wide range of machine learning methods in order to extract argument structure from documents, including feature-based methods, as well as methods based on word embeddings.\"", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": " The evaluation criteria and metrics used to evaluate the generated text are:\n\n* Minimum Edit Evaluation: factual mistakes and grammatical errors corrected\n* Factual Correctness: number of factual errors and their types\n* Fluency: most common error types and overall flow of the text\n* Product-Readiness Evaluation: usability for production use and WER (word error rate)\n\nUnanswerable: What are the specific metrics used for each evaluation criterion?", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": " According to the article, the classifiers used are deep neural networks. Therefore, the answer is \"yes\".", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " Based on the article, the answer is:\n\nUnanswerable.\n\nThe article does not provide information on the state of the art in multi-party conversational systems, and therefore cannot answer this question.", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": " Yes. According to the article, \"our framework combines text-embedding, geometric graph construction, and multi-resolution community detection to identify, rather than impose, content-based clusters from free, unstructured text in an unsupervised manner.\" Therefore, text embedding methodologies are used in the study.", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": " Yes. According to the article, the strong baseline is BLEU, which improves from 14.19 to 16.85 ($+18.75\\%$) on the test set.", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": " The difference in performance between the proposed model and baselines is significant, with the proposed model achieving a noticeable improvement in performance across all five folds.\n\nUnanswerable: Can you provide more information about the TV Tropes dataset used in the study?", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": " The data was collected through crowdsourcing on Amazon Mechanical Turk (MTurk).", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": " Yes. They build a predictive model to classify dogmatic posts from Reddit.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": " The answer to the question is:\n\nUnanswerable\n\nThe article does not provide information on how the data in the new corpus is sourced.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": " Unanswerable.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": " They looked at response time.", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": " Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": " The model is applied to two datasets:\n\n1. \"Conversations Gone Awry\" dataset\n2. \"in-the-wild moderation of the popular subreddit ChangeMyView\" dataset\n\nUnanswerable.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " The authors show that their learned policy generalizes better than existing solutions to unseen games by training a Seq2Seq model on the trajectories found by Go-Explore and then evaluating its performance on 4,440 games from the \"First TextWorld Problems\" dataset. They find that their policy performs better than existing competitive baselines on these unseen games, suggesting that it has better generalization ability. [Yes]", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": " Second order co-occurrence matrix: unanswerable.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order coâ€“occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": " Yes. According to the article, the state-of-the-art system is MPAD.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": " The size of their dataset is approximately 2,100 texts and 14,000 questions.", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": " The article uses the Europarl dataset for the experiments.\n\nYes.", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": " No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": " Based on the article, the baseline methods are:\n\n* NaiveNN\n* FANN\n\nYes.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": " Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " Unanswerable.", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": " The paper explores the following embedding techniques:\n\n* Word2vec\n* Word embeddings learned from structured corpora (such as literature and dictionaries)\n\nUnanswerable.", "answers": ["Skipâ€“gram, CBOW", "integrated vector-res, vector-faith, Skipâ€“gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " According to the article, the size of their data set is not explicitly mentioned. Therefore, the answer is \"unanswerable\".", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " The article does not provide information on the metrics used in evaluation, so the answer is \"unanswerable\".", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": " Yes. According to the article, the researchers use data from a particular social media platform for their analysis.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": " Coverage(INLINEFORM0) is used to establish that the strategy formulation ability of LiLi is improved.\n\nUnanswerable: What are the metrics used to evaluate the predictive performance of LiLi?", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " Yes. According to the article, the ML methods aim to identify argument components such as premise, claim, rebuttal, and backing.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " Yes, they use a newly released corpus of state speeches delivered during the UN General Debate.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": " Yes. According to the article, a node in the network approach represents a state in international politics.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": " The article does not provide information on the datasets used. Therefore, the answer is \"unanswerable\".", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " Unanswerable.", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a â€œbig questionâ€ that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": " Unanswerable. The article does not provide information on how the authors identify discussions of LGBTQ people in the New York Times.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": " Yes.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": " The datasets were annotated using a carefully designed and rigorously guided protocol for developing monolingual Multi-SimLex datasets with aligned concept pairs, which yields data with high inter-annotator agreement rates.\n\nUnanswerable: How were the datasets created?", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": " Yes. According to the article, the 12 languages covered are:\n\n1. German\n2. Italian\n3. Russian\n4. Mandarin Chinese\n5. French\n6. Spanish\n7. Arabic\n8. Kiswahili\n9. Welsh\n10. Yue Chinese\n11. Farsi\n12. Turkish.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
