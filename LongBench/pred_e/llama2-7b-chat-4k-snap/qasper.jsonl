{"pred": " According to the article, they chose different Facebook pages for training data based on their availability and intuition, but they did not provide a specific list of pages. Therefore, the answer is \"unanswerable\".", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " Yes", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": " Unanswerable. The article does not provide information on the size of improvements in small-scale unbalanced datasets when sentence representation is enhanced with topic information.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": " Based on the article, the pivot-based baselines are:\n\n* BIBREF: pivot-based method\n* BIBREF: pivot-based method\n* BIBREF: pivot-based method\n\nUnanswerable.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": " Unanswerable. The article does not provide information on the amount of training data used by the system.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": " The paper was published in 2022, based on the information provided in the article. Therefore, the answer is:\n\nYes.", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": " Unanswerable. The article does not provide information on how the vocabulary of word-like or phoneme-like units is automatically discovered.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": " The article mentions the following datasets:\n\n* MIMIC-III\n* INTEROP\n* MIMIC-WHO\n\nSo, the answer is:\n\nYes, the article mentions three datasets.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": " Based on the article, the answer is:\n\nYes, they look at 30% of the Amazon search terms, which contain parts of the book's title.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": " Unanswerable. The article does not provide information on the type of latent context used to predict instructor intervention.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": " Unanswerable", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": " Unanswerable. The article does not provide any information on why prior knowledge distillation techniques are ineffective in producing student models with vocabularies different from the original teacher models.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": " Yes.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": " The article mentions the following datasets were used:\n\n* EmotionLines (a dataset of dialogues)\n* EmotionX 2019 (a challenge dataset)\n\nSo, the answer is:\n\nYes", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": " Based on the article, the more predictive set of features to detect fake news is:\n\n\"Words embeddings, sentiment, and morality.\"\n\nUnanswerable: Can the article answer the question? No.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": " According to the article, the collection of COVID-19 literature contains over 45,000 articles.\n\nYes.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": " Unanswerable. The article does not provide information on any other evaluation metrics beyond F1-score and accuracy.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": " Yes", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": " No.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": " According to the article, attention and alignment differ in the following cases:\n\n* Attention agrees with traditional alignment to some extent (unanswerable).\n* Attention has different patterns based on the syntactic structure of the sentence (unanswerable).\n* Attention is more capable of capturing information beyond alignment in some cases (unanswerable).\n\nTherefore, the answer to the question is \"yes\".", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": " The size of the corpora used to train ELMo is not provided in the article. Therefore, the answer is \"unanswerable\".", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": " Yes, the article mentions that clustering algorithms were used to discover broad categories among the strengths, weaknesses, opportunities, and threats identified in the PA process. However, it does not provide any specific information about the clustering algorithms used. Therefore, the answer is \"unanswerable\".", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": " Based on the article, the answer is: unanswerable. The article does not provide any information about the number of people who participated in the evaluation study of table-to-text models.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": " The encoder has an LSTM architecture.\n\nUnanswerable: Does the system use a multilingual approach?", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " Yes.", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": " Yes, they test BERTBase-PN.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": " Yes. According to the article, the end-to-end speech recognition system uses a Recurrent Neural Network (RNN) with a sequence-to-sequence architecture.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": " Based on the article, the recipes are obtained from a dataset of 280,000 recipes scraped from Food.com.\n\nYes.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": " The models used in the experiment are:\n\n* SVM\n* CNN\n* LSTM\n\nYes.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " The baselines were (i) using a vanilla LSTM model and (ii) using a LSTM model with a fixed cell state.\n\nUnanswerable: Can you provide more information about the baselines?", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": " Yes, the article provides information on the textual patterns that are extracted for generating errors. According to the article, the method uses known incorrect sequences of words, called \"textual patterns\", to generate errors. These patterns are learned from an annotated corpus of text and are used to generate errors in a way that is similar to how they occur in the training data.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": " The datasets used in evaluation are:\n\n* Semantic Scholar\n* MovieLens\n* BookCrossing\n* Netflix\n\nYes.", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": " Yes.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": " According to the article, the length of the dataset for each step of the hierarchy is as follows:\n\nLevel A: Offensive language detection (14,000 tweets)\nLevel B: Type of offensive language (10 categories, 1,000 tweets per category)\nLevel C: Target of offensive language (10 categories, 1,000 tweets per category)\n\nTherefore, the total length of the dataset is approximately 15,000 tweets.\n\nUnanswerable: How many tweets are annotated for each", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": " Yes. According to the article, five keyphrase extraction models were reassessed: \"re-implemented five keyphrase extraction models, including four commonly used models (BM25, TF-IDF, Latent Semantic Analysis, and TextRank) and one state-of-the-art model (Graph-based)\".", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": " Yes. According to the article, a strong neural baseline was used, but the specific type of neural model is not mentioned.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": " Yes. According to the article, the following NER models were evaluated:\n\n1. Recurrent Neural Network (RNN)\n2. Stanford NER\n3. spaCy NER\n\nUnanswerable: Can you provide more information about the other NER models evaluated in the article?", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": " Yes, the article provides information about the model architecture used in the study. According to the article, the authors use a model architecture similar to that in [BERARD2018] but with three decoder layers like in [PINN2019]. Therefore, the answer to the question is \"yes\".", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": " Unanswerable. The article does not provide any information on how Gaussian-masked directional multi-head attention works.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": " Based on the article, attention captures different patterns based on the type of word being translated. For example, when translating verbs, attention pays more attention to the smeared context, while for nouns, it pays more attention to the aligned context. Therefore, the answer is \"yes\".", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": " Based on the article, the baseline is \"BIBREF1\".", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": " Yes. According to the article, the baseline method used is LibSVM.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": " Unanswerable. The article does not provide any information about the system's performance.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": " Unanswerable.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": " Based on the article, the answer to the question \"what data did they use?\" is:\n\nUnanswerable.\n\nThe article does not provide any information about the data used in the study.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": " They explore BiLSTM, BiLSTM with CRF, and BiLSTM with CRF and word embeddings.\n\nUnanswerable: What is the contribution of the paper?", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": " Yes. The article proposes using context tweets to improve the accuracy of neural network models for detecting abusive language.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " The article mentions the following datasets for evaluation:\n\n* Universal Dependency (UD)\n* Cross-lingual Sentiment Analysis (XLSA)\n* Zero-shot Sentiment Analysis (ZSA)\n\nTherefore, the answer is:\n\nYes", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": " The corpus used for the task is DIALEKTIS.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " According to the article, the ancient Chinese dataset comes from the following sources:\n\n* \"INLINEFORM0\" (a parallel corpus of ancient Chinese texts)\n* \"Test set\" (a subset of the parallel corpus used for evaluation)\n\nTherefore, the answer is:\n\nAnswer: INLINEFORM0", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " Baselines: BOW, TF-IDF, Logistic Regression, TextCNN, and Neural Network.", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequencyâ€“inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": " Multi-granularity and multi-tasking are specific to the neural architecture design in the article.\n\nYes.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": " Yes. The proposed model is compared to various baseline models, including traditional features by simply applying them to the abbreviation disambiguation task without any improvement (unanswerable).", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": " Yes", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " Based on the article, the answer to the question is:\n\nUnanswerable.\n\nThe article does not provide any information about the languages for which word embeddings are built.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": " No.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": " The backoff strategies work by falling back on a neutral variant of the word recognition model, which is less vulnerable to character-level attacks.", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": " Unanswerable. The article does not provide any information about the accents present in the corpus.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " Unanswerable", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": " Macaw consists of the following modules:\n\n* Retrieval\n* Question Answering\n* Co-reference Resolution\n* Named Entity Recognition\n* Dependency Parsing\n* Text Classification\n\nUnanswerable.", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": " Yes. According to the article, experiments are performed on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " Yes. According to the article, the authors performed qualitative experiments on benchmark datasets to evaluate the performance of the proposed approach. These experiments include comparing the performance of the proposed approach with existing approaches on various datasets.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": " According to the article, the authors train their models on the SemEval 2010 task 8 dataset, which is a benchmark dataset for relation classification. Therefore, the answer is:\n\nYes.", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": " Yes, the article mentions that the proposed method introduces significant improvements in F1 score for paraphrase identification tasks. The exact improvement is not provided in the article, but it states that the proposed method can achieve an F1 score of 83.33% on the test set, which is a significant improvement over the baseline method.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " The questions present in the dataset are natural language questions that are human-generated and come from a variety of sources, including news articles, product reviews, and social media posts.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": " Unanswerable. The article does not provide any information on how the variance from the model outputs is calculated.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": " The article mentions the following linguistic features are used in the proposed model:\n\n* Surface level features: word-level features such as word length, and word-level distribution of diacritics\n* Morphological features: morphological information such as part-of-speech tags, and morphological features such as the presence of prefixes, suffixes, and inflectional endings\n* Contextual features: contextual information such as the presence of certain words or phrases in the surrounding context\n\nTherefore, the answer to the question is:\n\nYes, the article mentions linguistic features", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": " Yes", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " Yes. According to the article, the new corpus contains 53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": " Unanswerable. The article does not provide information on 10 other phenotypes that are annotated.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": " Yes. According to the article, the two news domains that are country-independent are disinformation and misinformation news domains.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": " The average length of the essays in the NLI dataset is not provided in the article. Therefore, the answer is \"unanswerable\".", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": " Based on the article, they evaluate their resulting word embeddings using intrinsic evaluation methods such as word similarity, synonymy, and antonymy, as well as extrinsic evaluation methods such as downstream natural language processing tasks. Therefore, the answer is \"yes\".", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": " Unanswerable. The article does not provide any information about future possible improvements.", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": " Yes, they compare to BIBREF26 who use a task-specific architecture compared to their generic sequence-to-sequence model.", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": " Yes. According to the article, the Random Kitchen Sink (RKS) approach is an approach used to improve the evaluation of offensive language detection systems by explicitly mapping the data to a higher dimensional space using a random kernel.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": " unanswerable", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " Yes. According to the article, the authors' model improves interpretability compared to softmax transformers by allowing attention weights to be sparse, which can lead to more interpretable attention patterns.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " Yes. According to the article, several approaches have been applied to solve word segmentation in Vietnamese, including lexical approaches, machine learning approaches, and hybrid approaches.", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": " According to the article, they test their method on the following tasks:\n\n* Named entity recognition\n* Part-of-speech tagging\n* Dependency parsing\n* Text classification\n\nSo, the answer is \"yes\".", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " Yes, the article mentions that the baseline model used is \"Last-in-First-out (LIFO)\".", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": " They addressed three cyberbullying topics: personal attacks, racism, and sexism.\n\nYes.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": " Unanswerable.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " The paper uses the WN18 and FB15K datasets to evaluate the performance of the proposed method.", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": " The sentiment analysis dataset used is the IMDb dataset.\n\nYes", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " Yes. According to the article, two datasets are developed and released for the task of forecasting antisocial events in online conversations. These datasets include the \"Conversations about Antisocial Behavior\" (CAB) dataset and the \"Conversations about Hate Speech\" (CHS) dataset.", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": " The dataset used for this study is an existing, annotated Twitter dataset that was constructed based on a hierarchical classification of depression symptoms.", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": " Unanswerable based on the information provided in the article.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " Yes, according to the article, PolyResponse engine is used for restaurant search and booking system in 8 languages: English (Edinburgh), German, French, Italian, Spanish, Korean, and Japanese.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": " Yes, the article out-performs K-means and Bayesian approaches.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": " Yes, the article evaluates transfer learning tasks for Sentence-BERT.", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": " Yes. According to the article, the baseline is ELMO with a ROUGE-1 F1 score of 53.5.", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": " Yes, the article mentions the sources of the datasets as follows:\n\n* \"a multilingual news article dataset\" (no further information provided)\n* \"a dataset of news articles from various sources\" (no further information provided)\n* \"a collection of news articles from different sources\" (no further information provided)\n\nTherefore, the answer to your question is \"yes, the sources of the datasets are mentioned in the article\".", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": " Yes.", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": " Unanswerable. The article does not provide any information about manual Pyramid scores used.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": " Yes.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": " Yes, other sentence embeddings methods are evaluated in the article. The methods include:\n\n* BERT\n* RoBERTa\n* XLNet\n* InferSent\n* GloVe\n* Word2Vec\n\nThese methods are compared to SBERT, the sentence embedding method proposed in the article, in terms of their performance on various NLP tasks.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " Yes. According to the article, the real-life dataset used in the study contains 45,000 instances.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": " Based on the article, the answer to the question \"how large is the vocabulary?\" is:\n\nUnanswerable.\n\nThe article does not provide information on the size of the vocabulary used in the annotation task.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": " The evaluation proposed for this task is:\n\nYes", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " The size of the dataset is TABREF3000.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": " Future work planned: unanswerable", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": " Yes. According to the article, the best performing model among the author's submissions is the ensemble of Logistic Regression, Random Forest, and Gradient Boosting, which achieved an F1-score of 0.83 on the test set.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " The article mentions the following metrics for evaluation:\n\n* BLEU (Bilingual Evaluation Understudy)\n* METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n* ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n\nTherefore, the answer is \"yes\".", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": " The CORD-19 dataset is a dataset used in the article for developing natural language processing methods for COVID-19.\n\nYes.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": " Yes. The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of fine-tuning the pre-trained language model on downstream tasks and observing the performance of the model on different demographic groups.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " English", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": " The new context representation for relation classification is proposed by extending the middle context of a sentence, which is presented in the article. Therefore, the answer is \"yes\".", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " The article uses the following answer:\n\n\"The Neural Dialogue Corpus (NDC) is used to learn the behavior of the Neural Network.\"\n\nTherefore, the answer to the question is \"yes\".", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": " They mean that the model captures both the global context of the entire document and the local context of each section or topic within the document.\n\nUnanswerable: Can the model generate summary sentences based on the provided article?", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": " The two metrics proposed are:\n\n1. Average of the number of unique words in the keyphrases generated by each model.\n2. Average of the number of unique words in the keyphrases generated by each model that are also present in the original article.\n\nUnanswerable.", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": " Unanswerable. The article does not provide information on which dimension the semantically related words take larger values.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": " Based on the article, the state of the art methods for grammar induction are:\n\n* Neural Networks (Bibiano et al., 2017)\n* Deep Learning (Bhattacharya et al., 2019)\n\nTherefore, the answer is \"yes\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": " Unanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " The imbalance in the analyzed corpora is significant, with 65% of the speakers being male and 35% being female.\n\nUnanswerable: Is there a significant difference in the performance of the ASR system for male and female speakers?", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " Based on the article, the state-of-the-art models for judgment prediction are:\n\n* BERT-based models\n* CNN-based models\n* LSTM-based models\n\nYes.", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": " unanswerable", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " No.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": " unanswerable", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " Yes, the article provides information on the text-based games used in experiments. According to the article, the following games are used:\n\n1. CoinWorld\n2. CookingWorld\n3. FirstTextWorld\n4. GoWorld\n5. JigsawWorld\n6. MazeWorld\n7. PuzzleWorld\n8. TextAdventure\n9. TriviaWorld\n\nTherefore, the answer to the question is: The text-based games used in experiments are CoinWorld, CookingWorld, FirstTextWorld, GoWorld, JigsawWorld, Maz", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": " Yes, the article mentions that their model captures biases in the data collection process, including biases in the annotation of hate speech.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": " Based on the article, the success of style transfer is measured by the following:\n\n* \"monolingual transfer\": the model's ability to translate a sentence from one language to another without any errors.\n* \"grammatical correction\": the model's ability to correct grammatical errors in a sentence.\n* \"lexical substitution\": the model's ability to replace words in a sentence with their correct counterparts.\n* \"perceptual evaluation\": the model's ability to transfer the style of a sentence while preserving its meaning and readability.\n\nTherefore, the answer", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": " The article mentions that the following methods were used to reduce data sparsity effects:\n\n* Using different segmentation techniques for rare words (unanswerable)\n* Experimenting with different methods for wordpiece training (unanswerable)\n* Adapting the training data by using data augmentation techniques (unanswerable)\n\nTherefore, the answer is \"unanswerable\".", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": " The additive modification to the objective function is:\n\n\"an additive term to the cost function of each word in the GloVe matrix, which favors vectors that are similar to the vectors of words in a specific context.\"\n\nUnanswerable: Can the method be used to learn dense vector representations of words?", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": " Based on the article, the baseline is \"a strong baseline\" (yes).", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " Yes.", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": " The novelty of their document-level encoder is mentioned in the following sentence: \"We explore the potential of pre-training a document-level encoder based on BERT, which has shown impressive performance in various NLP tasks.\" (emphasis added)\n\nTherefore, the answer to your question is: \"novel\".", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": " Yes.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": " According to the article, the use of RNNs in the field of SMT improves the efficiency of the attention mechanism by about 5%. Therefore, the answer is \"yes, 5%\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": " They measure which words are under-translated by NMT models by exploiting the intermediate representations of the models and the gradient of the loss function with respect to the model's parameters. (Yes)", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": " Unanswerable based on the provided article. The article does not provide any information about instances being sentences or IE tuples, and does not provide any context that would allow for a definitive answer to this question.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": " Yes. According to the article, the proposed method improves over the best performing state-of-the-art by 0.65 ROUGE scores.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": " The article mentions that the experiment uses a real-world dataset of civil case INLINEFIGURE0 from the Supreme People's Court of China. The dataset contains INLINEFIGURE0 case documents, each of which is labeled with a judgment prediction result. The article does not provide any further information about the datasets used in the experiment. Therefore, the answer to the question is \"unanswerable\".", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": " Based on the article, the answer to the question \"How do they match annotators to instances?\" is:\n\nUnanswerable.\n\nThe article does not provide any information on how the authors match annotators to instances.", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": " Based on the article, the state of the art models for sarcasm detection are:\n\n* BIBREF0: A convolutional neural network (CNN) model that uses a combination of lexical and semantic features to detect sarcasm.\n* BIBREF1: A recurrent neural network (RNN) model that uses a combination of lexical and semantic features to detect sarcasm.\n* BIBREF2: A pre-trained language model that uses a combination of lexical and semantic features to detect sarcasm.\n\nUnanswerable: What are the specific features used", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": " The model they use is a joint model that combines fine-tuned Inception V3 and BERT models.\n\nYes.", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": " Yes, the article mentions several evaluation methods used in the field of text summarization, including ROUGE, METEOR, BLEU, and REAL.", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": " The core component for KBQA is hierarchical entity ranking.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": " They test their word importance approach on different model architectures, but the article does not provide the specific architectures they used. Therefore, the answer is \"unanswerable\".", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": " The global network features which quantify different aspects of the sharing process are:\n\n* Largest Cascade Size (LCS)\n* Main Structure (MS)\n* Largest K-Nearest Neighbors (KNN)\n* Largest Local Degree (LD)\n* Largest Global Degree (GD)\n\nUnanswerable.", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": " Yes.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": " The two large-scale datasets used are:\n\n1. Disinformation articles (source: United States)\n2. Influence articles (source: Italy)", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": " They used the BioASQ dataset. (yes)", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": " No", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": " Reuters database (unanswerable)", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": " Based on the article, the models use the gradients of the intermediate representations to compute the attention weights, which are then used to determine the importance of the output words. Therefore, the answer is \"yes\".", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": " The article states that the data manipulation techniques significantly improve the classification performance in the low-data regime and class-imbalance problems. However, the article does not provide specific numbers or percentages to answer the question directly. Therefore, the answer is \"unanswerable\".", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": " Yes. According to the article, the researchers use a variety of approaches towards text analysis, including measuring the elimination of hate speech, studying the impact of cultural context on the interpretation of texts, and examining the role of language in shaping social and political discourse.", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": " Based on the information provided in the article, the model is more reliable for correcting spelling errors and lexical errors, but less reliable for correcting grammatical errors. Therefore, the answer is:\n\nYes, the model is more reliable for correcting spelling errors and lexical errors, but less reliable for correcting grammatical errors.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": " Yes. According to the article, the machine learning methods used in experiments are mentioned as follows: \"We experiment on the annotated data using various machine learning methods.\"", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": " They utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications. (yes)", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": " Yes, the article mentions several publicly available datasets that are used for hate speech detection, including:\n\n* Twitter dataset by Waseem et al. (2016)\n* Reddit dataset by Zhang et al. (2017)\n* YouTube dataset by Sculley et al. (2017)\n* Wikipedia dataset by Zhang et al. (2018)\n\nThese datasets are used for fine-tuning and evaluating the performance of the proposed hate speech detection model.", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": " Based on the article, keyphrase diversity can be measured using the following methods:\n\n* Diversified sequence-to-sequence generation\n* Orthogonal phrase extraction\n* Keyphrase coverage\n\nTherefore, the answer to the question is \"unanswerable\" as the article does not provide information on how keyphrase diversity is measured.", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": " Yes, their highest recall score was 0.71.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " Yes. According to the article, KAR is an end-to-end model named Knowledge Aided Reader (KAR).", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": " Yes. According to the article, the following text embedding methodologies are used:\n\n1. Doc2vec\n2. Paragraph vector\n\nThese methodologies are used in the framework to identify, rather than a detailed analysis of the specific methodologies used in each case.", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": " Yes, they compared the performance of layer-wise pre-trained LSTM models with Xavier initialization and the performance of the 9-layer LSTM model with Xavier initialization.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": " The article mentions that two machine learning (ML) and deep learning (DL) methods are used for RQE:\n\n* Logistic Regression (LR)\n* Convolutional Neural Networks (CNNs)\n\nTherefore, the answer is:\n\nYes", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " Yes", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": " Unanswerable.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": " The accuracy merits of the approach are demonstrated through experimental results presented in the article. The article evaluates the performance of the proposed approach using multiple benchmark datasets and compares it with existing methods. The results show that the proposed approach outperforms existing methods in terms of accuracy, as stated in the article. Therefore, the answer to the question is \"yes\".", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": " According to the article, the size of the real-world civil case dataset is INLINEFIGURE0.\n\nUnanswerable.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": " Yes, the article proposes a method for modeling character profiles using a dataset of human-like dialogue, which is called Human Level Attributes (HLAs). The HLAs are based on the audience's perspective and are defined as the consistent personality traits and characteristics of a fictional character across different dialogues. The article does not provide information on how the dataset models character profiles, so the answer is \"unanswerable\".", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": " Unanswerable. The article does not provide information on the distinctive characteristics of how Arabic speakers use offensive language.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": " Based on the article, the authors crawled over 2 million tweets from Twitter to build a dataset of ironic and non-ironic tweets. Therefore, the answer is \"yes\".", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": " They achieve the state of the art on SimpleQuestions and WebQSP.\n\nUnanswerable: Can you provide more information about the benchmarks they used?", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": " Yes.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " No.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": " Yes, KBQA abbreviates to Knowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": " The baseline model was a neural network model based on contextual word embeddings.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": " Yes. The article reports the following evaluation metrics:\n\n* Accuracy\n* F1-score\n* INLINEFORM0\n* INLINEFORM1\n* BIBREF0\n* BIBREF1\n\nThese metrics are used to evaluate the performance of the proposed model in predicting the judgment of civil cases.", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": " The evaluation criteria and metrics used to evaluate the generated text were:\n\n* Human evaluation: factual errors, coherence, readability, and fluency\n* Word error rate (WER)\n* Fluency score\n* Coherence score\n* Readability score\n\nUnanswerable: What is the level of manual effort required for the manual annotation of the training data?", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " According to the article, the following real-world datasets are used:\n\n* Microblog dataset from Twitter\n* Cybersecurity dataset from Kaggle\n\nTherefore, the answer is \"yes\".", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": " Yes, the strong baseline is BLEU.", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": " Yes, their system's absolute accuracy is 59.4%.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": " Based on the article, the answer to the question \"What type of classifiers are used?\" is:\n\nUnanswerable.\n\nThe article does not provide any information about the type of classifiers used for event detection in microblog posts.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " Yes. They show there is space for further improvement by conducting a human study that demonstrates there is still room for improvement beyond the performance of their own model.", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": " Yes.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": " The data was collected through a combination of pilot studies and crowdsourcing. Specifically, the article states that the data was collected through a pilot study involving 10 participants, followed by a larger crowdsourcing campaign involving over 1,000 participants. The participants were asked to complete a series of tasks, including answering questions about a set of scripts, in order to generate the dataset.\n\nUnanswerable.", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": " The difference in performance between the proposed model and baselines is significant, with the proposed model achieving an overall Hits@1 score of 0.42 and the best baseline score being 0.27, resulting in a relative improvement of 63%. This is unanswerable based on the information provided in the article.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " Yes, the article provides information on the performance of different stock market sectors. According to the article, the sector-level results show that the proposed model outperforms the baseline model in all sectors, with the highest accuracy achieved in the Energy sector. Therefore, the answer to the question is \"Energy\".", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " Yes. They build a predictive model to classify dogmatic opinions based on linguistic features and psychological aspects.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": " The authors show that their learned policy generalizes better than existing solutions to unseen games by training a Seq2Seq model on a set of text-based games and then evaluating its performance on a set of unseen games. They report that their policy finds more winning trajectories than existing competitive baselines in this setting. Therefore, the answer is \"yes\".", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": " Yes. According to the article, the state-of-the-art system is MP framework.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": " The model is applied to two datasets:\n\n1. Triples in size entirely new dataset, entirely new dataset entirely new.\n2. Wikipedia's Change Log, which tracks changes to Wikipedia articles over time.\n\nUnanswerable: Can the model be applied to any other dataset?", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " The article uses the Europarl dataset.\n\nYes.", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": " No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": " Based on the article, the state of the art described is the design and implementation of a conversational AI system for a fintech company. The system is designed to handle a wide range of customer inquiries and provide personalized responses. The authors present a hybrid approach that combines rule-based and machine learning-based models to improve the system's performance. They also discuss the challenges of evaluating the system's performance and the need for further research in this area.\n\nUnanswerable: Is the system able to handle complex, open-ended questions?", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": " Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " No.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": " Based on the article, a second order co-occurrence matrix is \"a matrix that holds pairwise co-occurrence information between all terms in the corpus, where each cell represents the frequency of term i and term j co-occurring in the corpus\" (INLINEREF0). Therefore, the answer is \"unanswerable\" because the article does not provide any information about second order co-occurrence matrices.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order coâ€“occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": " Yes, the paper explores various embedding techniques, including word embeddings and neural network-based approaches.", "answers": ["Skipâ€“gram, CBOW", "integrated vector-res, vector-faith, Skipâ€“gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " The size of their dataset is 11,000 questions.\n\nUnanswerable.", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": " The answer to the question is:\n\nUnanswerable\n\nThe article does not provide information on how the data in the new corpus is sourced.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": " Yes. According to the article, the metrics used in evaluation are:\n\n* Macro-averaged precision\n* Macro-averaged recall\n* F1-score\n\nThese metrics are used to evaluate the performance of the propaganda detection model.", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": " Based on the article, the baseline methods are:\n\n* BIBREF0\n* BIBREF1\n* BIBREF2\n\nUnanswerable.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": " They looked at resource consumption metrics, such as CPU and memory consumption, to evaluate the performance of the chatbot.\n\nUnanswerable: Did they evaluate the chatbot's ability to handle complex or nuanced conversations?", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": " Unanswerable.", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": " Coverage is used to determine that this makes chatbots more knowledgeable and better at learning and conversation.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": " Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " The answer is: unanswerable. The article does not provide any information about the size of the data set used in the study.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " Yes, they use a newly released corpus of state texts.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": " Yes, they use data from Reddit.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": " Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": " The article does not provide information on the datasets used. Therefore, the answer is \"unanswerable\".", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " Yes.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": " No.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": " Yes. According to the article, the ML methods aim to identify several novel argument structure features, including:\n\n* Propositions: The ML methods aim to identify the propositions expressed in the text.\n* Premises: The ML methods aim to identify the premises of an argument.\n* Conclusions: The ML methods aim to identify the conclusions of an argument.\n* Warrants: The ML methods aim to identify the warrants that connect the premises to the conclusions.\n\nThese are the argument components that the ML methods aim to identify based on the article.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " Yes. According to the article, the 12 languages covered are:\n\n1. English\n2. German\n3. French\n4. Spanish\n5. Italian\n6. Portuguese\n7. Dutch\n8. Russian\n9. Arabic\n10. Mandarin Chinese\n11. Japanese\n12. Korean", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": " Yes. According to the article, a node in the network approach represents a country or a state.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": " Unanswerable.", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a â€œbig questionâ€ that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": " The datasets were created using a carefully designed protocol that ensures consistency and accuracy in the annotations. The exact details of the annotation process are not provided in the article, so the answer is \"unanswerable\".", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
